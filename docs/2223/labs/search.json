[
  {
    "objectID": "1_01_design_and_data.html",
    "href": "1_01_design_and_data.html",
    "title": "Research design & data",
    "section": "",
    "text": "Tip: Lab instructions\n\n\n\n\n\n\nPlease work through the lab exercises in small groups of 3 to 5 students.\nYou will be given some data that you will use throughout the next 5 weeks.\n\nAs a group, you have to produce a data analysis PDF report on those data.\n\nIn week 5, you will be asked to submit the PDF report, for which you will receive formative feedback in week 6.\nOne person is the driver, responsible for typing on the PC, and the rest are navigators and cannot type. Navigators are responsible for commenting on the strategy, code, and spotting typos or fixing errors. Each week you will rotate so that everyone experiences being a driver.\nDriver: open an Rmd file, and start writing your work there.\n\nNavigators: be alert and start providing suggestions and comments on the strategy and code.\n\nFormat\n\nPDF file, max 4 sides of A4 paper, keep the default settings in terms of Rmd knitting font and page margins.\nAppendix with all the code in a code chunk with the option results='hide'.\n\n\n\n\n\n\n\n\n\n\nTip: Lab help and support\n\n\n\n\n\nThe lab is structured to provide various levels of support. When attending the labs, you should directly attempt and work on the tasks. However, if you are unsure or stuck at any point, you can make use of the following help:\n\nSimply raise your hand and get help from a tutor\nHover your mouse on the superscript number to get a hint. The hints may sometimes show multiple equivalent ways of getting an answer - you just need one way\nScroll down to the Worked Example section, where you can read through a worked example.\n\n\n\n\n\n\n\n\n\n\nCaution: Did you register for RStudio Server Online?\n\n\n\n\n\n\n\nA. Yes\nB. No\nIf B didn’t work\n\n\n\n\nLogin to EASE using your university UUN and password.\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and RStudio password.\n\n\n\nTry these steps first to register for RStudio server online:\n\nLog in to EASE using your university UUN and password.\nSet your RStudio password here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you set above in (2).\n\n\n\n\nPlease complete this form and wait for an email. Please note that this can take up to four working days.\n\nOnce you receive an email from us, please follow the following instructions:\n\nSet your here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you just set above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant: Install tinytex\n\n\n\n\n\nBefore you begin, make sure you have tinytex installed in R so that you can “Knit” your Rmd document to a PDF file:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()"
  },
  {
    "objectID": "1_01_design_and_data.html#formative-report-a",
    "href": "1_01_design_and_data.html#formative-report-a",
    "title": "Research design & data",
    "section": "\n2 Formative report A",
    "text": "2 Formative report A\nIn the first five weeks of the course you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 6. The report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\n2.1 Data\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n\nIQ1-IQ50: IQ score of each of 50 audience raters\n\nSnacks: How many of the 50 audience raters brought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation\n\n\n\n\n\n\n\nFor formative report A, please only focus on the variables Movie to Year, ignoring anything beyond that. In other words, do not analyse the variables IQ1 to PrivateTransport in the next five weeks of the course. We will use those later in the course.\n\n\n\n\n2.2 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course:\n\n\n\n\n\n\nNote: This week’s task\n\n\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\n\n\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\nThis week you will only focus on task A1. Below there are some guided sub-steps you may want to consider to complete task A1.\n\n2.3 A1 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the movie data into R, and give it a useful name. Inspect the data by looking at the data in RStudio. By viewing, we actually mean looking at the data either on the viewer or the console.1\nHow many observations are there?2\nHow many variables are there?3\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\n\nWhat does dim(DATA) return?\nWhat is the function of appending a [1] or [2]?\n\n\n\n\n\nWhat is the type of each variable?4\nWhat’s the minimum and maximum budget in the sample? What about the average Rotten Tomatoes rating?5\nDo you notice any issues when computing the minimum and maximum Budget and the average RottenTomatoes rating?6\nWhat is the range (i.e. minimum and maximum) of the variables in the data? What about the number of missing values for each variable?7\nWrite-up a description of the dataset for the reader. You don’t need to show the actual data in the report, but a description in words is sufficient for the reader."
  },
  {
    "objectID": "1_01_design_and_data.html#worked-example",
    "href": "1_01_design_and_data.html#worked-example",
    "title": "Research design & data",
    "section": "\n3 Worked example",
    "text": "3 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\n\n\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows by default the top 6 rows of the data. Use the n = ... option to change the default behaviour, e.g. head(<data>, n = 10).\n\ndim(tips)\n\n[1] 157   7\n\n\n\n\nThis returns the number of rows and columns\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <chr> \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server <chr> \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatives to glimpse are the data “structure” function:\n\nstr(tips)\n\nspc_tbl_ [157 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Bill  : num [1:157] 23.7 36.1 32 17.4 15.4 ...\n $ Tip   : num [1:157] 10 7 5.01 3.61 3 2.5 3.44 2.42 3 2 ...\n $ Credit: chr [1:157] \"n\" \"n\" \"y\" \"y\" ...\n $ Guests: num [1:157] 2 3 2 2 2 2 2 2 2 2 ...\n $ Day   : chr [1:157] \"f\" \"f\" \"f\" \"f\" ...\n $ Server: chr [1:157] \"A\" \"B\" \"A\" \"B\" ...\n $ PctTip: num [1:157] 42.2 19.4 15.7 20.8 19.5 13.4 16 12.4 12.7 10.7 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Bill = col_double(),\n  ..   Tip = col_double(),\n  ..   Credit = col_character(),\n  ..   Guests = col_double(),\n  ..   Day = col_character(),\n  ..   Server = col_character(),\n  ..   PctTip = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nor:\n\nsapply(tips, data.class)\n\n       Bill         Tip      Credit      Guests         Day      Server \n  \"numeric\"   \"numeric\" \"character\"   \"numeric\" \"character\" \"character\" \n     PctTip \n  \"numeric\" \n\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nA dataset containing records on 7 variables related to tipping was obtained from https://uoepsy.github.io/data/RestaurantTips.csv, and was provided by the owner of a bistro in the US interested in studying which factors affected the tipping behaviour of the bistro’s customers. The data contains measurements for a total of 157 parties on four numeric variables: size of the bill (in dollars), size of the tip, number of guests in the group, and tip as a percentage of the bill total. The data also includes three categorical variables indicating whether or not the party paid with a credit card, the day of the week, as well as a server-specific identifier.\n\n\n\nsummary(tips)\n\n      Bill            Tip            Credit              Guests     \n Min.   : 1.66   Min.   : 0.250   Length:157         Min.   :1.000  \n 1st Qu.:15.19   1st Qu.: 2.075   Class :character   1st Qu.:2.000  \n Median :20.22   Median : 3.340   Mode  :character   Median :2.000  \n Mean   :22.73   Mean   : 3.807                      Mean   :2.096  \n 3rd Qu.:28.84   3rd Qu.: 5.000                      3rd Qu.:2.000  \n Max.   :70.51   Max.   :15.000                      Max.   :7.000  \n                 NA's   :1                                          \n     Day               Server              PctTip      \n Length:157         Length:157         Min.   :  6.70  \n Class :character   Class :character   1st Qu.: 14.30  \n Mode  :character   Mode  :character   Median : 16.20  \n                                       Mean   : 17.89  \n                                       3rd Qu.: 18.20  \n                                       Max.   :221.00  \n                                                       \n\n\n\n\nsummary returns a quick summary of the data.\nYou probably won’t understand some parts of the output above, but we will learn more in the coming weeks, so don’t worry too much about it. For the moment, you should be able to understand the minimum, maximum, and the mean.\nCurrently, it is not showing very informative output for the categorical variables.\nWe can replace each factor level with a clearer label:\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nUsing tidyverse, the function mutate is used to mutate a variable (column) in the data:\n\ntips <- tips %>%\n    mutate(\n        Day = factor(Day,\n                     levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                     labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")),\n        Credit = factor(Credit,\n                        levels = c(\"n\", \"y\"),\n                        labels = c(\"No\", \"Yes\")),\n        Server = factor(Server)\n    )\n\nThe functions %>% and mutate are part of the tidyverse package. The former, %>%, is called pipe.\nThe pipe works by taking what’s on the left and passing it to the operation on the right. For example, rounding to 2 decimal places the logarithm of the whole numbers from 1 to 10:\n\nround(log(1:10), digits = 2)\n\n [1] 0.00 0.69 1.10 1.39 1.61 1.79 1.95 2.08 2.20 2.30\n\n\nis equivalent to:\n\n1:10 %>%\n    log() %>%\n    round(digits = 2)\n\n [1] 0.00 0.69 1.10 1.39 1.61 1.79 1.95 2.08 2.20 2.30\n\n\n\n\n\nLet’s check the result of the changes to the variable types:\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <fct> No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <fct> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server <fct> A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip      \n A:60   Min.   :  6.70  \n B:65   1st Qu.: 14.30  \n C:32   Median : 16.20  \n        Mean   : 17.89  \n        3rd Qu.: 18.20  \n        Max.   :221.00  \n                        \n\n\n\n\nAfter making categorical variables factors, summary shows the count of each category for the categorical variables.\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips[tips$PctTip > 100, ]\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  <dbl> <dbl> <fct>   <dbl> <fct>    <fct>   <dbl>\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively, using tidyverse, the function filter is used to only filter the rows that satisfy a condition:\n\ntips %>% \n    filter(PctTip > 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  <dbl> <dbl> <fct>   <dbl> <fct>    <fct>   <dbl>\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\nWith a bill of 49.59, the tip would be 109.59 dollars:\n\n49.59 * 221 / 100\n\n[1] 109.5939\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip > 100] <- NA\n\n\n\na > b tests whether a is greater than b. a < b tests whether a is smaller than b. a == b tests whether a is equal to b; notice the double equal sign! You can also use >= or <=\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively you can use tidyverse:\n\ntips <- tips %>%\n    mutate(\n        PctTip = ifelse(PctTip > 100, NA, PctTip)\n    )\n\nWhere the function ifelse selects a value depending on a condition to test: ifelse(test, value_if_true, value_if_false). In the case above, each value in the column PctTip is replaced by NA if Pct > 100, and it is kept the same otherwise.\n\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip     \n A:60   Min.   : 6.70  \n B:65   1st Qu.:14.30  \n C:32   Median :16.15  \n        Mean   :16.59  \n        3rd Qu.:18.05  \n        Max.   :42.20  \n        NA's   :1      \n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe average bill size was $22.73, and the average tip was $3.85, corresponding to roughly 17% of the total bill. Out of 157 parties, only 51 paid with a credit card. Most parties tended to be of around 2 people each, and people tended to go to that restaurant more often on Wednesday. Among the three servers, server C was the one that served the least number of parties. The data also included a missing tipping value, corresponding to a bill $49.59, and a data inputting error for the corresponding measure of the tip as a percentage of the total bill."
  },
  {
    "objectID": "1_01_design_and_data.html#student-glossary",
    "href": "1_01_design_and_data.html#student-glossary",
    "title": "Research design & data",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, create a glossary of R functions. You can do so by opening Microsoft Word, Excel, or OneNote and creating a table with two columns: one where you should write the name of an R function, and the other column where you should provide a brief description of what the function does.\nThis “do it yourself” glossary is an opportunity for you to revise what you have learned in today’s lab and write down a few take-home messages. You will find this glossary handy as a reference to keep next to you when you will be doing the assessed weekly quizzes.\nBelow you can find an example to get you started:\n\n\n\n\n\n\nFunction\nUse and package\n\n\n\nread_csv\nFor reading comma separated value files. Part of tidyverse package\n\n\nView\n?\n\n\nhead\n?\n\n\nnrow\n?\n\n\nncol\n?\n\n\ndim\n?\n\n\nglimpse\n?\n\n\nstr\n?\n\n\nsummary\n?\n\n\nfactor\n?"
  },
  {
    "objectID": "1_02_categorical_data.html",
    "href": "1_02_categorical_data.html",
    "title": "Categorical data",
    "section": "",
    "text": "In the first five weeks of the course your group should produce a PDF report using Rmarkdown, to be submitted at the end of week 5. You will receive formative feedback on your submission in week 6.\nThe submitted report should be a PDF file of 4 pages at most. In week 5, you can add an Appendix in which you will collate all the R code in a chunk with the setting results = 'hide', which does not count towards the page limit.\nThe report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor formative report A, please only focus on the variables Movie to Year, ignoring anything beyond that. In other words, do not analyse the variables IQ1 to PrivateTransport in the next five weeks of the course, we will use those later in the course.\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n(Ignore for now) IQ1-IQ50: IQ score of each of 50 audience raters\n(Ignore for now) Snacks: How many of the 50 audience raters brought snacks\n(Ignore for now) PrivateTransport: How many of the 50 audience raters reached the cinema via private transportation\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.This week you will only focus on task A2. In the next section you will find some guided sub-steps you may want to consider to complete task A2.\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\n\n\n\n\n\n\nThis week’s task\n\n\n\nA2) Display and describe the categorical variables\n\n\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task A2.\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nSelecting a subset of columns\n\n\n\n\n\nConsider a table of toy data comprising a participant identifier (id: 1 to 5), the participant age, the course (A or B) they are enrolled into, and their height:\n\ntoy_data <- tibble(\n    id = 1:5,\n    age = c(18, 20, 25, 22, 19),\n    course = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n    height = c(171, 180, 168, 193, 174)\n)\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  <int> <dbl> <chr>   <dbl>\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo select the first two columns, you can either say the range from:to using numbers or the names of the columns:\n\ntoy_data %>%\n    select(1:3)\n\n# A tibble: 5 × 3\n     id   age course\n  <int> <dbl> <chr> \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\n\ntoy_data %>%\n    select(id:course)\n\n# A tibble: 5 × 3\n     id   age course\n  <int> <dbl> <chr> \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nHowever, if you check the data in toy_data, those didn’t change. The result of the above computation was only printed to the screen but not stored.\n\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  <int> <dbl> <chr>   <dbl>\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo store it, we need to assign the result:\n\ntoy_data <- toy_data %>%\n    select(id:course)\n\n\ntoy_data\n\n# A tibble: 5 × 3\n     id   age course\n  <int> <dbl> <chr> \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nBy doing the above, we have overwritten the data stored in toy_data with the selected columns.\n\n\n\n\nOverwrite the data to only include the first 15 variables (i.e. columns).2\nCreate a plot displaying the frequency distribution of movie genres.3\n\n\nCreate a plot displaying the frequency distribution of the lead studios.4\nWould it make sense to create a plot of the frequency distribution of movie names?5\n\n\n\n\n\n\n\nTip\n\n\n\nBefore applying a function to your data, you should always ask yourself if what you are about to do is going to convey any insight about the data, compared to just looking at the data itself.\nThe goal of data analysis is to to go from a multitude of values to insights that provide actionable information from a quick glance.\n\n\n\nDescribe the distribution of movie genres. You may want to include both the frequency and the percentage frequency.6\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nConsider this code:\n\ntoy_data %>%\n    count(course) %>%\n    mutate(\n        perc = round(n / sum(n) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course     n  perc\n  <chr>  <int> <dbl>\n1 A          3    60\n2 B          2    40\n\n\nYou can change the names of the frequency from n to Freq and perc to Perc using:\n\ntoy_data %>%\n    count(course, name = \"Freq\") %>%\n    mutate(\n        Perc = round(Freq / sum(Freq) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course  Freq  Perc\n  <chr>  <int> <dbl>\n1 A          3    60\n2 B          2    40\n\n\nAn alternative to the above involves using the group_by(), summarise(), and n() functions from tidyverse. The n() function counts the number of values, and we use it inside summarise() because we are summarising the data with a number. Before summarise, we use group_by(course) to tell R to do the computation for each unique course entry, i.e. for each group of rows defined by course.\n\ntoy_data %>%\n    group_by(course) %>%\n    summarise(\n        Freq = n()\n    ) %>%\n    mutate(\n        Perc = round(Freq / sum(Freq) * 100, 2)\n    )\n\n# A tibble: 2 × 3\n  course  Freq  Perc\n  <chr>  <int> <dbl>\n1 A          3    60\n2 B          2    40\n\n\n\n\n\n\nDescribe the distribution of lead studios. You may want to include both the frequency and the percentage frequency.7\nWhat is the most common genre and the most common lead studio?8"
  },
  {
    "objectID": "1_02_categorical_data.html#worked-example",
    "href": "1_02_categorical_data.html#worked-example",
    "title": "Categorical data",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n Variable Name \n    Description \n  \n\n\n Bill \n    Size of the bill (in dollars) \n  \n\n Tip \n    Size of the tip (in dollars) \n  \n\n Credit \n    Paid with a credit card? n or y \n  \n\n Guests \n    Number of people in the group \n  \n\n Day \n    Day of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday \n  \n\n Server \n    Code for specific waiter/waitress: A, B, or C \n  \n\n PctTip \n    Tip as a percentage of the bill \n  \n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\n\n\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows the top 6 rows of data. Use the n = ... option to change the default behaviour, e.g. head(<data>, n = 10).\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <chr> \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server <chr> \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package and is used to check the type of each variable.\nWe can use better labels for the categorical variables:\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <fct> No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <fct> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server <fct> A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nThe categorical variable course in toy_data with levels “A” and “B” can have the values relabelled to “Year1” and “Year2” via:\ntoy_data$course <- factor(\n  toy_data$course, \n  levels = c(\"A\", \"B\"), \n  labels = c(\"Year1\", \"Year2\")\n)\nLast week, we also saw that if someone tipped more than 100% of the bill size, it was likely a data input error and we decided to replace that value with NA (not available):\n\n\nThe mutate function takes as arguments:\n\ncolumn name\n\n=\n\nhow to compute that column\n\n\nThe syntax for ifelse is:\nifelse(test_condition, \n       value_if_true, \n       avalue_if_false)\n\n\ntips <- tips %>%\n    mutate(\n        PctTip = ifelse(PctTip > 100, NA, PctTip)\n    )\n\nThis displays the frequency distribution of credit card payers:\n\nplt_credit <- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\", y = \"Count\")\nplt_credit\n\n\n\n\nYou can even flip the coordinates, if you wish to, using the coord_flip() function:\n\nggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\") +\n    coord_flip()\n\n\n\n\nYou can use the patchwork package to place graphs side by side. Simply create an object for each graph, and concatenate the objects with | for horizontal concatenation and / for vertical concatenation of graphs. You can even combine this by using parentheses, e.g. (plot1 | plot2) / (plot3 | plot4) for 2 rows and 2 columns.\n\n\nRun install.packages(\"patchwork\") first in your R console\nWe can display the frequency distribution of all the categorical variables: Credit, Day, and Server:\n\n\n\n\n\n\nRotate x-axis labels\n\n\n\n\n\nTo rotate x-axis labels by 90 degrees, you can use this code:theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\nTo rotate the labels by 45 degrees, you can use: theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\nDon’t worry, no one remembers it. People always google “rotate x-axis labels ggplot” to find it.\n\n\n\n\nlibrary(patchwork)\n\nplt1 <- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paird by credit card?\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt2 <- ggplot(tips, aes(x = Day)) +\n    geom_bar() +\n    labs(x = \"Day of week\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt3 <- ggplot(tips, aes(x = Server)) +\n    geom_bar() +\n    labs(y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt1 | plt2 | plt3\n\n\n\n\nA frequency table can be obtained using:\n\ntbl_credit <- tips %>%\n    count(Credit) %>%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    )\ntbl_credit\n\n# A tibble: 2 × 3\n  Credit     n  perc\n  <fct>  <int> <dbl>\n1 No       106  67.5\n2 Yes       51  32.5\n\n\n\ntbl_day <- tips %>%\n    count(Day) %>%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    )\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n  perc\n  <fct>     <int> <dbl>\n1 Monday       20 12.7 \n2 Tuesday      13  8.28\n3 Wednesday    62 39.5 \n4 Thursday     36 22.9 \n5 Friday       26 16.6 \n\n\n\ntbl_server <- tips %>%\n    count(Server) %>%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    )\ntbl_server\n\n# A tibble: 3 × 3\n  Server     n  perc\n  <fct>  <int> <dbl>\n1 A         60  38.2\n2 B         65  41.4\n3 C         32  20.4\n\n\nYou can create nice tables with the kbl command from the kableExtra package.\n\n\nRun install.packages(\"kableExtra\") first in your R console\n\nlibrary(kableExtra)\n\nkbl(list(tbl_credit, tbl_day, tbl_server), booktabs = TRUE)\n\n\n\n\nFrequency tables of categorical variables\n\n\n\n\n\nPaid with a credit card \n \n Credit \n    n \n    perc \n  \n\n\n No \n    106 \n    67.52 \n  \n\n Yes \n    51 \n    32.48 \n  \n\n\n\n\n\n\nDay of the week \n \n Day \n    n \n    perc \n  \n\n\n Monday \n    20 \n    12.74 \n  \n\n Tuesday \n    13 \n    8.28 \n  \n\n Wednesday \n    62 \n    39.49 \n  \n\n Thursday \n    36 \n    22.93 \n  \n\n Friday \n    26 \n    16.56 \n  \n\n\n\n\n\n\nServer \n \n Server \n    n \n    perc \n  \n\n\n A \n    60 \n    38.22 \n  \n\n B \n    65 \n    41.40 \n  \n\n C \n    32 \n    20.38 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nArranging by descending frequency\n\n\n\n\n\nAdd arrange(desc(column_of_freq)). For example:\n\ntbl_day <- tips %>%\n    count(Day) %>%\n    mutate(\n        perc = round((n / sum(n)) * 100, 2)\n    ) %>%\n    arrange(desc(n))\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n  perc\n  <fct>     <int> <dbl>\n1 Wednesday    62 39.5 \n2 Thursday     36 22.9 \n3 Friday       26 16.6 \n4 Monday       20 12.7 \n5 Tuesday      13  8.28\n\n\n\n\n\n\n\n\n\n\n\nRename the frequency and percent columns\n\n\n\n\n\nAdd arrange(desc(column_of_freq)). For example:\n\ntbl_day <- tips %>%\n    count(Day, name = \"Freq\") %>%\n    mutate(\n        Perc = round((Freq / sum(Freq)) * 100, 2)\n    ) %>%\n    arrange(desc(Freq))\ntbl_day\n\n# A tibble: 5 × 3\n  Day        Freq  Perc\n  <fct>     <int> <dbl>\n1 Wednesday    62 39.5 \n2 Thursday     36 22.9 \n3 Friday       26 16.6 \n4 Monday       20 12.7 \n5 Tuesday      13  8.28\n\n\n\n\n\nFrom the univariate distribution (or marginal distribution) of each categorical variable we see that the most common payment method was not a credit card, and the most common day of the week to dine at that restaurant was Wednesday. Finally, most parties were waited on by server B.\n\n\nThe most common value is the mode."
  },
  {
    "objectID": "1_02_categorical_data.html#student-glossary",
    "href": "1_02_categorical_data.html#student-glossary",
    "title": "Categorical data",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\nfactor\n?\n\n\n%>%\n?\n\n\ngeom_bar\n?\n\n\nlabs\n?\n\n\ncount\n?\n\n\nmutate\n?\n\n\nsum\n?\n\n\nround\n?\n\n\ncoord_flip\n?\n\n\nkbl\n?\n\n\narrange\n?\n\n\ndesc\n?"
  },
  {
    "objectID": "1_03_numeric_data.html",
    "href": "1_03_numeric_data.html",
    "title": "Numeric data",
    "section": "",
    "text": "Instructions Recap - Formative Report A\n\n\n\n\n\n\nIn the first five weeks of the course your group should produce a PDF report using Rmarkdown, to be submitted at the end of week 5. You will receive formative feedback on your submission in week 6.\nThe submitted report should be a PDF file of 4 pages at most. In week 5, you can add an Appendix in which you will collate all the R code in a chunk with the setting results = 'hide', which does not count towards the page limit.\nThe report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\n\n\n\n\n\n\n\nFor formative report A, please only focus on the variables Movie to Year, ignoring anything beyond that. In other words, do not analyse the variables IQ1 to PrivateTransport in the next five weeks of the course, we will use those later in the course.\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n(Ignore for now) IQ1-IQ50: IQ score of each of 50 audience raters\n(Ignore for now) Snacks: How many of the 50 audience raters brought snacks\n(Ignore for now) PrivateTransport: How many of the 50 audience raters reached the cinema via private transportation"
  },
  {
    "objectID": "1_03_numeric_data.html#tasks",
    "href": "1_03_numeric_data.html#tasks",
    "title": "Numeric data",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.This week you will only focus on task A3. In the next section you will find some guided sub-steps you may want to consider to complete task A3.\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\n\n\n\n\n\n\nThis week’s task\n\n\n\nA3) Display and describe six numerical variables of your choice\n\n\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback"
  },
  {
    "objectID": "1_03_numeric_data.html#a3-sub-tasks",
    "href": "1_03_numeric_data.html#a3-sub-tasks",
    "title": "Numeric data",
    "section": "\n2 A3 sub-tasks",
    "text": "2 A3 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task A3.\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nVisualising the distribution of numerical data\n\n\n\n\n\nWe display numeric variables with histograms, density plots, or boxplots. Respectively, these use the function geom_histogram(), geom_density(), or geom_boxplot() from ggplot2, which is a package automatically loaded when you load tidyverse via library(tidverse). For illustration purposes, we will use the starwars dataset from tidyverse, containing information on Starwars characters.\n\nlibrary(tidyverse)\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       <chr> \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     <int> 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       <dbl> 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color <chr> \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color <chr> \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  <chr> \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year <dbl> 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        <chr> \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     <chr> \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  <chr> \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    <chr> \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      <list> <\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   <list> <\"Snowspeeder\", \"Imperial Speeder Bike\">, <>, <>, <>, \"Imp…\n$ starships  <list> <\"X-wing\", \"Imperial shuttle\">, <>, <>, \"TIE Advanced x1\",…\n\n\n\n\nHistogram\nDensity plot\nBoxplot\n\n\n\nThe distribution of the character heights (cm) can be displayed with a histogram:\n\nggplot(starwars, aes(x = height)) +\n    geom_histogram(color = 'gray', fill = 'lightblue') +\n    labs(x = \"Character height (cm)\", y = \"Frequency\")\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a density plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_density(color = 'dodgerblue') +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a box plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_boxplot() +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\nCreate six plots, each displaying the distribution of:2\n\nProduction budgets\nAudience scores\nRotten Tomatoes ratings\nWorld gross income\nForeign gross income\nYear of movie release\n\n\n\n\nArrange the above plots as a single figure comprising 2 by 3 panels3\n\n\n\n\n\n\n\n\nCompute the mean and standard deviation of a variable\n\n\n\n\n\nConsider again the starwars dataset. The mean and SD of the height variable can be computed as:\n\nstarwars %>%\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) %>%\n    round(digits = 2)\n\n# A tibble: 1 × 2\n      M    SD\n  <dbl> <dbl>\n1  174.  34.8\n\n\nTo make a nice table for the PDF document, you can use the kbl() function from the kableExtra package:\n\nlibrary(kableExtra)\n\nstarwars %>%\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) %>%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\n M \n    SD \n  \n\n 174.36 \n    34.77 \n  \n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises the production budgets using the mean and standard deviation.4\n\n\n\n\n\n\n\n\nTable of descriptive statistics\n\n\n\n\n\nUsing summarise for more than a couple of variables would make the job very tedious and long. There is a shortcut, which uses the describe function from the psych package.\nThe following code creates a table of descriptive statistics (via the describe function from the psych package) and ensures the table is in proper format by using the kbl function from the kableExtra package.\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars %>%\n    select(height, mass) %>%\n    describe() %>%\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\n   \n    vars \n    n \n    mean \n    sd \n    median \n    trimmed \n    mad \n    min \n    max \n    range \n    skew \n    kurtosis \n    se \n  \n\n\n height \n    1 \n    81 \n    174.36 \n    34.77 \n    180 \n    178.17 \n    19.27 \n    66 \n    264 \n    198 \n    -1.03 \n    1.78 \n    3.86 \n  \n\n mass \n    2 \n    59 \n    97.31 \n    169.46 \n    79 \n    75.44 \n    16.31 \n    15 \n    1358 \n    1343 \n    6.97 \n    48.93 \n    22.06 \n  \n\n\n\n\nTo only show the columns n, mean, sd, median you can use:\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars %>%\n    select(height, mass) %>%\n    describe() %>%\n    select(n, mean, sd, median) %>%\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\n   \n    n \n    mean \n    sd \n    median \n  \n\n\n height \n    81 \n    174.36 \n    34.77 \n    180 \n  \n\n mass \n    59 \n    97.31 \n    169.46 \n    79 \n  \n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises (using the mean and standard deviation) the six numeric variables which you plotted above. 5\n\n\n\nWrite up a summary of what you have reported in the table, using proper rounding to 2 decimal places and avoiding any reference to R code or functions.\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\nMake sure that all R code and output is not visible in the PDF report. The PDF report should only include text, tables and plots."
  },
  {
    "objectID": "1_03_numeric_data.html#worked-example",
    "href": "1_03_numeric_data.html#worked-example",
    "title": "Numeric data",
    "section": "\n3 Worked example",
    "text": "3 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n Variable Name \n    Description \n  \n\n\n Bill \n    Size of the bill (in dollars) \n  \n\n Tip \n    Size of the tip (in dollars) \n  \n\n Credit \n    Paid with a credit card? n or y \n  \n\n Guests \n    Number of people in the group \n  \n\n Day \n    Day of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday \n  \n\n Server \n    Code for specific waiter/waitress: A, B, or C \n  \n\n PctTip \n    Tip as a percentage of the bill \n  \n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <chr> \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server <chr> \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <fct> No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <fct> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server <fct> A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\ntips <- tips %>%\n    mutate(\n        PctTip = ifelse(PctTip > 100, NA, PctTip)\n    )\n\nWe can create a histogram of tips via:\n\nggplot(tips, aes(x = Bill)) + \n    geom_histogram(color = 'white') + \n    labs(x = \"Size of the tip (US dollars)\")\n\n\n\n\nWe can create a single figure with the distribution of all numeric variables by using the patchwork package:\n\nlibrary(patchwork)\n\npltBill <- ggplot(tips, aes(x = Bill)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Size of the bill (US dollars)\")\n\npltTip <- ggplot(tips, aes(x = Tip)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Size of the tip (US dollars)\")\n\npltGuests <- ggplot(tips, aes(x = Guests)) +\n    geom_bar(fill = 'lightblue') +\n    labs(x = \"Number of people in the group\")\n\npltPctTip <- ggplot(tips, aes(x = PctTip)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Tip as a percentage of the bill\")\n\n(pltBill | pltTip) / (pltGuests | pltPctTip)\n\n\n\n\nTo summarise one numeric variable, you can use the summarise function from tidyverse, which takes the data and computes a numeric summary. The syntax is:\ndata %>%\n    summarise(\n        column_name = computation\n    )\nThis computes the mean and SD of tip size (in US dollars):\n\ntips %>%\n    summarise(\n        M = mean(Tip, na.rm = TRUE),\n        SD = sd(Tip, na.rm = TRUE)\n    ) %>%\n    kbl(digits = 2, booktabs = TRUE)\n\n\n\n\n\n\n M \n    SD \n  \n\n 3.81 \n    2.37 \n  \n\n\n\nTo summarise all of the numeric variables into a single table of descriptive statistics you can use the describe function from the psych package:\n\nlibrary(kableExtra)\nlibrary(psych)\n\ntips %>%\n    select(Bill, Tip, Guests, PctTip) %>%\n    describe() %>%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\n   \n    vars \n    n \n    mean \n    sd \n    median \n    trimmed \n    mad \n    min \n    max \n    range \n    skew \n    kurtosis \n    se \n  \n\n\n Bill \n    1 \n    157 \n    22.73 \n    12.16 \n    20.22 \n    21.37 \n    11.03 \n    1.66 \n    70.51 \n    68.85 \n    1.22 \n    1.89 \n    0.97 \n  \n\n Tip \n    2 \n    156 \n    3.81 \n    2.37 \n    3.34 \n    3.50 \n    1.99 \n    0.25 \n    15.00 \n    14.75 \n    1.50 \n    3.13 \n    0.19 \n  \n\n Guests \n    3 \n    157 \n    2.10 \n    0.93 \n    2.00 \n    1.98 \n    0.00 \n    1.00 \n    7.00 \n    6.00 \n    2.22 \n    7.81 \n    0.07 \n  \n\n PctTip \n    4 \n    156 \n    16.59 \n    4.39 \n    16.15 \n    16.25 \n    2.74 \n    6.70 \n    42.20 \n    35.50 \n    2.50 \n    12.39 \n    0.35 \n  \n\n\n\n\nTo only keep the sample size, mean, SD, median, min, max, we use the select function from tidyverse:\n\nlibrary(kableExtra)\nlibrary(psych)\n\ntips %>%\n    select(Bill, Tip, Guests, PctTip) %>%\n    describe() %>%\n    select(n, mean, sd, median, min, max) %>%\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\n   \n    n \n    mean \n    sd \n    median \n    min \n    max \n  \n\n\n Bill \n    157 \n    22.73 \n    12.16 \n    20.22 \n    1.66 \n    70.51 \n  \n\n Tip \n    156 \n    3.81 \n    2.37 \n    3.34 \n    0.25 \n    15.00 \n  \n\n Guests \n    157 \n    2.10 \n    0.93 \n    2.00 \n    1.00 \n    7.00 \n  \n\n PctTip \n    156 \n    16.59 \n    4.39 \n    16.15 \n    6.70 \n    42.20 \n  \n\n\n\n\n\n\n\n\n\n\nUse the appropriate summary for each variable type\n\n\n\nEnsure that you summarise variables correctly:\n\nFor categorical variables use frequency tables\nFor continuous variables use a table of descriptive statistics (mean, SD, Median, etc.)\n\nYou should not summarise categorical variables with the mean, SD, and this is why it’s important to use select() before describe() to only keep the variables that are continuous.\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe distributions of bill size, tip size, and group size are skewed to the right. The distribution of tips, as a percentage of the total bill, appears to be approximately bell shaped, with three outliers in the right tail of the distribution.\nThe average bill was about $22.73, with a SD of $12.16. The average tip was $3.81, with a SD of $2.37, corresponding to an average tip as a percentage of the total bill of $16.59, with a SD of $4.39. The average party size comprised 2 guests, with a SD of roughly 1 person."
  },
  {
    "objectID": "1_03_numeric_data.html#student-glossary",
    "href": "1_03_numeric_data.html#student-glossary",
    "title": "Numeric data",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_density\n?\n\n\ngeom_boxplot\n?\n\n\npatchwork: | and /\n\n?\n\n\nsummarise\n?\n\n\nselect\n?\n\n\nkbl\n?\n\n\ndescribe\n?"
  },
  {
    "objectID": "1_04_relationships.html",
    "href": "1_04_relationships.html",
    "title": "Relationships",
    "section": "",
    "text": "Formative report A\n\n\n\n\n\nIn the first five weeks of the course your group should produce a PDF report using Rmarkdown, to be submitted at the end of week 5. You will receive formative feedback on your submission during the labs of week 6.\nThe submitted report should be a PDF file of 4 pages at most. In week 5, you can add an Appendix in which you will collate all the R code in a chunk with the setting results = 'hide', which does not count towards the page limit.\nThe report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file."
  },
  {
    "objectID": "1_04_relationships.html#tasks",
    "href": "1_04_relationships.html#tasks",
    "title": "Relationships",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.This week you will only focus on task A4. In the next section you will find some guided sub-steps you may want to consider to complete task A4.\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\n\n\n\n\n\n\nThis week’s task\n\n\n\nA4) Display and describe a relationship of interest between two or three variables of your choice\n\n\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback"
  },
  {
    "objectID": "1_04_relationships.html#a4-sub-tasks",
    "href": "1_04_relationships.html#a4-sub-tasks",
    "title": "Relationships",
    "section": "\n2 A4 sub-tasks",
    "text": "2 A4 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task A4.\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nChoose two variables and create a plot displaying their relationship.2\n\n\nSummarise the relationship with either a table or a number, depending on the type of the variables.3\n\n\n\n\nChoose a third variable and visualise how the relationship above varies across the third variable.\n\nIf you have chosen a categorical-numeric relationship, choose a categorical variable for the third variable\n\nIf you have chosen a numeric-numeric relationship, choose a categorical variable for the third variable\n\nIf you have chosen a categorical-categorical relationship, choose a numeric variable for the third variable and invert the order of one categorical variable with the numeric one\n\n\nSummarise with table how the relationship above varies across the third variable.4\n\nOrganise your report to have three sections:\n\n\nIntroduction: where you write a concise introduction to the data for a generic reader\n\nAnalysis: where you present all your tables and plots\n\nDiscussion: where you write take-home messages about the data and the insights you discovered\n\n\nKnit the report to PDF, making sure that only text, tables, and plots are visible. Hide the R code chunks so that no R code is visible.\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_04_relationships.html#worked-example",
    "href": "1_04_relationships.html#worked-example",
    "title": "Relationships",
    "section": "\n3 Worked example",
    "text": "3 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\n\n\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWe can replace each factor level with a clearer label:\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips %>% \n    filter(PctTip > 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  <dbl> <dbl> <fct>   <dbl> <fct>    <fct>   <dbl>\n1  49.6    NA Yes         4 Thursday C         221\n\n\nWith a bill of 49.59, the tip would be 109.59 dollars:\n\n49.59 * 221 / 100\n\n[1] 109.5939\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip > 100] <- NA\n\nConsider, for example, the relationship between bill and tip size. As these are two numerical variables, we visualise the relationship with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\")\n\n\n\nFigure 1: Scatterplot displaying the relationship between bill and tip size\n\n\n\n\n\n\nThe code above first sets up a blank canvas for plotting the dataset tip, and placing on the x axis the variable Bill and on the y axis the variable Tip:\nggplot(tips, aes(x = Bill, y = Tip))\nThe following line adds a geometric shape to the plot, in this case points:\ngeom_point()\nThe final line uses more informative labels for the reader, setting a label for the x and y axis respectively:\nlabs(x = \"Bill size (in US dollars)\", \n     y = \"Tip size (in US dollars)\")\nThe layers of the plot need to be added to each other with a + symbol at the end of each line, excluding the last one.\nWe can numerically summarise this relationship with the covariance between the two variables:\n\ncov(tips$Bill, tips$Tip)\n\n[1] NA\n\n\nThere are missing values, so the covariance cannot be computed. To fix this, we use the option use = \"pairwise.complete.obs\" to tell R to only keep the complete pairs to compute the covariance, i.e. ignoring pairs where at least one number is NA:\n\nround(cov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\"), digits = 2)\n\n[1] 25.96\n\n\nTo investigate the relationship between bill and tip size for those who paid by credit card and those who didn’t we can create faceted scatterplots:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit)\n\n\n\n\nWe can improve the labelling by using labeller = \"label_both\", which displays not only the group value as label, but both the variable and value:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\")\n\n\n\nFigure 2: Relationship between bill and tip size by paying method\n\n\n\n\nSimilarly, you can compute grouped covariances via:\n\nlibrary(kableExtra)\ntips %>%\n    group_by(Credit) %>%\n    summarise(COV = cov(Bill, Tip, use = \"pairwise.complete.obs\")) %>% \n    kbl(digits = 2, booktabs = TRUE)\n\n\n\n\n\n\nTable 1:  Relationship between bill and tip size by credit card usage \n \n Credit \n    COV \n  \n\n\n No \n    15.38 \n  \n\n Yes \n    37.68 \n  \n\n\n\n\n\n\n\n\n\n\n\nExample write-up\n\n\n\nFigure 1 highlights a strong positive relationship between bill and tip size (in US dollars). The covariance between the two variables is 25.96 squared dollars. The relationship between bill and tip size is stronger for those who paid by credit card than those who did not, as highlighted by Figure 2 and Table 1, where the covariance between the two variables is 37.68 for those that used a credit card and 15.38 for those that did not.\n\n\nRelationships between variables\n\n\nCategorical-Categorical\nNumerical-Categorical\nNumerical-Numerical\n\n\n\nVisualise with a mosaic plot:\n\nlibrary(ggmosaic)\nggplot(tips)+\n    geom_mosaic(aes(x = product(Credit, Server), fill=Credit))\n\n\n\n\nSummarise with a contingency table:\n\ntips %>%\n    select(Credit, Server) %>%\n    table() %>%\n    kbl(booktabs = TRUE)\n\n\n\n\n\n\n   \n    A \n    B \n    C \n  \n\n\n No \n    39 \n    50 \n    17 \n  \n\n Yes \n    21 \n    15 \n    15 \n  \n\n\n\n\n\n\nBoxplot\n\nggplot(tips, aes(x = Credit, y = Tip)) +\n    geom_boxplot()\n\n\n\n\nor grouped histogram\n\nggplot(tips, aes(x = Tip)) +\n    geom_histogram(color='white') +\n    facet_wrap(~Credit)\n\n\n\n\nSummarise via a grouped table of descriptive statistics:\n\ntips %>%\n    group_by(Credit) %>%\n    summarise(N = n(),\n              M = mean(Tip),\n              SD = sd(Tip)) %>%\n    kbl(digits = 2, booktabs = TRUE)\n\n\n\n\n\n\n Credit \n    N \n    M \n    SD \n  \n\n\n No \n    106 \n    3.25 \n    1.93 \n  \n\n Yes \n    50 \n    4.99 \n    2.77 \n  \n\n\n\n\n\n\nVisualise with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point()\n\n\n\n\nSummarise with covariance:\n\ncov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\") %>%\n    round(digits = 2)\n\n[1] 25.96"
  },
  {
    "objectID": "1_04_relationships.html#student-glossary",
    "href": "1_04_relationships.html#student-glossary",
    "title": "Relationships",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_density\n?\n\n\ngeom_boxplot\n?\n\n\ngeom_point\n?\n\n\ngeom_mosaic\n?\n\n\nfacet_wrap\n?\n\n\ngroup_by\n?\n\n\nsummarise\n?\n\n\ncor\n?\n\n\nround\n?"
  },
  {
    "objectID": "1_05_formative_report_a.html",
    "href": "1_05_formative_report_a.html",
    "title": "Formative report A",
    "section": "",
    "text": "This week: Submission of Formative report A\n\n\n\n\n\n\nYour group must submit one PDF file for formative report A by 12 noon on Friday 21 October 2022.\n\nTo submit go to the course Learn page, click “Assessment” from the left-hand side menu, then click “Report submission”, and then “Submit Formative Report A here (PDF file only)”.\n\nOnly one person per group is required to submit on behalf of the entire group. However, to ensure that everyone in the group can see the feedback, please double check that you self-registered in the group on Learn (see group name on the desk).\n\n\nAs mentioned on the Course Information page, there will be no extensions allowed for group-based reports.\nYou will receive formative feedback on your submission during the labs of week 6. Please attend your lab next week!\nThe submitted report should be a PDF file of 4 pages at most.\nYou can add an Appendix in which you will collate all the R code in a chunk with the settings results = 'hide', fig.show = 'hide'. This will not count towards the page limit.\nExcluding the Appendix, the report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge."
  },
  {
    "objectID": "1_05_formative_report_a.html#tasks",
    "href": "1_05_formative_report_a.html#tasks",
    "title": "Formative report A",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.This week you will only focus on task A5. In the next section you will find some guided sub-steps you may want to consider to complete task A5.\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\n\n\n\n\n\n\nThis week’s task\n\n\n\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback"
  },
  {
    "objectID": "1_05_formative_report_a.html#a5-sub-tasks",
    "href": "1_05_formative_report_a.html#a5-sub-tasks",
    "title": "Formative report A",
    "section": "\n2 A5 sub-tasks",
    "text": "2 A5 sub-tasks\nIn this section you will find some guided sub-steps you may want to consider to complete task A5.\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nOrganise the Rmd file to have the following structure:\n\n---\ntitle: \"Formative report A\"\nauthor: \"Group ?.?\"\ndate: \"Write the date here\"\noutput: bookdown::pdf_document2\n---\n\n\nThis is the metadata block. It includes the:\n\ndocument title\nauthor name\ndate (to leave empty, use an empty string \"\")\nthe output type\n\nThe output type could be html_document, pdf_document, etc.\nWe use bookdown::pdf_document2 so that we can reference figures, which pdf_document doesn’t let you do.\nThe code bookdown::pdf_document2 simply means to use the pdf_document2 type from the bookdown package.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)\n```\n\n\nThis is the setup chunk and should always be included in your Rmd document. It sets the global options for all code chunks that will follow.\n\nIf echo=TRUE, the R code in chunks is displayed. If FALSE, not.\nIf message=TRUE, information messages are displayed. If FALSE, not.\nIf warning=TRUE, warning messages are printed. If FALSE, not.\n\nIf you want to change the setting in a specific code chunk, you can do so via:\n```{r, echo=FALSE}\n# A code chunk\n```\n\n```{r, include=FALSE}\n# week 1 code below\nlibrary(tidyverse)\n\n# week 2 code below\npltEye <- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# week 3 code below\n\n# week 4 code below\n```\n\n\nThis code chunks contains your rough work from each week. Give names to plots and tables, so that you can reference those later on. The option include=FALSE hides both code and output.\nTo run each line of code while you are working, put your cursor on the line and press Control + Enter on Windows or Command + Enter on a macOS.\n## Introduction\n\nWrite here an introduction to the data, the variables, and anything worth of notice in the data.\n\n\n## Analysis\n\nPresent here your tables, plots, and results. In the code chunk below, you do \nnot need to put the chunk option `echo=FALSE` as you set this option globally \nin the setup chunk. \n\n```{r}\npltEye\n```\n\nIf you didn't set it globally, you would need to put it in the chunk options:\n\n```{r, echo=FALSE}\npltEye\n```\n\nMore text...\n\n\n## Discussion\n\nWrite up your take home messages here...\n\n\nThis contains your actual textual reporting, as well as tables and figures. To show in place a plot previously created, just include the plot name in a code chunk with the option echo = FALSE to hide the code but display the output.\n## Appendix\n\n```{r, echo=TRUE, results='hide', fig.show='hide'}\n# copy and paste here all your R code\n\n# week 1 code below\nlibrary(tidyverse)\n\n# week 2 code below\npltEye <- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# week 3 code below\n\n# week 4 code below\n```\n\n\nThis allows the marker to see the code you used to obtain your results. Please note that only the code should be visible in the appendix, no output.\nThe chunk options echo=TRUE, results='hide', fig.show='hide' ensure that the appendix code is visible (echo=TRUE), the output is hidden (results=‘hide’), and figures are hidden (fig.show=‘hide’).\nThe appendix does not count towards the 4-page limit.\n\nKnit the document to PDF\n\nSubmit the PDF file on Learn:\n\nGo to the Learn page of the course\nClick Assessments on the left-hand side menu\nClick Report submission\nClick Formative Report A\nFollow the instructions\n\n\n\n\n\n\n\n\n\nReferencing figures\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the figure, in this case short-label but you should use a more descriptive name.\n```{r short-label, fig.cap = \"Figure caption\"}\npltEye <- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\npltEye\n```\n\n\n\n\nFigure caption\n\n\n\n\nTo reference a figure, for example the one above, you would \nwrite see Figure \\@ref(fig:short-label).\nwhich, when you Knit to PDF, becomes:\nTo reference a figure, for example the one above, you would write see Figure 1.\n\n\n\n\n\n\n\n\n\nReferencing tables\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the table, in this case tbl-short-label but you should use a more descriptive name.\n```{r tbl-short-label, echo=FALSE}\nlibrary(kableExtra)\ntblEye <- starwars %>%\n    count(eye_color) %>%\n    kbl(booktabs = TRUE, caption = \"Short table caption\")\ntblEye\n```\n\n\n\n\nShort table caption\n \n eye_color \n    n \n  \n\n\n black \n    10 \n  \n\n blue \n    19 \n  \n\n blue-gray \n    1 \n  \n\n brown \n    21 \n  \n\n dark \n    1 \n  \n\n gold \n    1 \n  \n\n green, yellow \n    1 \n  \n\n hazel \n    3 \n  \n\n orange \n    8 \n  \n\n pink \n    1 \n  \n\n red \n    5 \n  \n\n red, blue \n    1 \n  \n\n unknown \n    3 \n  \n\n white \n    1 \n  \n\n yellow \n    11 \n  \n\n\n\n\nThe table is referenced as, see Table \\@ref(tab:tbl-short-label).\nWhich, when you knit to PDF, is displayed as:\nThe table is referenced as, see Table 1.\nFor details on styling PDF tables, see this link.\n\n\n\n\n\n\n\n\n\nReducing figure size\n\n\n\n\n\nYou could place multiple panels into a single figure using the functions | and / from the patchwork package.\nYou could adjust the figure height and width by playing with a few options for the numbers fig.height = ? and fig.width = ?, for example 5 and 4, or 12 and 8, and so on. Please note this is typically found by trial and error. Keep in mind, however, that the figure labels should still be legible in the plot you show.\n```{r, fig.height = 5, fig.width = 4}\n# your code to display the figure here\n```\n\n\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_05_formative_report_a.html#worked-example",
    "href": "1_05_formative_report_a.html#worked-example",
    "title": "Formative report A",
    "section": "\n3 Worked example",
    "text": "3 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n Variable Name \n    Description \n  \n\n\n Bill \n    Size of the bill (in dollars) \n  \n\n Tip \n    Size of the tip (in dollars) \n  \n\n Credit \n    Paid with a credit card? n or y \n  \n\n Guests \n    Number of people in the group \n  \n\n Day \n    Day of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday \n  \n\n Server \n    Code for specific waiter/waitress: A, B, or C \n  \n\n PctTip \n    Tip as a percentage of the bill \n  \n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\n\n\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWe can replace each factor level with a clearer label:\n\ntips$Day <- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit <- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server <- factor(tips$Server)\n\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips %>% \n    filter(PctTip > 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  <dbl> <dbl> <fct>   <dbl> <fct>    <fct>   <dbl>\n1  49.6    NA Yes         4 Thursday C         221\n\n\nWith a bill of 49.59, the tip would be 109.59 dollars:\n\n49.59 * 221 / 100\n\n[1] 109.5939\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip > 100] <- NA\n\nConsider, for example, the relationship between bill and tip size. As these are two numerical variables, we visualise the relationship with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\")\n\n\n\n\nWe can numerically summarise this relationship with the covariance between the two variables:\n\nround(cov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\"), digits = 2)\n\n[1] 25.96\n\n\nThe relationship looks roughly like a line. You can superimpose a “best-fit” line with the function geom_smooth(method = lm, se = FALSE). The argument method = lm tells to fit a line (in R this is called a linar model, lm), and se = FALSE tells R to not plot the uncertainty bands.\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    geom_smooth(method = lm, se = FALSE)\n\n\n\n\nYou will only learn how to find the functional relationship between two variables in the second-year course DAPR2, so for now I will give it to you:\n\n\n\n\\[\ny = -0.26 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]\nWhat is the predicted tip for a bill of 50 US dollars? Let’s do the computation:\n\n-0.26 + 0.18 * 50\n\n[1] 8.74\n\n\nFrom the plot above, a tip of 8.74 US dollars seems roughly right!\nLet’s find the tips for bills of size 20, 40, 60.\n\ntibble(\n    bills = c(20, 40, 60),\n    tips  = -0.26 + 0.18 * bills\n)\n\n# A tibble: 3 × 2\n  bills  tips\n  <dbl> <dbl>\n1    20  3.34\n2    40  6.94\n3    60 10.5 \n\n\nTo investigate the relationship between bill and tip size for those who paid by credit card and those who didn’t we can create faceted scatterplots:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\")\n\n\n\n\nYou can also fit a best-fit line by payment method:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\") +\n    geom_smooth(method = lm, se = FALSE)\n\n\n\n\nTo extend the lins for the full range of the x-axis, you can use the option fullrange = TRUE:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\") +\n    geom_smooth(method = lm, se = FALSE, fullrange = TRUE)\n\n\n\n\n\n\n\nAgain, you will not know how to find out the functional relationship between the variables within each group until the course DAPR2 in 2nd year, so I will give it to you.\nFor those who did not pay by credit card:\n\\[\ny = -0.17 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]\nFor those who paid by credit card:\n\\[\ny = -0.34 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "1_05_formative_report_a.html#student-glossary",
    "href": "1_05_formative_report_a.html#student-glossary",
    "title": "Formative report A",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_smooth\n?\n\n\ntibble\n?\n\n\nknitr::opts_chunk$set()\n?"
  },
  {
    "objectID": "1_07_prob_theory.html",
    "href": "1_07_prob_theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Are you registered for your group on Learn?\n\n\n\n\n\nGo to the course Learn page, on the left-hand side click “Groups information”, then the lab, and then the group name. Click Sign up.\n\n\n\n\n\n\n\n\n\nChange driver this week\n\n\n\n\n\nIn the next five weeks your group will be creating a new formative report, Formative Report B.\n\nChoose a driver for this week\n\nThe driver should login to the PC provided with the desk, and access RStudio Server\nThe driver is the only person allowed to type the report during this lab\n\n\nThe others in the group are the navigators\n\nNavigators are responsible for suggesting and commenting on the strategy that the driver needs to follow to answer the tasks, as well as correct typos and coding errors.\n\n\nIt is important that your group chooses a driver for this week, and in the next weeks the driver rotates every week to ensure that everyone in the group has contributed to the writing of the report.\n\n\n\n\n\n\n\n\n\n\nCreate a new RMD file\n\n\n\n\n\n\nCreate a new Rmd file for formative report B which you will build upon each week in your group.\nAt the end of each lab, save the Rmd file and share it with your group. If you go to your group area on Learn, you can click “Send Email” to share the file with your group.\n\n\n\n\n\n\n\n\n\n\nFormative report B - Instructions\n\n\n\n\n\n\nIn the next five weeks of the course you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 12.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 2nd of December 2022 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed)\nAppendix B will contain the code to reproduce the report results (just like Formative Report A).\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible.\n\n\n\n\n\n\n\n\n\n\nFormative report B - Data\n\n\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. The following variables were recorded:\n\n\nMovie: Title of the movie\n\nLeadStudio: Primary U.S. distributor of the movie\n\nRottenTomatoes: Rotten Tomatoes rating (critics)\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\nTheatersOpenWeek: Number of screens for opening weekend\n\nOpeningWeekend: Opening weekend gross (in millions)\n\nBOAvgOpenWeekend: Average box office income per theater, opening weekend\n\nBudget: Production budget (in millions)\n\nDomesticGross: Gross income for domestic (U.S.) viewers (in millions)\n\nWorldGross: Gross income for all viewers (in millions)\n\nForeignGross: Gross income for foreign viewers (in millions)\n\nProfitability: WorldGross as a percentage of Budget\n\nOpenProfit: Percentage of budget recovered on opening weekend\n\nYear: Year the movie was released\n\nIQ1-IQ50: IQ score of each of 50 audience raters\n\nSnacks: How many of the 50 audience raters brought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation"
  },
  {
    "objectID": "1_07_prob_theory.html#tasks",
    "href": "1_07_prob_theory.html#tasks",
    "title": "Probability Theory",
    "section": "\n2 Tasks",
    "text": "2 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.This week you will only focus on task B1.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB1) Create and summarise categorical variables, before calculating probabilities.\n\n\nB2) Investigate if events are independent, and compute probabilities.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_07_prob_theory.html#b1-sub-tasks",
    "href": "1_07_prob_theory.html#b1-sub-tasks",
    "title": "Probability Theory",
    "section": "\n3 B1 sub-tasks",
    "text": "3 B1 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B1.\n\nChoose the top 3 most frequent movie genres, and filter your data to only include these rows.1\n\n\n\nCreate a variable called “Rating” where the Audience Score variable is recoded so that those scoring less than or equal to 50 are coded as “Bad” and those scoring over 50 are “Good”.2\n\n\n\nEnsure Rating and Genre are coded as factors.3\n\n\n\nCreate a contingency table displaying how many ratings were good or bad for each of your chosen genres.4\n\n\n\nTransform the table of counts to a relative frequency table.5\n\n\n\nDo the numbers in the table satisfy the requirements of probabilities?6\n\n\n\nInstead of checking the probability requirements manually, add a “sum” column to your relative frequency table.7\n\n\n\nVisualise the relative frequency table as a mosaic plot, making sure to add a main title and clear axis titles.8\n\n\n\nDoes your plot have any NAs? If so, you can drop the missing values before re-plotting.9\n\n\n\nIn the introduction section of your report, write up a small introduction to the data.\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_07_prob_theory.html#worked-example",
    "href": "1_07_prob_theory.html#worked-example",
    "title": "Probability Theory",
    "section": "\n4 Worked Example",
    "text": "4 Worked Example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n Variable Name \n    Description \n  \n\n\n Bill \n    Size of the bill (in dollars) \n  \n\n Tip \n    Size of the tip (in dollars) \n  \n\n Credit \n    Paid with a credit card? n or y \n  \n\n Guests \n    Number of people in the group \n  \n\n Day \n    Day of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday \n  \n\n Server \n    Code for specific waiter/waitress: A, B, or C \n  \n\n PctTip \n    Tip as a percentage of the bill \n  \n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   <dbl> 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    <dbl> 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit <chr> \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests <dbl> 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server <chr> \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip <dbl> 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\nWe can filter our data to only include rows of data from Servers A and B, and save this filtered data to a new dataset called “tips2”.\nBecause we want to include servers A and B, but not C, we can use the != (or does not equal) operator.\n\n\nSome common operators include:\n\n\nOperator\nDescription\n\n\n\n<\nless than\n\n\n>\nmore than\n\n\n<=\nless than or equal to\n\n\n>=\nless than or equal to\n\n\n==\n(only) equal to\n\n\n!=\nnot equal to\n\n\n%in%\nis <left> a member of <right>?\n\n\n\n\ntips2 <- tips %>% \n    filter(Server != \"C\")\n\nConsider if, for example, we wanted to recode the Tip variable so that those tipping less than or equal to 15% were coded as “Below” average, and those tipping over 15% were coded as “Above” average (15% being used as below/above average tips cut-off in relation to usual US tipping rates). To do so, we could create a new variable called “Tip_Avg”.\n\ntips2 <- tips2 %>%\n    mutate(Tip_Avg = ifelse(PctTip <=15, 'Below', 'Above'))\n\ntable(tips2$Tip_Avg)\n\n\nAbove Below \n   80    45 \n\n\nNow that we have the variables we want, it would be a good point to make these both factors:\n\ntips2$Server <- factor(tips2$Server)\ntips2$Tip_Avg <- factor(tips2$Tip_Avg)\n\nTo visually represent the distribution of how many customers were served by each server and if they left a below or above average tip, we could Create a contingency table:\n\nfreq_tbl <- table(tips2$Server, tips2$Tip_Avg)\nfreq_tbl\n\n   \n    Above Below\n  A    40    20\n  B    40    25\n\n\nWe could then transform the table of counts above to instead represent a relative frequency table:\n\nrel_freq_tbl <- freq_tbl %>%\n    prop.table()\nrel_freq_tbl\n\n   \n    Above Below\n  A  0.32  0.16\n  B  0.32  0.20\n\n\nBefore we interpret our results, we must ensure that the numbers above satisfy the requirements of probabilities. We can do this two ways:\n\n\nManually\nUsing addmargins()\n\n\n\nCheck that all values in the proportions table are greater than or equal 0, all values are less than or equal to 1, and they all sum to 1:\n\nall(rel_freq_tbl >= 0)\n\n[1] TRUE\n\n\n\nall(rel_freq_tbl <= 1)\n\n[1] TRUE\n\n\n\nsum(rel_freq_tbl)\n\n[1] 1\n\n\n\n\nInstead of checking manually, we can use the function addmargins() to check that the probabilities sum to 1:\n\nrel_freq_tbl <- freq_tbl %>%\n    prop.table() %>%\n    addmargins()\nrel_freq_tbl\n\n     \n      Above Below  Sum\n  A    0.32  0.16 0.48\n  B    0.32  0.20 0.52\n  Sum  0.64  0.36 1.00\n\n\n\n\n\nIn order to visualise our results in a figure, we could use a mosaic plot:\n\nlibrary(ggmosaic)\nmos_plot <- ggplot(tips2) +\n    geom_mosaic(aes(x = product(Server, Tip_Avg), fill = Server)) +  \n    labs(title = \"Association between Servers and Tips\", x = \"Tip\", y = \"Server\")\nmos_plot\n\n\n\nFigure 3: Association between Servers and Tips\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nMore customers tipped above (64%) than below (36%) average. Both Server A and Server B received an equal distribution of tips above average (32%), but server B had a higher proportion of tips below average (20%) in comparison to server A (16%). These associations are visually represented in Figure 3."
  },
  {
    "objectID": "1_07_prob_theory.html#student-glossary",
    "href": "1_07_prob_theory.html#student-glossary",
    "title": "Probability Theory",
    "section": "\n5 Student Glossary",
    "text": "5 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nfilter\n?\n\n\nmutate\n?\n\n\ndrop_na\n?\n\n\nfactor\n?\n\n\ntable\n?\n\n\nifelse\n?\n\n\nprop.table\n?\n\n\naddmargins\n?\n\n\nall\n?\n\n\n|\n?\n\n\ngeom_mosaic\n?"
  },
  {
    "objectID": "1_08_prob_rules.html",
    "href": "1_08_prob_rules.html",
    "title": "Probability Rules",
    "section": "",
    "text": "Instructions Recap - Formative Report B\n\n\n\n\n\n\nIn this block of the course (weeks 7-11), you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 12.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 2nd of December 2022 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed)\nAppendix B will contain the code to reproduce the report results (just like Formative Report A).\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible."
  },
  {
    "objectID": "1_08_prob_rules.html#tasks",
    "href": "1_08_prob_rules.html#tasks",
    "title": "Probability Rules",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task B2.\nB1) Create and summarise categorical variables, before calculating probabilities.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB2) Investigate if events are independent, and compute probabilities.\n\n\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_08_prob_rules.html#b2-sub-tasks",
    "href": "1_08_prob_rules.html#b2-sub-tasks",
    "title": "Probability Rules",
    "section": "\n2 B2 sub-tasks",
    "text": "2 B2 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B2.\n\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work (i.e., the top 3 most frequent movie genres that were rated as ‘good’ and ‘bad’), and build on it.1\n\n\n\nWhat’s the probability of a movie being rated as Good?2\n\n\n\nGiven that a viewer watched a Drama movie, what’s the probability of them giving a good rating?3\n\n\n\nGiven that a viewer watched an Action movie, what’s the probability of them giving a good rating?\nGiven that a viewer watched a Comedy movie, what’s the probability of them giving a good rating?\nGiven that a viewer gave a bad rating, what’s the probability of them having watched a non-drama movie? 4\n\n\n\n\n\n\n\nAdvanced material\n\n\n\n\n\nConsider three mutually exclusive events \\(A_1, A_2, A_3\\) and another event \\(B\\).\nWe have that:\n\\[\n\\begin{aligned}\nP\\big((\\sim A_3) | B\\big)\n&= P\\big( (A_1 \\cup A_2) | B \\big) \\\\\n&= \\frac{P\\big((A_1 \\cup A_2) \\cap B\\big)}{P(B)} \\\\\n&= \\frac{P\\big((A_1 \\cap B) \\cup (A_2 \\cap B)\\big)}{P(B)} \\\\\n&= \\frac{P(A_1 \\cap B) + P(A_2 \\cap B)}{P(B)}\n\\end{aligned}\n\\]\nSuppose \\(A_1 = Action\\), \\(A_2 = Comedy\\), \\(A_3 = Drama\\), and \\(B = Bad\\).\n\\[\n\\begin{aligned}\nP\\big((\\sim Drama) \\mid Bad\\big)\n&= P\\big( (Action \\cup Comedy) \\mid Bad \\big) \\\\\n&= \\frac{P\\big((Action \\cup Comedy) \\cap Bad\\big)}{P(Bad)} \\\\\n&= \\frac{P\\big((Action \\cap Bad) \\cup (Comedy \\cap Bad)\\big)}{P(Bad)} \\\\\n&= \\frac{P(Action \\cap Bad) + P(Comedy \\cap Bad)}{P(Bad)}\n\\end{aligned}\n\\]\n\n\n\n\nWhat’s the probability of a rater watching a Drama movie or rating a movie as Bad?5\n\n\n\nDo you think that a movie receiving a Good rating is independent of Genre?6\n\n\n\nBased on your analysis above, which movie Genre do you think lead studios should invest in for their next movie?7\n\n\n\nUsing a conditional mosaic plot, display the conditional distribution of movie genres being rated as either good or bad, making sure to add a main title and clear axis titles.8\n\n\n\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions. In particular, focus on whether events were independent, and which genre of movie lead studios should consider investing in based on audience ratings.9"
  },
  {
    "objectID": "1_08_prob_rules.html#worked-example",
    "href": "1_08_prob_rules.html#worked-example",
    "title": "Probability Rules",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n Variable Name \n    Description \n  \n\n\n Bill \n    Size of the bill (in dollars) \n  \n\n Tip \n    Size of the tip (in dollars) \n  \n\n Credit \n    Paid with a credit card? n or y \n  \n\n Guests \n    Number of people in the group \n  \n\n Day \n    Day of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday \n  \n\n Server \n    Code for specific waiter/waitress: A, B, or C \n  \n\n PctTip \n    Tip as a percentage of the bill \n  \n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWorking with the “Tip_Avg” variable created last week, we can see our relative frequency table for all of our servers (A, B, and C) who were tipped either Above or Below the standard tipping rate in the US (i.e., 15%).\n\ntips2 <- tips %>%\n    mutate(Tip_Avg = ifelse(PctTip <= 15, 'Below', 'Above'))\n\nrel_freq_tbl <- table(tips2$Server, tips2$Tip_Avg) %>%\n    prop.table() %>%\n    addmargins()\nrel_freq_tbl\n\n     \n           Above      Below        Sum\n  A   0.25477707 0.12738854 0.38216561\n  B   0.25477707 0.15923567 0.41401274\n  C   0.12738854 0.07643312 0.20382166\n  Sum 0.63694268 0.36305732 1.00000000\n\n\n\nWhat’s the probability of a customer tipping above average?\n\n\n# P(above) = 0.25477707 + 0.25477707 + 0.12738854\n# P(above) = 0.63694268\n\n\n# indexing: table[row numbers, col numbers]\nrel_freq_tbl[4, 1]\n\n[1] 0.6369427\n\n# or indexing: table[row names, col names]\nrel_freq_tbl['Sum', 'Above']\n\n[1] 0.6369427\n\n\n\\(P(Above) ≈ 0.64\\)\n\nGiven that the server is A, what’s the probability of receiving an above average tip?\n\n\n#P(above | server A) = 0.25477707 / 0.38216561\n#P(above | server A) = 0.6666667\n\n\\(P(Above | Server A) ≈ 0.67\\)\n\nGiven that the server is B, what’s the probability of receiving an above average tip?\n\n\n#P(above | server B) = 0.25477707 / 0.41401274\n#P(above | server B) = 0.6153846\n\n\\(P(Above | Server B) ≈ 0.62\\)\n\nGiven that the server is C, what’s the probability of receiving an above average tip?\n\n\n#P(above | server C) = 0.12738854 / 0.20382166\n#P(above | server C) = 0.625\n\n\\(P(Above | Server C) ≈ 0.63\\)\n\nGiven that a server received a tip below average, what’s the probability of them being Server A or B?\n\n\n# P( (server A ∪ server B) | below ) = \n# (P( server A ∩ below ) + P( server B ∩ below )) / P(below) = \n# (0.12738854  + 0.15923567) / 0.36305732 =\n# 0.7894737 \n\n\\(P((Server A \\cup Server B) \\mid Below) ≈ 0.79\\)\n\nWhat’s the probability of the customer being served by server A or tipping below 15%?\n\n\n# P(server A) = 0.38216561\n# P(below) = 0.36305732\n# P(server A ∩ below) = 0.12738854\n\n# P(server A) + P(below) - P(server A ∩ below) = \n# (0.38216561 + 0.36305732) - 0.12738854 =\n# 0.6178344 ≈ 0.62\n\n\\(P(Server A \\cup Below) ≈ 0.62\\)\n\nIs tipping above average independent of the server?\n\nNo, the events seem to be dependent, but very weakly. The conditional probabilities of tipping above average for each server are different from P(above), even though to a small degree. In particular, the probability of tipping above average after service from Server A is higher.\n\nBased on your analysis above, which server do you think offers the best customer service based on their tips?\n\nServer A appears to offer the best service to their customers, based solely on their personal tips - they had a much higher probability of receiving an above average tip (67%) than a below average tip (33%).\n\nTo visualise our findings, we could use a conditional mosaic plot:\n\n\nlibrary(ggmosaic)\nmos_cond_plot <- ggplot(tips2) +\n    geom_mosaic(aes(x = product(Tip_Avg), fill = Tip_Avg, conds = product(Server))) +  \n    labs(title = \"Conditional Association between Servers and Tips\", \n         x = \"Server\", \n         y = \"Tip Average\", \n         fill = \"Tip Average\")\nmos_cond_plot\n\n\n\nFigure 2: Conditional Association between Servers and Tips\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nIt was more likely for customers to tip above (64%) than below (36%) average. Though it was likely that all servers would receive an above average tip, tipping did not appear to be independent of server, based on conditional probabilities. Based on their personal tips, Server A appeared to offer the best service, where they were more likely to receive an above average tip (67%). Servers B and C were almost equally likely to receive above average tips (62% and 63% respectively). These associations are visually represented in Figure 2.\n\n\n\n\n\n\n\n\nAdvanced material: Three definitions of independence\n\n\n\n\n\nRecall the frequency table\n\nrel_freq_tbl\n\n     \n           Above      Below        Sum\n  A   0.25477707 0.12738854 0.38216561\n  B   0.25477707 0.15923567 0.41401274\n  C   0.12738854 0.07643312 0.20382166\n  Sum 0.63694268 0.36305732 1.00000000\n\n\nThe following three definitions of independence are equivalent. Two events \\(A\\) and \\(B\\) are independent if one of these holds:\n\n\\(P(A | B) = P(A)\\)\nor \\(P(B | A) = P(B)\\)\n\nor \\(P(A \\cap B) = P(A) P(B)\\)\n\n\nFor now, let’s focus on the third definition. To see if tipping above average is independent of the specific server, we can checks that condition separately for each server:\nA. Is \\(P(Server A \\cap Above)\\) equal to \\(P(Server A) P(Above)\\)?\n\n# 0.25477707 is P(Server A ∩ Above)\n0.38216561 * 0.63694268 # P(Server A) * P(Above)\n\n[1] 0.2434176\n\n\nB. Is \\(P(Server B \\cap Above)\\) equal to \\(P(Server B) P(Above)\\)?\n\n# 0.25477707 is P(Server B ∩ Above)\n0.41401274 * 0.63694268 # P(Server B) * P(Above)\n\n[1] 0.2637024\n\n\nC. Is \\(P(Server C \\cap Above)\\) equal to \\(P(Server C) P(Above)\\)?\n\n# 0.12738854 is P(Server C ∩ Above)\n0.20382166 * 0.63694268 # P(Server C) * P(Above)\n\n[1] 0.1298227\n\n\nFor server A and B, the values are close enough but not exactly equal. However, for server C, the values are identical up to the 2nd decimal place. This suggests the events are dependent, but to a small extent."
  },
  {
    "objectID": "1_08_prob_rules.html#student-glossary",
    "href": "1_08_prob_rules.html#student-glossary",
    "title": "Probability Rules",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\n|\n?\n\n\n/\n?\n\n\nconds\n?"
  },
  {
    "objectID": "1_09_discrete_dist.html",
    "href": "1_09_discrete_dist.html",
    "title": "Random Variables (Discrete)",
    "section": "",
    "text": "Instructions Recap - Formative Report B\n\n\n\n\n\n\nIn this block of the course (weeks 7-11), you should produce a PDF report using Rmarkdown for which you will receive formative feedback in week 12.\nThe report should not include any reference to R code or functions, but be written or a generic reader who is only assumed to have a basic statistical understanding without any R knowledge. You should also avoid any R code output or printout in the PDF file.\nYou will be required to submit a PDF file by 12 noon on Friday the 2nd of December 2022 via Learn. One person needs to submit on behalf of your group.\n\nThe report should be at most 6 pages long. At the end of the report, you are allowed two appendices which both don’t count towards the page limit.\n\nAppendix A will contain any tables or figures which you cannot fit in the page limit (no text allowed)\nAppendix B will contain the code to reproduce the report results (just like Formative Report A).\n\n\nNo extensions allowed. As this is group-based work, no extensions are possible."
  },
  {
    "objectID": "1_09_discrete_dist.html#tasks",
    "href": "1_09_discrete_dist.html#tasks",
    "title": "Random Variables (Discrete)",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week you will only focus on task B3.\nB1) Create and summarise categorical variables, before calculating probabilities.\nB2) Investigate if events are independent, and compute probabilities.\n\n\n\n\n\n\nThis week’s task\n\n\n\nB3) Computing and plotting probabilities with a binomial distribution.\n\n\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Plot standard error of the mean, and finish the report write-up (i.e., knit to PDF, and submit the PDF for formative feedback)."
  },
  {
    "objectID": "1_09_discrete_dist.html#b3-sub-tasks",
    "href": "1_09_discrete_dist.html#b3-sub-tasks",
    "title": "Random Variables (Discrete)",
    "section": "\n2 B3 sub-tasks",
    "text": "2 B3 sub-tasks\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nFocus on completing all of the lab tasks, and leave non-essential things like changing colors for later.\nIf, after looking at the hint, you still have no clue on how to answer a question, check the worked example below!\n\n\n\nIn this section you will find some guided sub-steps you may want to consider to complete task B3.\nA new movie theatre is opening in Texas, and the management team are reviewing the requirements for snack stalls and their car parking capacity. They are interested in determining whether to have only one or multiple snack stalls selling popular food and drink items; and are considering whether to expand their car park size or introduce a bus stop on cinema grounds.\nA recent survey suggested that 49% of movie viewers always buy some form of snack item (i.e., popcorn, drinks, sweets) when watching a movie, and another suggested that 70% of movie viewers travel to the cinema via private transport.\nIn this lab, you will need to consider both the Snacks and Private Transport variables from the Hollywood movies dataset when answering the questions below.\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nConsider the Snacks and Private Transport variables, are they discrete or continuous?\nPlot separately the sample frequency distribution of both the Snacks and Private Transportvariables with barplots.2\n\n\nWhat kind of distribution do these follow? Estimate the parameters of your two distributions from the sample data.3\n\n\n\nPlot the fitted Binomial distribution on top of the sample frequency distribution for each variable. Is the Binomial distribution a good model for each variable?4\n\n\n\nWhat is the probability that exactly half of the audience for each movie (i.e., 25 viewers) bought snacks? What is the probability that exactly half traveled via private transport?5\n\n\n\nWhat is the probability that more than half of the audience for each movie (i.e., >25 viewers) bought snacks? What is the probability that less than half of the audience for each movie (i.e., <25 viewers) traveled via private transport?6\n\n\n\nBased on the probabilities you have reported above, do you think that the new movie theatre should (1) invest in multiple snack stations, or just one; and (2) increase car parking capacity or add a bus stop? Justify your answer.\nAre your two estimates consistent with the survey-reported values?7\n\n\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_09_discrete_dist.html#worked-example",
    "href": "1_09_discrete_dist.html#worked-example",
    "title": "Random Variables (Discrete)",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe dataset available at https://uoepsy.github.io/data/RestaurantTips2.csv was collected by the owner of a US bistro, and contains 99 observations on 10 variables. It is a subset of the RestaurantTips.csv data presented in the past weeks, focusing only on parties of 2 people.8\nThe bistro owner is interested in coffee sales, and whether they should consider introducing a 2 for 1 coffee deal to entice customers to purchase one of their Christmas coffees. Another option they are considering is starting a loyalty scheme for customers to be rewarded for every coffee purchase. Your job is to advise them on which scheme they should run to benefit most customers.\nFor context, Americans drink a lot of coffee, but slightly less than Norwegians (89.4% of Norwegians drink at least one coffee per day!9). We are interested in estimating the probability that an adult American will drink coffee.\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\nHadCoffee\nNumber of guests in the group who had coffee\n\n\nIQ1\nScore on IQ test for guest 1\n\n\nIQ2\nScore on IQ test for guest 2\n\n\n\n\n\n\nlibrary(tidyverse)\ntips2 <- read_csv(\"https://uoepsy.github.io/data/RestaurantTips2.csv\")\nhead(tips2)\n\n# A tibble: 6 × 10\n   Bill   Tip Credit Guests Day   Server PctTip HadCoffee   IQ1   IQ2\n  <dbl> <dbl> <chr>   <dbl> <chr> <chr>   <dbl>     <dbl> <dbl> <dbl>\n1  23.7 10    n           2 f     A        42.2         2    93   100\n2  32.0  5.01 y           2 f     A        15.7         2    96    98\n3  17.4  3.61 y           2 f     B        20.8         2    94    99\n4  15.4  3    n           2 f     B        19.5         2    99   108\n5  18.6  2.5  n           2 f     A        13.4         2   129   106\n6  21.6  3.44 n           2 f     B        16           2    82   118\n\n\n\nIf we were asked to describe what kind of variable HadCoffee is, and to comment on the kind of probability distribution it may follow, we could say:\n\nThe number of coffee-consuming guests out of parties of size 2 is a discrete random variable that could be modeled by a Binomial probability distribution.\n\nWe can plot the frequency distribution of the HadCoffee variable in the sample as following:\n\nCompute the frequency distribution and relative frequency:\n\nfreq_distr <- tips2 %>%\n    count(HadCoffee) %>%\n    mutate(rel_freq = n / sum(n)) %>%\n    rename(freq = n)    # rename(new_name = current_name)\n                        # this renames column 'n' to 'freq'\nfreq_distr\n\n# A tibble: 3 × 3\n  HadCoffee  freq rel_freq\n      <dbl> <int>    <dbl>\n1         0     9   0.0909\n2         1    29   0.293 \n3         2    61   0.616 \n\n\nPlot the frequency distribution using a barplot:\n\n\ngeom_col() is used to create a barplot when you have already computed the bar heights, i.e. the frequency table.\ngeom_bar() takes the original data, and does the counting for you.\n\nggplot(freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_col() +\n    labs(x = \"Coffee-drinking customers per party\",\n         y = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\n\nFitting a distribution\n\n\n\n\n\nFitting a Binomial distribution to data involves estimating the parameters of the distribution from the data. In other words, we want to find values for \\(n\\) and \\(p\\) from the variable HadCoffee in the our data.\n\n\n\n\n\nWe can now fit a Binomial distribution to the variable. To do so, we need to start by estimating the parameters of the Binomial distribution:\n\n\n\\(n\\), the number of trials (or size)\n\n\\(p\\), the probability of success\n\n\n\nFor this dataset, \\(n\\) represents the size of each party, i.e. \\(n = 2\\). The discrete variable HadCoffee represents how many guests had coffee, out of the 2 possible guests per party.\n\n\nIn a Binomial distribution, the number of trials (\\(n\\)) should not be confused with the sample size. The sample size would be the total number of parties in the dataset, i.e. 99. It’s just unfortunate that both use the same symbol \\(n\\), but which one is the correct one should be clear from the context.\nThe event “had coffee” represents our “success”, and \\(p\\) denotes the probability of success. In other words, \\(p\\) represents the probability of an individual having coffee.\n\n\n\n\n\n\nEstimating the probability of success\n\n\n\n\n\nThe expected value \\(E(X)\\) of a Binomial distribution (i.e., the mean) is \\(E(X) = n * p\\) where \\(n\\) = number of trials = size, and \\(p\\) is the probability of success. From this, we have that \\(p = E(X) / n\\).\nWe typically denote the estimated probability of success from the sample data with a hat on top, \\(\\hat{p}\\), written in text as $\\hat{p}$.\n\n\n\n\n# The mean of the discrete random variable\nEX <- mean(tips2$HadCoffee)  \n\n# Each party size (n in the formula)\nsize <- 2\n\n# Estimated probability of having coffee: E(X) / n\np_hat <- EX / size\n\n#check value\np_hat\n\n[1] 0.7626263\n\n\n\nWe can then compare the sample frequency distribution to the Binomial distribution, and comment on whether the Binomial fit is good:\n\nWe can create a new tibble having two columns. The first has the possible values of the Binomial distribution (0, 1, or 2 guests ordering coffee out of the 2). The second column has the theoretical probabilities, for each of the possible values, predicted by the Binomial distribution with parameters \\(n = 2\\) and \\(p = 0.76\\).\n\nbinom_distr <- tibble(\n    HadCoffee = 0:2,\n    binom_prob = dbinom(x = HadCoffee, size = 2, prob = p_hat)\n)\nbinom_distr\n\n# A tibble: 3 × 2\n  HadCoffee binom_prob\n      <int>      <dbl>\n1         0     0.0563\n2         1     0.362 \n3         2     0.582 \n\n\nWe can plot the sample frequency distribution as a bar plot and put on top the fitted Binomial probability distribution as dots (see Option 1), and even add segments to show the Binomial distribution (see Option 2).\n\n\noption 1\noption 2\n\n\n\n\nggplot() +\n    geom_col(data = freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_point(data = binom_distr, \n               aes(x = HadCoffee, y = binom_prob),  # adds points\n               colour = 'red', size = 3)\n\n\n\n\n\n\n\nggplot() +\n    geom_col(data = freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_point(data = binom_distr, \n               aes(x = HadCoffee, y = binom_prob),  # adds points\n               colour = 'red', size = 3) +\n    geom_segment(data = binom_distr, \n                 aes(x = HadCoffee, y = binom_prob, # adds line (optional) \n                     xend = HadCoffee, yend = 0),   # from (x,y) to (xend,yend)\n                 colour = 'red')\n\n\n\nFigure 1: Probability of guests drinking coffee.\n\n\n\n\n\n\n\nThe Binomial distribution seems to be a good fit for the sample distribution, as the probabilities tend to agree. Among all parties of two guests, the highest probability is that both guests had coffee, and no one having coffee has the lowest probability.\n\n\n\n\n\n\nProbability Mass Function\n\n\n\n\n\n\n\n\\(P(X = x)\\) = dbinom(x, size, prob)\n\n\nThe probability mass function computes \\(P(X = x)\\) for a Binomial distribution where number of trials is size and probability of success is prob.\n\n\n\n\nTo calculate the probability that 1 person in the party orders coffee, we can compute the following:\n\nThe probability P(X = 1) for a Binomial with \\(x = 1\\), \\(size = 2\\), and \\(p = 0.76\\) is:\n\ndbinom(1, size = 2, prob = p_hat)\n\n[1] 0.3620549\n\n\n\nTo calculate the probability that 2 people in the party order coffee, we can compute the following:\n\nThe probability P(X = 2) for a Binomial with \\(x = 2\\), \\(size = 2\\), and \\(p = 0.76\\) is:\n\ndbinom(2, size = 2, prob = p_hat)\n\n[1] 0.5815988\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\n\n\n\n\n\\(P(X \\leq q)\\) = pbinom(q, size, prob)\n\n\nThe cumulative distribution function \\(P(X \\leq q)\\) gives the probability of having less than or equal to \\(q\\) successes.\nFrom the total probability rule, the probability of a value being greater than \\(q\\) is computed as \\(P(X > q) = 1 - P(X \\leq q)\\):\n# option 1: P(X > q) = 1 - P(X <= q)\n1 - pbinom(q, size, prob)\n# option 2: P(X > q) directly\npbinom(q, size, prob, lower.tail = FALSE)\n\n\n\n\nTo calculate the probability that 1 person or less orders coffee, we can compute the following:\n\nWe do this with the pbinom(q, size, prob) function, with q = 1 to have \\(P(X \\leq 1) = P(X = 0) + P(X = 1)\\):\n\npbinom(1, size = 2, prob = p_hat)\n\n[1] 0.4184012\n\n\nYou can also see this by creating a new column in the binomial distribution which has the cumulative sums of the probabilities, i.e. the values P(X = 0), followed by P(X = 0) + P(X = 1), and finally by P(X = 0) + P(X = 1) + P(X = 2):\n\nbinom_distr %>% \n    mutate(\n        cumul_prob = cumsum(binom_prob)\n    )\n\n# A tibble: 3 × 3\n  HadCoffee binom_prob cumul_prob\n      <int>      <dbl>      <dbl>\n1         0     0.0563     0.0563\n2         1     0.362      0.418 \n3         2     0.582      1     \n\n\nAs you can see, P(X = 0) + P(X = 1) is 0.42 in the second row, which agrees with the result computed using pbinom above.\n\nTo calculate the probability that at least one person from the group orders coffee, we can compute one of the below four equivalent calucaltions:\n\n\n\n1 - P(X <= 0)\nP(X > 0)\nP(X = 1) + P(X = 2)\n1 - P(X = 0)\n\n\n\n\n1 - pbinom(0, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\npbinom(0, size = 2, prob = p_hat, lower.tail = FALSE)\n\n[1] 0.9436537\n\n\n\n\n\n# sum up the probability of that 1 person orders coffee and the probability that both persons order coffee\n# P(1 guest has coffee) + P(2 guests has coffee)\ndbinom(1, size = 2, prob = p_hat) + dbinom(2, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\n# 1 - P(X = 0)\n# As the total probability is 1, we can get it as 1 - P(0 guests have coffee). \n# This follows from the total probability rule\n1 - dbinom(0, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\n\nTo check whether our estimated probability of drinking coffee consistent with the one reported by a recent YouGov survey, which reported that three quarters of adult Americans drink coffee, we can take a look at our estimated \\(\\hat{p}\\). We can also then comment on whether adult Americans are more or less likely to drink coffee than Norwegians?\n\n\n# YouGov reported probability\np_survey <- 3/4\np_survey\n\n[1] 0.75\n\n\n\n# Our estimate\np_hat\n\n[1] 0.7626263\n\n\nThe estimated probability of drinking coffee, based on the data from a US bistro, is 0.76. This is relatively close to the YouGov reported result of 0.75. The small deviation may be due to sampling variability, due to having chosen a different sample of people from the one that were considered in this bistro.\nAccording to a recent survey, 89.4% of Norwegians drink at least one coffee per day10 As such, while adult Americans drink lots of coffee, an adult American is less likely to drink coffee than a Norwegian.\n\n\n\n\n\n\nExample writeup\n\n\n\nFigure 1 displays the distribution of coffee-drinking customers in two-party tables, with a Binomial fit superimposed as red dots. Both party guests are more likely to both drink coffee (0.58) than not (0.06). The probability of only one guest drinking coffee is 0.36. As such, the probability of at least one guest drinking coffee is 0.94. The owner of the bistro should not consider running a 2 for 1 offer on Christmas coffees, as they would lose out on income. It is most likely that groups of two customers will already buy two coffees when they visit the bistro (0.58). Instead, the owner might want to offer a loyalty scheme to reward customers for purchasing coffees, regardless of the quantity, as this will also reward parties of two where only one person purchases a coffee."
  },
  {
    "objectID": "1_09_discrete_dist.html#student-glossary",
    "href": "1_09_discrete_dist.html#student-glossary",
    "title": "Random Variables (Discrete)",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nrename()\n?\n\n\ndbinom()\n?\n\n\npbinom()\n?\n\n\ngeom_col()\n?\n\n\ngeom_point()\n?\n\n\ngeom_segment()\n?\n\n\ncumsum()\n?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is the homepage of DAPR1 labs. Please use the navigation bar above to go to your week’s content."
  },
  {
    "objectID": "rd1_01.html",
    "href": "rd1_01.html",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Recommended option: access it via the RStudio server. Try these steps first to register for RStudio server online:\n\nLog in to EASE using your university UUN and password.\nSet your RStudio password here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you set above in (2).\n\nIf the steps above did not work:\n\nPlease complete this form and wait for an email. Please note that this can take up to four working days.\n\nOnce you receive an email from us, please follow the following instructions:\n\nSet your here, the username will be the same as your UUN (make sure you type your UUN correctly).\nAccess the server from https://rstudio.ppls.ed.ac.uk using your university UUN and the password you just set above.\n\n\n\nAlternative: install it locally on your PC following these instructions.\n\n\nCreate a new folder on the server. Give it a useful name like the name of the course: DAPR1. (In the picture we have used “USMR” but please use “DAPR1”).\n\n\n\nIn the top right, click Project > New Project\n\n\n\nClick “Existing Directory”\n\n\n\nClick “Browse” and then choose the folder you just created. Click “Choose” and then click “Create Project”.\n\n\n\nYou should now be able to tell that you have the project open, because it shows you in the top right.\n\n\nYou’re ready to go!"
  },
  {
    "objectID": "rd1_01.html#a-first-look-at-rstudio",
    "href": "rd1_01.html#a-first-look-at-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "\n2 A first look at RStudio",
    "text": "2 A first look at RStudio\nOkay, now you should have RStudio and a project open, and you should see something which looks more or less like the image below, where there are several little windows.\n\nWe are going to explore what each of these little windows offer by just diving in and starting to do things.\n\n2.1 R as a calculator\nStarting in the left-hand window, you’ll notice the blue sign > which is where we R code gets executed.\nType 2+2, and hit Enter ↵. You should discover that R is a calculator.\nLet’s work through some of the basic operations (adding, subtracting, etc). Try these commands yourself:\n\n2 + 5\n10 - 4\n2 * 5\n10 - (2 * 5)\n(10 - 2) * 5\n10 / 2\n\n3^2 (Hint, interpret the ^ symbol as “to the power of”)\n\n\n\n\n\n\n\nWhenever you see the blue sign >, it means R is ready and waiting for you to provide a command.\nIf you type 10 + and press Enter, you’ll see that instead of a blue > you are left with a blue +. This means that R is waiting for more. Either give it more, or cancel the command by pressing the escape key on your keyboard.\n\n\n\nAs well as performing calculations, we can ask R things, such as “Is 3 less than 5?”:\n\n3 < 5\n\n[1] TRUE\n\n\nAs the computation above returns TRUE, we notice that such questions return either TRUE or FALSE. These are not numbers and are called logical values.\nTry the following:\n\n\n3 > 5 “is 3 greater than 5?”\n\n3 <= 5 “is 3 less than or equal to 5?”\n\n3 >= 3 “is 3 greater than or equal to 3?”\n\n3 == 5 “is 3 equal to 5?”\n\n(2 * 5) == 10 “is 2 times 5 equal to 10?”\n\n(2 * 5) != 11 “is 2 times 5 NOT equal to 11?”\n\n2.2 R as a calculator with a memory\nWe can also store things in R’s memory, and to do that we just need to give them a name. Type x <- 5 and press Enter.\nWhat has happened? We’ve just stored something named x which has the value 5. We can now refer to the name and it will give us the value! Try typing x and hitting Enter. It should give you the number 5. What about x * 3?\n:::{.callout-info} #### Storing things in R\nThe <- symbol, pronounced arrow, is used to assign a value to a named object:\n[name] <- [value]\nNote, there are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the <- don’t matter):\n\n\nlucky_number <- 5 ✔   lucky number <- 5 ❌\n\n\n\nNames must start with a letter:\n\n\nlucky_number <- 5 ✔   1lucky_number <- 5 ❌\n\n\n\nCase sensitive:\n\n\nlucky_number is different from Lucky_Number\n\n\n\nReserved words - there is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function\n(Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\nYou might have noticed that something else happened when you executed the code x <- 5. The thing we named x with a value of 5 suddenly appeared in the top-right window. This is known as the environment, and it shows everything that we store things in R:\n\nWe’ve now used a couple of the windows - we’ve been executing R code in the console, and learned about how we can store things in R’s memory (the environment) by assigning a name to them:\n\nNotice that in the screenshot above, we have moved the console down to the bottom-left, and introduced a new window above it. This is the one that we’re going to talk about next."
  },
  {
    "objectID": "rd1_01.html#r-scripts-and-rmarkdown",
    "href": "rd1_01.html#r-scripts-and-rmarkdown",
    "title": "Getting started with R and RStudio",
    "section": "\n3 R scripts and Rmarkdown",
    "text": "3 R scripts and Rmarkdown\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWell, we can write and edit our code in a separate place before sending it to the console to be executed!!\n\n3.1 R scripts\n\n\n\n\n\n\nTask\n\n\n\n\nOpen an R script\n\nFile > New File > R script\n\n\nCopy and paste the following into the R script\n\n\nx <- 210\ny <- 15\nx / y\n\n\nPosition your text-cursor (blinking vertical line) on the top line and press:\n\nCtrl + Enter on Windows\nCmd + Enter on macOS\n\n\n\n\n\nNotice what has happened - it has sent the command x <- 210 to the console, where it has been executed, and x is now in your environment. Additionally, it has moved the text-cursor to the next line.\n\n\n\n\n\n\n\nTask\n\n\n\nPress Ctrl + Enter (Windows) or Cmd + Enter (macOS) again. Do it twice (this will run the next two lines).\nThen, change x to some other number in your R script, and run the lines again (starting at the top).\n\n\n\n\n\n\n\n\nTask\n\n\n\nAdd the following line to your R script and execute it (send it to the console pressing Ctrl/Cmd + Enter):\n\nplot(1,5)\n\n\n\nA very basic plot should have appeared in the bottom-right of RStudio. The bottom-right window actually does some other useful things.\n\n\n\n\n\n\nTask\n\n\n\n\nSave the R script you have been working with:\n\nFile > Save\ngive it an appropriate name, and click save.\n\n\nCheck that you can now see that file in the file pane, by clicking on the “Files” tab of the bottom-right window.\n\n\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n3.2 Rmarkdown\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\nIn addition to R scripts, there is another type of document we can create, known as an “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\nWe’re going to use Rmarkdown documents throughout this course. We’ll get into it how to write them lower down, but it basically involves writing normal text interspersed with “code-chunks” (i.e., chunks of code!). In the example below, you can see the grey boxes indicating the R code, with text in between. We can then compile the document into either a .pdf or a .html file."
  },
  {
    "objectID": "rd1_01.html#recap",
    "href": "rd1_01.html#recap",
    "title": "Getting started with R and RStudio",
    "section": "\n4 Recap",
    "text": "4 Recap\nOkay, so we’ve now seen all of the different windows in RStudio in action:\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code and Rmarkdown documents. You can then send this to the console for it to be executed.\nThe bottom-right window shows you the plots that you create, the files in your project, and some other things (we’ll get to these later)."
  },
  {
    "objectID": "rd1_01.html#take-a-breather",
    "href": "rd1_01.html#take-a-breather",
    "title": "Getting started with R and RStudio",
    "section": "\n5 Take a breather",
    "text": "5 Take a breather\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to learning about how we store data in R.\n\n\n\n\n\n\nUseful Settings 1: Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools > Global Options…\n\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n\n\n\n\n\n\n\nUseful Settings 2: Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx <- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools > Global Options…\n\nIn the left menu of the box, click “Code”\n\nTick the box for “Soft-wrap R source files”"
  },
  {
    "objectID": "rd1_01.html#r-packages",
    "href": "rd1_01.html#r-packages",
    "title": "Getting started with R and RStudio",
    "section": "\n6 R Packages",
    "text": "6 R Packages\n\n6.1 Installing R packages\nAlongside the basic installation of R and RStudio, there are many add-on packages which the R community create and maintain.\nThe thousands of packages are part of what makes R such a powerful and useful tool - there is a package for almost everything you could want to do in R.\n\n\n\n\n\n\nTask\n\n\n\nIn the console, type install.packages(\"cowsay\") and hit Enter.\nLots of red text will come up, and it will take a bit of time.\nWhen it has finished, and R is ready for you to use again, you will see the blue sign >.\n\n\n\n6.2 Using R packages\nIt’s not enough just to install a package - to actually use the package, we need to load it using library().\nWe install a package only once. But each time we open RStudio, we have to load the packages we need.\n\n\n\n\n\n\n\nTask\n\n\n\nIn the console again, type library(cowsay) and hit enter. This loads the package for us to use it.\nThen, type say(\"hello world\", by = \"cow\") and hit enter.\nHopefully you got a similar result to ours:\n\nlibrary(cowsay)\nsay(\"Hi Folks!\", by = \"cow\")\n\n\n ----- \nHi Folks! \n ------ \n    \\   ^__^ \n     \\  (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||"
  },
  {
    "objectID": "rd1_01.html#your-first-.rmd-file",
    "href": "rd1_01.html#your-first-.rmd-file",
    "title": "Getting started with R and RStudio",
    "section": "\n7 Your first .Rmd file",
    "text": "7 Your first .Rmd file\nIn order to be able to write and compile Rmarkdown documents (and do a whole load of other things which we are going to need throughout the course) we are now going to install a set of packages known collectively as the “tidyverse” (this includes the “rmarkdown” package).\n\n\nIf you installed R/Rstudio on your own computer, then in the console, type install.packages(\"tidyverse\") and hit Enter. You may have to wait a while.\n\nIf you are using rstudio.ppls.ed.ac.uk, then we have already installed “tidyverse” and a few other useful packages for you, so you don’t have to do anything!\n\n\n\n\n\n\n\nTask\n\n\n\nOpen a new Rmarkdown document: File > New File > R Markdown…\nWhen the box pops-up, give a title of your choice (“Intro lab”, maybe?) and your name as the author.\n\n\n\n7.1 Writing code in a .Rmd file\nThe file which you have just created will have some template stuff in it. Delete everything below the first code chunk to start with a fresh document:\n\n\n\n\n\n\n\nTask\n\n\n\nInsert a new code chunk by either using the Insert button in the top right of the document and selecting R, or by typing Ctrl + Alt + i on Windows or Option + Cmd + i on MacOS.\nInside the chunk, type:\nprint(\"Hello world! My name is ?\")\nTo execute the code inside the chunk, you can either:\n\ndo as you did in the R script - put the text-cursor on the first line, and hit Ctrl/Cmd + Enter to run the lines sequentially;\nclick the little green arrow at the top right of your code-chunk to run all of the code inside the chunk;\nwhile your cursor is inside the code chunk, press Cmd + Shift + Enter to run all of the code inside the chunk.\n\nYou can see that the output gets printed below.\n\n\n\nWe’re going to use some functions which are in the tidyverse package, which we already installed above (or which we installed for you on the server).\nTo use the package, we need to load it.\nWhen writing analysis code, we want it to be reproducible - we want to be able to give somebody else our code and the data, and ensure that they can get the same results. To do this, we need to show what packages we use.\nIt is good practice to load any packages you use at the top of your code, so that users of your code will know what packages they will need to install to run your code.\nIn your first code chunk, type:\n\n# I'm going to use these packages in this document:\nlibrary(tidyverse)\n\nand run the chunk.\nNOTE: You might get various messages popping up below when you run this chunk, that is fine.\nComments in code\nNote that using # in R code makes that line a comment, which basically means that R will ignore the line. Comments are useful for you to remind yourself of what your code is doing.\n\n7.2 Writing text in a .Rmd file\nPlace your cursor outside the code chunk, and below the code chunk add a new line with the following:\n# R code examples\nNote that when the # is used in a Rmarkdown file outside of a code-chunk, it will make that line a heading when we finally get to compiling the document. Below, what you see on the left will be compiled to look like those on the right:\n\n\n\n\n\n\n\nRECALL:\n\n\n\n\n\nInside a code-chunk, one or more #s will create a comment\n\n\nOutside a code-chunk, one ore more #s will create headings\n\n\n\n\nIn your Rmarkdown document, choose a few of the symbols below, and write an explanation of what it does, giving an example in a code chunk. You can see an example of the first few below.\n\n+\n-\n*\n/\n()\n^\n<-\n<\n>\n<=\n>=\n==\n!=\n\n\n\n7.3 Storing data into R\nWe’ve already seen how to assign a value to a name/symbol using <-. However, we’ve only seen how to assign a single number, e.g, x <- 5.\nTo store a sequence of numbers into R, we combine the values using the combine function c() and give the sequence a name. A sequence of elements all of the same type is called a vector. To view the stored content, simply type the name of the vector.\n\nmyfirstvector <- c(1, 5, 3, 7)\nmyfirstvector\n\n[1] 1 5 3 7\n\n\nWe can perform arithmetic operations on each value of the vector. For example, to add five to each entry:\n\nmyfirstvector + 5\n\n[1]  6 10  8 12\n\n\nRecall that vectors are sequences of elements all of the same type. They do not have to be always numbers; they could be words such as real or fictional animals. Words need to be written inside quotations, e.g. “anything”, and instead of being of numeric type, we say they are characters.\n\nwordsvector <- c(\"cat\", \"dog\", \"parrot\", \"peppapig\")\nwordsvector\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nNOTE\nYou can use either double-quote or single-quote:\n\nc(\"cat\", \"dog\", \"parrot\", \"peppapig\")\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\nc('cat', 'dog', 'parrot', 'peppapig')\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nThe function class() will tell you the type of the object. In this case, it is a character vector.\n\nclass(wordsvector)\n\n[1] \"character\"\n\n\nIt does not make sense to add a number to words, hence some operations like addition and multiplication are only defined on vectors of numeric type. If you make a mistake, R will warn you with a red error message.\n\nwordsvector + 5\n\n\nError in wordsvector + 5 : non-numeric argument to binary operator\n\nFinally, it is important to notice that if you combine together in a vector a number and a word, R will transform all elements to be of the same type. Why? Recall: vectors are sequences of elements all of the same type. Typically, R chooses the most general type between the two. In this particular case, it would make everything a character, check the ““, as it would be harder to transform a word into a number!\n\nmysecondvector <- c(4, \"cat\")\nmysecondvector\n\n[1] \"4\"   \"cat\"\n\n\n\n7.4 Reading data into R\nWhile we can manually input data like we did above, more often, we will need to read in data which has been created elsewhere (like in excel, or by some software which is used to present participants with experiments).\n\n\n\n\n\n\nTask\n\n\n\nAdd a new heading by typing the following:\n# Reading and storing data\nRemember: We make headings using the # outside of a code chunk.\n\n\n\n\n\n\n\n\nTask\n\n\n\nOpen Microsoft Excel, or LibreOffice Calc, or whatever spreadsheet software you have available to you, and create some data with more than one variable.\nIt can be whatever you want, but we’ve used a very small example here for you to follow, so feel free to use it if you like.\nWe’ve got two sets of values here: the names and the birth-years of each member of the Beatles. The easiest way to think of this would be to have a row for each Beatle, and a column for each of name and birth-year.\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSave the data as a .csv file.\nAlthough R can read data when it’s saved in Microsoft/LibreOffice formats, the simplest, and most universal way to save data is as simple text, with the values separated by some character - .csv stands for comma separated values.\nIn Microsoft Excel, if you go to File > Save as\nIn the Save as Type box, choose to save the file as CSV (Comma delimited).\nImportant: save your data in the project folder you created at the start of this lab.\n\n\nBack in RStudio…\nNext, we’re going to read the data into R. We can do this by using the read_csv() function, and directing it to the file you just saved.\nIf you are using RStudio on the server, you will need to upload the file you just saved to the server. The video below shows an example of this:\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nCreate a new code-chunk in your Rmarkdown and, in the chunk, type: read_csv(\"name-of-your-data.csv\"), where you replace name-of-your-data with whatever you just saved your data as in your spreadsheet software.\n\n\nHelpful tip\nIf you have your text-cursor inside the quotation marks, and press the tab key on your keyboard, it will show you the files inside your project. You can then use the arrow keys to choose between them and press Enter to add the code:\n\n\n\n\n\nWhen you run the line of code you just wrote, it will print out the data, but will not store it. To do that, we need to assign it as something:\n\n\n\n\nbeatles <- read_csv(\"data_from_excel.csv\")\n\nNote that this will now turn up in the Environment pane of RStudio.\nNow that we’ve got our data in R, we can print it out by simply invoking its name:\n\nbeatles\n\n# A tibble: 4 × 2\n  name   birth_year\n  <chr>       <dbl>\n1 John         1940\n2 Paul         1942\n3 George       1943\n4 Ringo        1940\n\n\nAnd we can do things such as ask R how many rows and columns there are:\n\ndim(beatles)\n\n[1] 4 2\n\n\nThis says that there are 4 members of the Beatles, and for each we have 2 measurements.\nTo get more insight into what the data actually are, you can either use the structure str() function, or glimpse() function to get a glimpse at the data:\n\nstr(beatles)\n\nspc_tbl_ [4 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ name      : chr [1:4] \"John\" \"Paul\" \"George\" \"Ringo\"\n $ birth_year: num [1:4] 1940 1942 1943 1940\n - attr(*, \"spec\")=\n  .. cols(\n  ..   name = col_character(),\n  ..   birth_year = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nglimpse(beatles)\n\nRows: 4\nColumns: 2\n$ name       <chr> \"John\", \"Paul\", \"George\", \"Ringo\"\n$ birth_year <dbl> 1940, 1942, 1943, 1940\n\n\n\n\n\n\n\n\nTask\n\n\n\nUse dim() to confirm how many rows and columns are in your data.\nUse str() or glimpse() to take a look at the structure of the data. Don’t worry about the output of str() right now, we’ll pick up with this in the next chapter.\n\n\n\n7.5 Getting help in R\ndim(), str(), read_csv() are all functions.\nFunctions perform specific operations / transformations in computer programming.\nThey can have inputs and outputs. For example, dim() takes some data you have stored in R as its input, and gives the dimensions of the data as its output.\nIn R, functions come with help pages, where you can see information about the various inputs and outputs, and examples of how to use them.\nIn the console, type ?dim (or ?dim() will work too) and press Enter.\nThe bottom-right pane (where things like plots are also shown), should switch to the Help tab, and open the documentation page for the dim() function!\n\n\n\n\n\n\nWhy did we ask you to write this bit in the console, whereas previously we’ve been writing stuff in the RMarkdown document in the editor?\nWell, when writing an RMarkdown document, the aim at the end is to have a nice document which we can read. For instance, we can write statistical reports, journal papers, coursework reports etc, in Rmarkdown. But the reader doesn’t need to see that we’re looking up how to use some function - just like they don’t need to know that we might look up a word in the dictionary before using it.\n\n\n\n\n7.6 Compiling a .Rmd file\n\n\n\n\n\n\nTask\n\n\n\nBy now, you should have an Rmardkown document (.Rmd) with your answers to the tasks we’ve been through today.\nCompile the document by clicking on the Knit button at the top (it will ask you to save your document first). The little arrow to the right of the Knit button allows you to compile to either .pdf or .html."
  },
  {
    "objectID": "rd1_01.html#checklist-for-today",
    "href": "rd1_01.html#checklist-for-today",
    "title": "Getting started with R and RStudio",
    "section": "\n8 Checklist for today",
    "text": "8 Checklist for today\n\n\nEITHER:\n\nOption A: Get started with the PPLS RStudio Server   ✔\nOption B: Install R and RStudio   ✔\n\n\nStart a new project for the course   ✔\nChange a few RStudio settings (recommended)   ✔\nInstall some R packages (the “tidyverse”)   ✔\nCreate a new Rmarkdown document   ✔\nComplete today’s tasks and exercises on storing data in R   ✔\nCompile your Rmarkdown document   ✔\nCelebrate!   ✔ 🎉"
  },
  {
    "objectID": "rd1_01.html#glossary",
    "href": "rd1_01.html#glossary",
    "title": "Getting started with R and RStudio",
    "section": "\n9 Glossary",
    "text": "9 Glossary\n\nConsole: where the code gets executed\nEnvironment: R’s memory, it lists all the names of things with stuff stored into them\nEditor: where we edit code\nR script: a file with R code and comments\nRmarkdown document: an enhanced file where you can combine together R code, explanatory text, and plots.\npackages (also library): user-created bundles providing additional functionality to your local R installation\nfunctions: they take inputs, do some transformation or computation on them, and return a result (output)\n?: returns the help page of a function, e.g. ?dim.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nExample\n\n\n\n+\nAdds two numbers together\n\n2+2 - two plus two\n\n\n-\nSubtract one number from another\n\n3-1 - three minus one\n\n\n*\nMultiply two numbers together\n\n3*3 - three times three\n\n\n/\nDivide one number by another\n\n9/3 - nine divided by three\n\n\n()\ngroup operations together\n\n(2+2)/4 is different from 2+2/4\n\n\n\n^\nto the power of..\n\n4^2 - four to the power of two, or four squared\n\n\n<-\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx <- 10\n\n\n=\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx = 10\n\n\n<\nis less than?\n2 < 3\n\n\n>\nis greater than?\n2 > 3\n\n\n<=\nis less than or equal to?\n2 <= 3\n\n\n>=\nis greater than or equal to?\n2 >= 2\n\n\n==\nis equal to?\n(5+5) == 10\n\n\n!=\nis not equal to?\n(2+3) != 4\n\n\nc()\ncombines values into a vector (a sequence of values)\nc(1,2,3,4)"
  },
  {
    "objectID": "rd1_02.html",
    "href": "rd1_02.html",
    "title": "Categorical data",
    "section": "",
    "text": "Before we get started on the statistics, we’re going to briefly introduce a crucial bit of R code. We have seen already seen a few examples of R code such as:\n\n# show the dimensions of the data\ndim(somedata)\n\n# show a summary of the data\nsummary(somedata)\n\n# factorise and show the \"somevariable\" variable in the \"somedata\" dataframe \nas.factor(somedata$somevariable)\n\nAnd we can actually wrap functions inside functions:\n\n# factorise the \"somevariable\" variable in the \"somedata\" dataframe, \n# then show a summary of it\nsummary(as.factor(somedata$somevariable))\n\nR evaluates code from the inside-out!\nYou can end up with functions inside functions inside functions …\n\n# Don't worry about what all these functions do, \n# it's just an example -\nround(mean(log(cumsum(diff(1:10)))))\n\n[1] 1\n\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: %>%\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side. The %>% symbol gets called a “pipe”.\nLet’s see it in action with the starwars2 dataset. The data contains information on various characteristics of characters from Star Wars. Before we can use the pipe operator, %>%, we need to load the tidyverse packages, because that is where %>% is found.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nstarwars2 <- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\nstarwars2 %>%\n    head()\n\n# A tibble: 6 × 6\n  name           height hair_color  eye_color homeworld species\n  <chr>           <dbl> <chr>       <chr>     <chr>     <chr>  \n1 Luke Skywalker    172 blond       blue      Tatooine  Human  \n2 C-3PO             167 <NA>        yellow    Tatooine  Human  \n3 R2-D2              96 <NA>        red       Naboo     Droid  \n4 Darth Vader       202 none        yellow    Tatooine  Human  \n5 Leia Organa       150 brown       brown     Alderaan  Human  \n6 Owen Lars         178 brown, grey blue      Tatooine  Human  \n\nstarwars2 %>%\n    summary()\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWe can now write code that requires reading it from the inside-out:\n\nsummary(as.factor(starwars2$homeworld))\n\nor which requires reading it from left to right:\n\nstarwars2$homeworld %>%\n    as.factor() %>%\n    summary()\n\n      Alderaan    Aleen Minor         Bespin     Bestine IV Cato Neimoidia \n             3              1              1              1              1 \n         Cerea       Champala      Chandrila   Concord Dawn       Corellia \n             1              1              1              1              2 \n     Coruscant       Dathomir          Dorin          Endor         Eriadu \n             3              1              1              1              1 \n      Geonosis    Glee Anselm     Haruun Kal        Iktotch       Iridonia \n             1              1              1              1              1 \n         Kalee         Kamino       Kashyyyk      Malastare         Mirial \n             1              3              2              1              2 \n      Mon Cala     Muunilinst          Naboo      Nal Hutta           Ojom \n             1              1              8              1              1 \n       Quermia          Rodia         Ryloth        Serenno          Shili \n             1              1              2              1              1 \n         Skako        Socorro    Springfield        Stewjon        Sullust \n             1              1              2              1              1 \n      Tatooine       Toydaria      Trandosha        Troiken           Tund \n            10              1              1              1              1 \n        Utapau        Vulpter          Zolan \n             1              1              1 \n\n\nAnd that long line of code from above:\n\n# again, don't worry about all these functions, \n# just notice the difference in the two styles.\nround(mean(log(cumsum(diff(1:10)))))\n\nbecomes:\n\n1:10 %>%\n    diff() %>%\n    cumsum() %>%\n    log() %>%\n    mean() %>%\n    round()\n\nWe’re going to use this way of writing a lot throughout the course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with %>%."
  },
  {
    "objectID": "rd1_02.html#data-exploration",
    "href": "rd1_02.html#data-exploration",
    "title": "Categorical data",
    "section": "\n2 Data Exploration",
    "text": "2 Data Exploration\nOnce we have collected some data, one of the first things we want to do is explore it - and we can do this through describing (or summarising) and visualising variables.\nWe are already familiar with the function summary(), which provides high-level information about our data, showing us things such as the minimum and maximum and mean of continuous variables, or the numbers of entries falling into each possible response level for a categorical variable:\n\nsummary(starwars2)\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWhat we are doing here is providing numeric descriptions of the distributions of values in each variable.\n\n\n\n\n\n\nDistribution\nThe distribution of a variable shows how often different values occur. We’re going to focus on describing and visualising distributions of categorical data.\nThe graph showing the distribution of a variable shows us where the values are centred, how the values vary, and gives some information about where a typical value might fall. It can also alert you to the presence of outliers (unexpected observations)."
  },
  {
    "objectID": "rd1_02.html#unordered-categorical-nominal-data",
    "href": "rd1_02.html#unordered-categorical-nominal-data",
    "title": "Categorical data",
    "section": "\n3 Unordered Categorical (Nominal) Data",
    "text": "3 Unordered Categorical (Nominal) Data\nFor variables with a discrete number of response options, we can easily measure “how often” values occur in terms of their frequency.\n\n\n\n\n\n\nFrequency distribution\nA frequency distribution is an overview of all distinct values in some variable and the number of times they occur.\n\n\n\nSupposing that we have surveyed the people working in a psychology department and asked them what sub-discipline of psychological research they most strongly identify as working within (If you would like to work along with the reading, the data is available at https://uoepsy.github.io/data/psych_survey.csv).\n\n\nVariable Name\nDescription\n\n\n\nparticipant\nSubject identifier\n\n\narea\nRespondent’s sub-discpline of psychology\n\n\n\n First, we read our data in to R and store it in an object called “psych_disciplines”:\n\npsych_disciplines <- read_csv(\"https://uoepsy.github.io/data/psych_survey.csv\")\npsych_disciplines\n\n# A tibble: 74 × 2\n   participant   area                  \n   <chr>         <chr>                 \n 1 respondent_1  Differential          \n 2 respondent_2  Social                \n 3 respondent_3  Differential          \n 4 respondent_4  Social                \n 5 respondent_5  Differential          \n 6 respondent_6  Differential          \n 7 respondent_7  Language              \n 8 respondent_8  Language              \n 9 respondent_9  Cognitive Neuroscience\n10 respondent_10 Language              \n# … with 64 more rows\n\n\nWe can get the frequencies of different response levels of the discipline variable by using the following code:\n\n# start with the psych_disciplines dataframe \n# %>%\n# count() the values in the \"area\" variable \npsych_disciplines %>%\n    count(area)\n\n# A tibble: 5 × 2\n  area                       n\n  <chr>                  <int>\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\n\n\n\n\n\n\nExtra detail on how this works\n\n\n\n\n\nIn the code above, R knows to look for the area variable inside the psych_disciplines data because we used %>% to “pipe” in the psych_disciplines dataframe.\nWe could have also done:\n\n# count(data, variable)\ncount(psych_disciplines, area)\n\n# A tibble: 5 × 2\n  area                       n\n  <chr>                  <int>\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\nBut this would not work:\n\ncount(area)\n\n\nError in group_vars(x) : object ‘area’ not found\n\n\n\n\nFrequency table\nTo describe a distribution like this, we can simply provide the frequency table.\nLet’s store it as an object in R:\n\n# make a new object called \"freq_table\", and assign it:\n# the counts of values of \"area\" variable in \n# the psych_discipline dataframe.\nfreq_table <- \n    psych_disciplines %>%\n    count(area)\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 2\n  area                       n\n  <chr>                  <int>\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\nFor a report, we might want to make it a little more easily readable:\n\n\n\nCentral tendency\nOften, we might want to summarise data into a single summary value, reflecting the point at (or around) which most of the values tend to cluster. This is known as a measure of central tendency. For numeric data, we can use measures such as the mean, which you will likely have heard of. For nominal data (unordered categorical data), however, our only option is to use the mode.\n\n\n\n\n\n\nMode\nThe most frequent value (the value that occurs the greatest number of times).\n\n\n\nIn our case, the mode is the “Cognitive Neuroscience” category.\nRelative frequencies\nWe might alternatively want to show the percentage of respondents in each category, rather than the raw frequencies.\nThe percentages show the relative frequency distribution\n\n\n\n\n\n\nRelative frequency distribution\nA relative frequency distribution shows the proportion of times each value occurs\n(contrast this with the frequency distribution which shows the number of times).\nRelative frequencies can be written as fractions, percents, or decimals.\n\n\n\nIn the object “freq_table”, we have a variable called n, which is the frequencies (the number in each category).\nThe total of this column is equal to the total number of respondents:\n\n# sum all the values in the \"n\" variable in the \"freq_table\" object\nsum(freq_table$n)\n\n[1] 74\n\n\nAnd therefore, each value in freq_table$n, divided by the total, is equal to the proportion in each category:\n( Tip: Proportions are percentages/100. So 0.4 is another way of expressing 40%)\n\n# take the values in the \"n\" variable from the \"freq_table\" object, \n# and divide by the sum of all the values in the \"n\" variable in \"freq_table\"\nfreq_table$n/sum(freq_table$n)\n\n[1] 0.3243243 0.1351351 0.2702703 0.1216216 0.1486486\n\n\nWe can then simply add the proportions as a new column to our table of frequencies by assigning the values we just calculated to a new variable:\n\n# the variable \"prop\" in the \"freq_table\" object is now assigned \n# the values we calculated above (the proportions)\nfreq_table$prop <- freq_table$n/sum(freq_table$n)\n\n# print the \"freq_table\" object\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  <chr>                  <int> <dbl>\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\n However, we can also do this within a sequence of pipes (%>%). To do so, we use a new function called mutate().\n\n\n\n\n\n\nmutate\n\n\n\nThe mutate() function is used to add or modify variables to data.\n\n# take the data\n# %>%\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata %>%\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\nNote: Inside mutate(), we don’t have to keep using the dollar sign $, as we have already told it what data to look for variables in.\nTo ensure that our additions/modifications of variables are stored in R’s environment (rather than simply printed out), we need to reassign the name of our dataframe:\n\ndata <- \n  data %>%\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\n\n\nWe can actually add this step to our earlier code:\n\n# make a new object called \"freq_table\", and assign it:\n# the counts of values of \"area\" variable in \n# the psych_discipline dataframe.\n# from there, 'mutate' such that there is a variable called \"prop\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable.\nfreq_table <- \n  psych_disciplines %>%\n  count(area) %>%\n  mutate(\n    prop = n/sum(n)\n  )\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  <chr>                  <int> <dbl>\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\nVisualising\n\n“By visualizing information, we turn it into a landscape that you can explore with your eyes. A sort of information map. And when you’re lost in information, an information map is kind of useful.”_ David McCandless\n\nWe’re going to now make our first steps into the world of data visualisation. R is an incredibly capable language for creating visualisations of almost any kind. It is used by many media companies (e.g., the BBC), and has the capability of producing 3d visualisations, animations, interactive graphs, and more.\nWe are going to use the most popular R package for visualisation, ggplot2. This is actually part of the tidyverse, so if we have an Rmarkdown document and have loaded the tidyverse packages at the start (by using library(tidyverse)), then ggplot2 will be loaded too).\nRecall our frequency distribution table:\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  <chr>                  <int> <dbl>\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\nWe can plot these values as a bar chart:\n\nggplot(data = freq_table, aes(x = area, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n\n\n\n\nggplot components\nNote the key components of the ggplot code.\n\n\ndata = where we provide the name of the dataframe.\n\naes = where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the x-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\n\nThen we add (using +) some geometry. These are the shapes (in our case, the columns/bars), which will be put in the correct place according to what we specified in aes().\n\n\n+ geom_col() Adds columns to the plot.\n\n\n\n\n\n\n\n\n\n\nOptional - Different aes() and geoms, and labels\n\n\n\n\n\nUse these as reference for when you want to make changes to the plots you create.\nAdditionall, remember that google is your friend - there are endless forums with people asking how to do something in ggplot, and you can just copy and paste bits of code to add to your plots!\n\nFill the geoms:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()\n\n\n\n\n\nChange the axis labels:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")\n\n\n\n\n\nChange the geom:\n(Note that using geom_col had the y axis starting at 0, but geom_point starts just below the lowest value.\n\n\n# note that we also need to change \"fill = area\" to \"col = area\". \nggplot(data = freq_table, aes(x = area, y = n, col = area)) +\n    geom_point()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")\n\n\n\n\n\nChange the limits of the axes:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)\n\n\n\n\n\nRemove (or reposition) the legend:\n\n\n# setting legend.position as \"bottom\" would put it at the bottom!\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)+\n    theme(legend.position = \"none\") \n\n\n\n\n\nChanging the theme:\n\n\n# there are many predefine themes, including: \n# theme_bw(), theme_classic(), theme_light()\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)+\n    theme_minimal()"
  },
  {
    "objectID": "rd1_02.html#ordered-categorical-ordinal-data",
    "href": "rd1_02.html#ordered-categorical-ordinal-data",
    "title": "Categorical data",
    "section": "\n4 Ordered Categorical (Ordinal) Data",
    "text": "4 Ordered Categorical (Ordinal) Data\nRecall that ordinal data is categorical data which has a natural ordering of the possible responses. One of the most common examples of ordinal data which you will encounter in psychology is the Likert Scale. You will probably have come across these before, perhaps when completing online surveys or questionnaires.\n\n\n\n\n\n\nLikert Scale\nA five or seven point scale on which an individual express how much they agree or disagree with a particular statement.\n\n\n\nWith Likert data, there is a set of discrete response options (it is categorical data). The response options can be ranked, making it ordered categorical ( strongly disagree < disagree < neither < agree < strongly agree ). Importantly, the distance between responses is not measurable.\nFrequency table\nLet’s suppose that as well as collecting information on the sub-discipline of psychology they identified with, we also asked our respondents to rate their level of happiness from 1 to 5, as well as their job satisfaction from 1 to 5.\n\n\nVariable Name\nDescription\n\n\n\nparticipant\nSubject identifier\n\n\nhappiness\nRespondent’s level of happiness from 1 to 5\n\n\njob_sat\nRespondent’s level of job satisfaction from 1 to 5\n\n\n\n\npsych_survey <- read_csv(\"https://uoepsy.github.io/data/psych_survey2.csv\")\npsych_survey\n\n# A tibble: 74 × 3\n   participant   happiness job_sat\n   <chr>             <dbl>   <dbl>\n 1 respondent_1          3       3\n 2 respondent_2          3       4\n 3 respondent_3          2       5\n 4 respondent_4          4       5\n 5 respondent_5          3       5\n 6 respondent_6          4       4\n 7 respondent_7          4       2\n 8 respondent_8          5       5\n 9 respondent_9          1       5\n10 respondent_10         3       4\n# … with 64 more rows\n\n\nFor these questions (variables happiness and job_sat), we could do the same thing as we did above for unordered categorical data, and summarise this into frequencies:\n\n# take the \"psych_survey\" dataframe %>%\n# count() the values in the \"happiness\" variable \npsych_survey %>%\n    count(happiness)\n\n# A tibble: 5 × 2\n  happiness     n\n      <dbl> <int>\n1         1     6\n2         2    13\n3         3    27\n4         4    21\n5         5     7\n\n# take the \"psych_survey\" dataframe %>%\n# count() the values in the \"job_sat\" variable \npsych_survey %>%\n    count(job_sat)\n\n# A tibble: 5 × 2\n  job_sat     n\n    <dbl> <int>\n1       1     3\n2       2     6\n3       3    11\n4       4    16\n5       5    38\n\n\nCentral tendency\nWe could again use the Mode - the most common value - to summarise this data. However, because the responses are ordered, it can be more useful to think about the percentages of respondents in and below/above each category. For instance, we might want to talk about asking which category has 50% of the observations in a lower category, and 50% of the observations in a higher category. This is mid-point is known as the Median.\n\n\n\n\n\n\nMedian\nThe value for which 50% of observations a lower and 50% are higher. It is the mid-point of a list of ordered values.\nTo find the median:\n\nrank order the values\nfind the middle value:\n\nIf there are \\(n\\) values, find the value at position \\(\\frac{n+1}{2}\\).\n\nIf \\(n\\) is even, \\(\\frac{n+1}{2}\\) will not be a whole number.\nFor instance, if \\(n = 20\\), you are looking for the \\(\\frac{n+1}{2} = \\frac{20+1}{2} = 10.5^{th}\\) value.\n\nWhen calculating the median for ordinal data, if the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{n+1}{2}^{th}\\) values are different, report both.\nWhen calculating the median for numeric data, report the midpoint of the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{n+1}{2}^{th}\\) values.\n\n\n\n\n\n\n\n\nYou can tell R explicitly that a variable is of a certain type using functions such as as.factor(), as.numeric(), and so on.\nYou may notice that we haven’t done this yet with the data we have been working with in so far today:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and tell me what type/class it is\nclass(psych_survey$happiness)\n\n[1] \"numeric\"\n\n\nThis is because there are some benefits to letting R think your data is numeric, even when it is not. It means we can use functions such as median() to quickly find the median:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and find the median\nmedian(psych_survey$happiness)\n\n[1] 3\n\n\n\n\n\n\n\n\nCaution!\n\n\n\nWhile we can make R treat this data is numeric, it is important to remember that it is actually measured on an ordinal scale.\nFor example, if the median falls between levels, R will tell us that the median is the mid-point:\n\n# for the values 2,1,2,3,4,5, \n# find the median\nmedian(c(2,1,2,3,5,4))\n\n[1] 2.5\n\n\nBut because our data is ordinal, then we know that 2.5 is not a valid response.\n\n\nWe can also use functions such as min() and max() to find the minimum and maximum values:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and find the minimum value\nmin(psych_survey$happiness)\n\n[1] 1\n\n# and find the maximum value\nmax(psych_survey$happiness)\n\n[1] 5\n\n\nCumulative percentages, Quartiles\nIn calculating the median, we are going beyond talking about the relative frequencies (i.e., the percentage in each category), to talking about the cumulative percentage.\n\n\n\n\n\n\nCumulative percentage\nCumulative percentages are another way of expressing a frequency distribution.\nThey are the successive addition of percentages in each category. For example, the cumulative percentage for the 3rd category is the percentage of respondents in the 1st, 2nd and 3rd category:\n\n\n\n\n\n\n\n\n\n\n\nCategory\nFrequency count (n)\nRelative frequency (%)\nCumulative frequency\nCumulative percentage\n\n\n\nResponse 1\n10\n13.33\n10\n13.33\n\n\nResponse 2\n10\n13.33\n20\n26.67\n\n\nResponse 3\n20\n26.67\n40\n53.33\n\n\nResponse 4\n25\n33.33\n65\n86.67\n\n\nResponse 5\n10\n13.33\n75\n100.00\n\n\n\n\n\n\n\n\nWe saw before how we can calculate the proportions/percentages in each category:\n( Note: We multiply by 100 here to turn the proportion into a percentage)\n\n# take the \"psych_survey\" dataframe %>%\n# count() the values in the \"happiness\" variable (creates an \"n\" column), and\n# from there, 'mutate' such that there is a variable called \"percent\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable.\npsych_survey %>%\n  count(happiness) %>%\n  mutate(\n    percent = n/sum(n)*100\n  )\n\n# A tibble: 5 × 3\n  happiness     n percent\n      <dbl> <int>   <dbl>\n1         1     6    8.11\n2         2    13   17.6 \n3         3    27   36.5 \n4         4    21   28.4 \n5         5     7    9.46\n\n\nWe can add another variable, and make it the cumulative percentage, by using the cumsum() function.\n\n# take the \"psych_survey\" dataframe %>%\n# count() the values in the \"happiness\" variable (creates an \"n\" column), and\n# from there, 'mutate' such that there is a variable called \"percent\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable,\n# and also make a variable called \"cumulative_percent\" which is the \n# successive addition of the values in the \"percent\" variable\npsych_survey %>% \n  count(happiness) %>% \n  mutate(\n    percent = n/sum(n)*100,\n    cumulative_percent = cumsum(percent)\n  )\n\n# A tibble: 5 × 4\n  happiness     n percent cumulative_percent\n      <dbl> <int>   <dbl>              <dbl>\n1         1     6    8.11               8.11\n2         2    13   17.6               25.7 \n3         3    27   36.5               62.2 \n4         4    21   28.4               90.5 \n5         5     7    9.46             100   \n\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nThink about why this will not work:\n\npsych_survey %>% \n  count(happiness) %>% \n  mutate(\n    cumulative_percent = cumsum(percent),\n    percent = n/sum(n)*100\n  )\n\n\nError: object ‘percent’ not found\n\nAnswer: Inside the mutate() function, we are trying to assign the cumulative_percent variable based on the values of the percent variable. But in the code above, percent gets defined after cumulative_percent, and so it will not work. Hence the error message (“percent not found”).\n\n\n\nWhile the median splits the data in two (50% either side), you will often see data being split into four equal blocks.\nThe points which divide the four blocks are known as quartiles.\n\n\n\n\n\n\nQuartiles\nQuartiles are the points in rank-ordered data below which falls 25%, 50%, and 75% of the data.\n\nThe first quartile is the first category for which the cumulative percentage is \\(\\geq 25\\%\\).\n\nThe median is the first category for which the cumulative percentage is \\(\\geq 50\\%\\).\n\nThe third quartile is the first category for which the cumulative percentage is \\(\\geq 75\\%\\).\n\n\n\n\nBy looking at the quartiles, it gives us an idea of how spread out the data is.\nAs an example, if we had 10 categories A, B, C, D, E, F, G, H, I, J, and we knew that:\n\n\n\\(Q_1\\) (the \\(1^{st}\\) quartile) = G,\n\n\\(Q_2\\) (the \\(2^{nd}\\) quartile, the median) = H,\n\n\\(Q_3\\) (the \\(3^{rd}\\) quartile) = H,\n\nThis tells us that the first 25% of the data falls in one of the categories from A to G (quite a large range), the second 25% falls in categories G and H (a small range), and the third 25% of the data falls entirely in category H.\nSo a lot of the data is between G and H, with the data being more sparse in the lower and higher categories.\n\n\n\n\n\n\nLooking ahead to numeric data\nWe will talk about quartiles in numeric data too, where we commonly use the difference between the first and third quartile as a measure of how spread out the data are. This gets known as the inter-quartile range (IQR).\n\n\n\nVisualising\nWe can visualise ordered categorical data in the same way we did for unordered.\nFirst we save our frequencies/percentages as a new object:\n\nfreq_table2 <- psych_survey %>%\n  count(happiness) %>%\n  mutate(\n    percent = n/sum(n)*100\n  )\n\nThen we give that object to our ggplot code, with the appropriate aes() mappings:\n\n# make a ggplot with the object \"freq_table2\". \n# on the x axis put the possible values in the \"happiness\" variable,\n# on the y axis put the possible values in the \"n\" variable.\n# add columns for each entry in the data. \nggplot(data = freq_table2, aes(x = happiness, y = percent)) + \n  geom_col()"
  },
  {
    "objectID": "rd1_02.html#glossary",
    "href": "rd1_02.html#glossary",
    "title": "Categorical data",
    "section": "\n5 Glossary",
    "text": "5 Glossary\n\n\ndistribution: How often different possible values in a variable occur.\n\nfrequency: Number of occurrences (count) in a given response value.\n\nrelative frequency: Percentage/proportion of occurrences in a given response value.\n\ncumulative percentage: Percentage of occurrences in or below a given reponse value (requires ordered data).\n\nmode: Most common value.\n\nmedian: Middle value. \n\n\n%>% Takes the output of whatever is on the LHS and gives it as the input of whatever is on the RHS.\n\ncount() Counts the number of occurrences of each unique value in a variable.\n\nmutate() Used to add variables to the dataframe, or modify existing variables.\n\nmin() Returns the minimum value of a variable.\n\nmax() Returns the maximum value of a variable.\n\nmedian() Returns the median value of a variable.\n\nggplot() Creates a plot. Takes data= and a set of mappings aes() from the data to properties of the plot (e.g., x/y axes, colours).\n\ngeom_col() Adds columns to a ggplot."
  },
  {
    "objectID": "rd1_03.html",
    "href": "rd1_03.html",
    "title": "Numeric data",
    "section": "",
    "text": "In the previous week, we looked into describing and visualising categorical data. We looked at using the mode and median as measures of central tendency, before discussing how for ordered categorical data we could look at the quartiles (the points in rank-ordered data below which falls 25%, 50%, 75% and 100% of the data) to gain an understanding of how spread out the data are.\nWe now move to looking at measures of central tendency and of spread for numeric data."
  },
  {
    "objectID": "rd1_03.html#central-tendency",
    "href": "rd1_03.html#central-tendency",
    "title": "Numeric data",
    "section": "\n2 Central tendency",
    "text": "2 Central tendency\nIn the following examples, we are going to use some data on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests.\nIt is available at https://uoepsy.github.io/data/wechsler.csv\n\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\nsummary(wechsler)\n\n participant              iq              age            test1      \n Length:120         Min.   : 58.00   Min.   :20.00   Min.   :30.00  \n Class :character   1st Qu.: 88.00   1st Qu.:29.50   1st Qu.:45.75  \n Mode  :character   Median :100.50   Median :37.00   Median :49.00  \n                    Mean   : 99.33   Mean   :36.63   Mean   :49.33  \n                    3rd Qu.:109.00   3rd Qu.:44.00   3rd Qu.:53.25  \n                    Max.   :137.00   Max.   :71.00   Max.   :67.00  \n     test2      \n Min.   : 2.00  \n 1st Qu.:42.00  \n Median :52.50  \n Mean   :51.24  \n 3rd Qu.:62.00  \n Max.   :80.00  \n\n\nMode and median revisited\nWe saw for categorical data two different measures of central tendency:\n\n\nMode: The most frequent value (the value that occurs the greatest number of times).\n\n\nMedian: The value for which 50% of observations a lower and 50% are higher. It is the mid-point of the values when they are rank-ordered.\n\nWe applied both of these to categorical data, but we can also use them for numeric data.\n\n\n\n\n\n\n\n\n\nMode\nMedian\nMean\n\n\n\nNominal (unordered categorical)\n✔\n✘\n✘\n\n\nOrdinal (ordered categorical)\n✔\n✔\n\n?(you may see it sometimes for certain types of ordinal data - there’s no consensus)\n\n\nNumeric Continuous\n✔\n✔\n✔\n\n\n\n The mode of numeric variables is not frequently used. Unlike categorical variables where there are a distinct set of possible values which the data can take, for numeric variables, data can take a many more (or infinitely many) different values. Finding the “most common” is sometimes not possible. The most frequent value (the mode) of the iq variable is 97:\n\n# take the \"wechsler\" dataframe %>%\n# count() the values in the \"iq\" variable (creates an \"n\" column), and\n# from there, arrange() the data so that the \"n\" column is descending - desc()\nwechsler %>%\n    count(iq) %>%\n    arrange(desc(n))\n\n# A tibble: 53 × 2\n      iq     n\n   <dbl> <int>\n 1    97     7\n 2   108     6\n 3    92     5\n 4   103     5\n 5   105     5\n 6   110     5\n 7    85     4\n 8    99     4\n 9   107     4\n10   113     4\n# … with 43 more rows\n\n\nRecall that the median is found by ordering the data from lowest to highest, and finding the mid-point. In the wechsler dataset we have IQ scores for 120 participants. We find the median by ranking them from lowest to highest IQ, and finding the mid-point between the \\(60^{th}\\) and \\(61^{st}\\) participants’ scores.\nWe can also use the median() function with which we are already familiar:\n\nmedian(wechsler$iq)\n\n[1] 100.5\n\n\nMean\nOne of the most frequently used measures of central tendency for numeric data is the mean.\n\n\n\n\n\n\nMean: \\(\\bar{x}\\)\nThe mean is calculated by summing all of the observations together and then dividing by the total number of obervations (\\(n\\)).\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the mean is:\n\\[\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\]\n\n\n\n\n\n\nHelp reading mathematical formulae\n\n\n\n\n\nThis might be the first mathematical formula you have seen in a while, so let’s unpack it.\nThe \\(\\sum\\) symbol is used to denote a series of additions - a “summation”.\nWhen we include the bits around it: \\(\\sum\\limits_{i = 1}^{n}x_i\\) we are indicating that we add together all the terms \\(x_i\\) for values of \\(i\\) between \\(1\\) and \\(n\\): \\[\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n\\]\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the \\(1^{st}\\) to the \\(n^{th}\\) (where \\(n\\) is the total number of values), and we divide that by \\(n\\).\n\n\n\n\n\n\n\n\n\nSamples and populations\n\n\n\n\n\nStatistics is all about drawing inferences from some sampled data about the larger population from which it is sampled.\nA statistic which we calculate from our sample provides us with an estimate of something in the population (for instance, we might take the average age of students at Edinburgh University as an estimate of the age of all students).\nBecause of this, statisticians have different notations for when we are talking about populations vs talking about samples:\n\n\n\n\n\n\n\n\nSample\nPopulation\n\n\n\nNumber of observations\n\\(n\\)\n\\(N\\)\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\)\n\\(\\mu = \\frac{\\sum\\limits_{i = 1}^{N}x_i}{N}\\)\n\n\n\n\n\n\n\n\n\nWe can do the calculation by summing the iq variable, and dividing by the number of observations (in our case we have 120 participants):\n\n# get the values in the \"iq\" variable from the \"wechsler\" dataframe, and\n# sum them all together. Then divide this by 120\nsum(wechsler$iq)/120\n\n[1] 99.33333\n\n\nOr, more easily, we can use the mean() function:\n\nmean(wechsler$iq)\n\n[1] 99.33333\n\n\nSummarising variables\nFunctions such as mean(), median(), min() and max() can quickly summarise data, and we can use them together really easily in combination with summarise().\n\n\n\n\n\n\nsummarise()\nThe summarise() function is used to reduce variables down to a single summary value.\n\n# take the data %>%\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata %>%\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n\nNote: Just like with mutate() we don’t have to keep using the dollar sign $, as we have already told it what dataframe to look for the variables in.\n\n\n\nSo if we want to show the mean IQ score and the mean age of our participants:\n\n# take the \"wechsler\" dataframe %>%\n# summarise() it, such that there is a value called \"mean_iq\", which\n# is the mean() of the \"iq\" variable, and a value called \n# \"mean_age\" which is the mean() of the \"age\" variable. \nwechsler %>%\n    summarise(\n        mean_iq = mean(iq),\n        mean_age = mean(age)\n    )\n\n# A tibble: 1 × 2\n  mean_iq mean_age\n    <dbl>    <dbl>\n1    99.3     36.6"
  },
  {
    "objectID": "rd1_03.html#spread",
    "href": "rd1_03.html#spread",
    "title": "Numeric data",
    "section": "\n3 Spread",
    "text": "3 Spread\nInterquartile range\nIf we are using the median as our measure of central tendency and we want to discuss how spread out the spread are around it, then we will want to use quartiles (recall that these are linked: the \\(2^{nd}\\) quartile = the median).\nWe have already briefly introduced how for ordinal data, the 1st and 3rd quartiles give us information about how spread out the data are across the possible response categories. For numeric data, we can likewise find the 1st and 3rd quartiles in the same way - we rank-order all the data, and find the point at which 25% and 75% of the data falls below.\nThe difference between the 1st and 3rd quartiles is known as the interquartile range (IQR).( Note, we couldn’t take the difference for ordinal data, because “difference” would not be quantifiable - the categories are ordered, but intervals are between categories are unknown)\nIn R, we can find the IQR as follows:\n\nIQR(wechsler$age)\n\n[1] 14.5\n\n\nAlternatively, we can use this inside summarise():\n\n# take the \"wechsler\" dataframe %>%\n# summarise() it, such that there is a value called \"median_age\", which\n# is the median() of the \"age\" variable, and a value called \"iqr_age\", which\n# is the IQR() of the \"age\" variable.\nwechsler %>% \n  summarise(\n    median_age = median(age),\n    iqr_age = IQR(age)\n  )\n\n# A tibble: 1 × 2\n  median_age iqr_age\n       <dbl>   <dbl>\n1         37    14.5\n\n\nVariance\nIf we are using the mean as our as our measure of central tendency, we can think of the spread of the data in terms of the deviations (distances from each value to the mean).\nRecall that the mean is denoted by \\(\\bar{x}\\). If we use \\(x_i\\) to denote the \\(i^{th}\\) value of \\(x\\), then we can denote deviation for \\(x_i\\) as \\(x_i - \\bar{x}\\).\nThe deviations can be visualised by the red lines in Figure @ref(fig:deviations).\n\n\n\n\nDeviations from the mean\n\n\n\n\n\n\n\n\n\n\nThe sum of the deviations from the mean, \\(x_i - \\bar x\\), is always zero\n\\[\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n\\]\nThe mean is like a center of gravity - the sum of the positive deviations (where \\(x_i > \\bar{x}\\)) is equal to the sum of the negative deviations (where \\(x_i < \\bar{x}\\)).\n\n\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider squared deviations.\nSquaring the deviations makes them all positive. Observations far away from the mean in either direction will have large, positive squared deviations. The average squared deviation is known as the variance, and denoted by \\(s^2\\)\n\n\n\n\n\n\nVariance: \\(s^2\\)\nThe variance is calculated as the average of the squared deviations from the mean.\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the variance is:\n\\[s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nWhy n minus 1?\n\n\n\n\n\nThe top part of the equation \\(\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\) can be expressed in \\(n-1\\) terms, so we divide by \\(n-1\\) to get the average.Example: If we only have two observations \\(x_1\\) and \\(x_2\\), then we can write out the formula for variance in full quite easily. The top part of the equation would be: \\[\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\]\nThe mean for only two observations can be expressed as \\(\\bar{x} = \\frac{x_1 + x_2}{2}\\), so we can substitute this in to the formula above. \\[\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 \\qquad = \\qquad \\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2\n\\] Which simplifies down to one value: \\[\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2 \\qquad = \\qquad  \\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n\\]  So although we have \\(n=2\\) datapoints (\\(x_1\\) and \\(x_2\\)), the top part of the equation for the variance has only 1 (\\(n-1\\)) units of information. In order to take the average of these bits of information, we divide by \\(n-1\\).\n\n\n\n\n\n\nWe can get R to calculate this for us using the var() function:\n\nwechsler %>%\n  summarise(\n    variance_iq = var(iq)\n  )\n\n# A tibble: 1 × 1\n  variance_iq\n        <dbl>\n1        238.\n\n\nStandard deviation\nOne difficulty in interpreting variance as a measure of spread is that it is in units of squared deviations. It relects the typical squared distance from a value to the mean.\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the standard deviation.\n\n\n\n\n\n\nStandard Deviation: \\(s\\)\nThe standard deviation, denoted by \\(s\\), is a rough estimate of the typical distance from a value to the mean.\nIt is the square root of the variance (the typical squared distance from a value to the mean).\n\\[\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n\\]\n\n\n\nWe can get R to calculate the standard deviation of a variable sd() function:\n\nwechsler %>%\n  summarise(\n    variance_iq = var(iq),\n    sd_iq = sd(iq)\n  )\n\n# A tibble: 1 × 2\n  variance_iq sd_iq\n        <dbl> <dbl>\n1        238.  15.4"
  },
  {
    "objectID": "rd1_03.html#visualisations",
    "href": "rd1_03.html#visualisations",
    "title": "Numeric data",
    "section": "\n4 Visualisations",
    "text": "4 Visualisations\nBoxplots\nBoxplots provide a useful way of visualising the interquartile range (IQR). You can see what each part of the boxplot represents in Figure @ref(fig:boxplotdesc).\n\n\n\n\nAnatomy of a boxplot\n\n\n\n\nWe can create a boxplot of our age variable using the following code:\n\n# Notice, we put age on the x axis, making the box plot vertical. \n# If we had set aes(y = age) instead, then it would simply be rotated 90 degrees \nggplot(data = wechsler, aes(x = age)) +\n  geom_boxplot()\n\n\n\n\nHistograms\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these influence visualisations of numeric variables.\nWe can visualise numeric data using a histogram, which shows the frequency of values which fall within bins of an equal width.\n\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\nWe can specifiy the width of the bins:\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\nLet’s take a look at the means and standard deviations of participants’ scores on the other tests (the test1 and test2 variables):\n\nwechsler %>% \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n\n# A tibble: 1 × 4\n  mean_test1 sd_test1 mean_test2 sd_test2\n       <dbl>    <dbl>      <dbl>    <dbl>\n1       49.3     7.15       51.2     14.4\n\n\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n\n\n\n\n\nDensity curves\nIn addition to grouping numeric data into bins in order to produce a histogram, we can also visualise a density curve.\nFor the time being, you can think of the density as a bit similar to the notion of relative frequency, in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. Because there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, …), we could group the data into infinitely many bins. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()+\n  xlim(50,150)"
  },
  {
    "objectID": "rd1_03.html#skew",
    "href": "rd1_03.html#skew",
    "title": "Numeric data",
    "section": "\n5 Skew",
    "text": "5 Skew\nSkewness is a measure of asymmetry in a distribution. Distributions can be positively skewed or negatively skewed, and this influences our measures of central tendency and of spread to different degrees (see Figure @ref(fig:skewplot)).\n\n\n\n\nSkew influences the mean and median to different degrees."
  },
  {
    "objectID": "rd1_03.html#glossary",
    "href": "rd1_03.html#glossary",
    "title": "Numeric data",
    "section": "\n6 Glossary",
    "text": "6 Glossary\n\n\nInterquartile Range (IQR): The \\(3^{rd}\\) quartile minus the \\(1^{st}\\) quartile.\n\nMean: The sum of all observations divided by the total number of observations. The center of gravity of a variable.\n\nDeviation: The distance from an observation to the mean value.\n\nVariance: The average squared distance from observations to the mean value.\n\nStandard deviation: Square root of variance - can be thought of as the average distance from observations to the mean value.\n\nBoxplot: Displays the median and the IQR, and any extreme values.\n\n\nHistogram: Shows the frequency of values which fall within bins of an equal width.\n\nDensity curve: A curve for reflecting the distribution of a variable, for which the area under the curve sums to 1.\n\nSkew: A measure of asymmetry in a distribution. \n\n\nsummarise() To summarise variables into a single value according to whatever calculation we give it.\n\nIQR() To calculate the interquartile range for a given variable.\n\nmean() To calculate the mean of a given variable.\n\nsd() To calculate the standard deviation of a given variable.\n\nvar() To calculate the variance of a given variable.\n\ngeom_boxplot() To add a boxplot to a ggplot.\n\ngeom_histogram() To add a histogram to a ggplot.\n\ngeom_density() To add a density curve to a ggplot.\n\ngeom_vline() To add a vertical line to a ggplot."
  },
  {
    "objectID": "rd1_04.html",
    "href": "rd1_04.html",
    "title": "Visualising and describing relationships",
    "section": "",
    "text": "In the previous couple of weeks, we looked at how to handle different types of data, and how to describe and visualise categorical and numeric distributions. More often than not, research involves investigating relationships between variables, rather than studying variables in isolation.\n\n\n\n\n\n\nIf we are using one variable to help us understand or predict values of another variable, we call the former the explanatory variable and the latter the outcome variable.\nOther names\n\noutcome variable = dependent variable = response variable = Y\nexplanatory variable = independent variable = predictor variable = X\n\n(referring to outcome/explanatory variables as Y and X respectively matches up with how we often want to plot them - the outcome variable on the y-axis, and the explanatory variable on the x-axis)\n\n\n\nThe distinction between explanatory and outcome variables is borne out in how we design experimental studies - the researcher manipulates the explanatory variable for each unit before the response variable is measured (for instance, we might randomly allocate participants to one of two conditions). This contrasts with observational studies in which the researcher does not control the value of any variable, but simply observes the values as they naturally exist.\nWe’re going to use data from a Stroop task.\n\n\n\n\n\n\nStroop data\n\n\n\nThe data we are going to use for these exercises is from an experiment using one of the best known tasks in psychology, the “Stroop task”. 130 participants completed an online task in which they saw two sets of coloured words. Participants spoke out loud the colour of each word, and timed how long it took to complete each set. In the first set of words, the words matched the colours they were presented in (e.g., word “blue” was coloured blue). In the second set of words, the words mismatched the colours (e.g., the word “blue” was coloured red, see Figure @ref(fig:stroop)). Participants’ recorded their times for each set (matching and mismatching).\nParticipants were randomly assigned to either do the task once only, or to record their times after practicing the task twice.\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html.\nThe data is available at https://uoepsy.github.io/data/strooptask.csv\n\n\n\n\nStroop Task - Color word interference. Images from https://faculty.washington.edu/chudler/java/ready.html\n\n\n\n\n\n\n\nlibrary(tidyverse)\nstroopdata <- read_csv(\"https://uoepsy.github.io/data/strooptask.csv\")\n\n# calculate the \"stroop effect\" - the difference in time taken to complete\n# the matching vs mismatching sets \nstroopdata <- \n    stroopdata %>% \n        mutate(\n            stroop_effect = mismatching - matching\n        )\n\nThe data is experimental - researchers controlled the presentation of the stimuli (coloured words) and the assignment of whether or not participants received practice.\nThe researchers are interested in two relationships:\n\nthe relationship between receiving practice (categorical) and the stroop-effect (numeric)\nthe relationship between age (numeric) and the stroop-effect (numeric)"
  },
  {
    "objectID": "rd1_04.html#numeric-and-categorical",
    "href": "rd1_04.html#numeric-and-categorical",
    "title": "Visualising and describing relationships",
    "section": "\n2 Numeric and Categorical",
    "text": "2 Numeric and Categorical\nRecall that the “stroop-effect” is the difference (in seconds) between participants’ times on the mismatching set of words vs the matching set. We know how to describe a numeric variable such as the stroop-effect, for instance by calculating the mean and standard deviation, or median and IQR. We saw how to produce visualisations of numeric variables in the form of density curves, histogram, and boxplots.\n\n# take the \"stroopdata\" dataframe %>%\n# summarise() it, such that there is a value called \"mean_stroop\", which\n# is the mean() of the \"stroop_effect\" variable, and a value called \"sd_stroop\", which\n# is the standard deviation of the \"stroop_effect\" variable.\nstroopdata %>% \n  summarise(\n    mean_stroop = mean(stroop_effect),\n    sd_stroop = sd(stroop_effect)\n  )\n\n# A tibble: 1 × 2\n  mean_stroop sd_stroop\n        <dbl>     <dbl>\n1        2.40      5.02\n\n\n\nggplot(data = stroopdata, aes(x = stroop_effect)) + \n  geom_histogram()\n\n\n\n\nTo understand the relationship between categorical (practice) and the numeric (stroop effect), for now we will simply calculate these summary statistics for the numeric variable when it is split by the different levels in the categorical variable.\nIn other words, we want to calculate the mean and standard deviation of the stroop_effect variable separately for those observations where practice is “no”, and for those where practice is “yes”:\n\n\n\n\npractice\nstroop_effect\n\n\n\nno\n9.69\n\n\nno\n10.07\n\n\nyes\n-2.97\n\n\nyes\n-0.23\n\n\nyes\n-5.59\n\n\nno\n3.67\n\n\nyes\n1.41\n\n\nyes\n2.1\n\n\nyes\n-0.33\n\n\n…\n…\n\n\n\n\n\nWe can do this using the group_by() function.\n\n\n\n\n\n\ngroup_by()\nThe group_by() function creates a grouping in the dataframe, so that subsequent functions will be computed on each group.\nIt is most useful in combination with summarise(), to reduce a variable into a summary value for each group in a grouping variable:\n\n# take the data %>%\n# make it grouped by each unique value in the \"grouping_variable\" %>%\n# summarise() it FOR EACH GROUP, creating a value called \"summary_value\" ()\ndata %>% \n  group_by(grouping_variable) %>%\n  summarise(\n    summary_value = ...\n  )\n\n\n\n\nLet’s do this for the Stroop Task data - we will summarise() the stroop_effect variable, after grouping the data by the practice variable:\n\n# take the \"stroopdata\" %>%\n# and group it by each unique value in the \"practice\" variable (yes/no) %>%\n# then summarise() it FOR EACH GROUP, creating summary values called \n# \"mean_stroop\" and \"sd_stroop\" which are the means and standard deviations of \n# the \"stroop_effect\" variable entries for each group of \"practice\".\nstroopdata %>%\n  group_by(practice) %>%\n  summarise(\n    mean_stroop = mean(stroop_effect),\n    sd_stroop = sd(stroop_effect)\n  )\n\n# A tibble: 2 × 3\n  practice mean_stroop sd_stroop\n  <chr>          <dbl>     <dbl>\n1 no            4.54        4.25\n2 yes           0.0229      4.75\n\n\nVisualising - Colours\nGiven the output above, which of the following visualisations is most representative of these statistics?\n\n\n\n\n\n\n\n\n\n\n\nWe know that the stroop effect for those with practice (blue line) was on average less than those without practice (red line). Both figures A and C don’t fit with this.\nIn both of the figures B and D, the blue (with practice) distribution peaks at about 0, and the red (without practice) distribution peaks at about 5. However, in the figure D, the red distribution is much flatter and wider. It has a larger standard deviation than the blue distribution. In our calculations above, the distributions have very similar standard deviations.\nSo the best visualisation of the two means and standard deviations we calculated is figure B.\n\n\n\nWe can visualise the data using the same code we had before, but with one small addition - we tell ggplot to colour the data according to the different values in the practice variable.\nNote we add this inside the aes() mappings, because we are mapping something on the plot (the colour) to something in the data (the practice variable). If we just wanted to make the line blue, we could put col = \"blue\" outside the aes().\n\nggplot(data = stroopdata, aes(x = stroop_effect, col = practice)) +\n  geom_density()\n\n\n\n\nVisualising - Facets\nInterpreting two density curves on top of one another works well, but overlaying two histograms on top of one another doesn’t. Instead, we might want to create separate histograms for each set of values (the stroop_effect variable values for each of practice/no practice groups).\nfacet_wrap() is a handy part of ggplot which allows us to easily split one plot into many:\n\nggplot(data = stroopdata, aes(x = stroop_effect)) +\n  geom_histogram() +\n  facet_wrap(~practice)"
  },
  {
    "objectID": "rd1_04.html#numeric-and-numeric",
    "href": "rd1_04.html#numeric-and-numeric",
    "title": "Visualising and describing relationships",
    "section": "\n3 Numeric and Numeric",
    "text": "3 Numeric and Numeric\nWhen we are interested in the relationship between two numeric variables, such as the one we have between age and the stroop-effect, the most easily interpreted visualisation of this relationship is in the form of a scatterplot:\n\n# make a ggplot with the stroopdata\n# put the possible values of the \"age\" variable on the x axis,\n# and put the possible values of the \"stroop_effect\" variable on the y axis.\n# for each entry in the data, add a \"tomato1\" coloured geom_point() to the plot, \nggplot(data = stroopdata, aes(x = age, y = stroop_effect)) +\n  geom_point(col=\"tomato1\")\n\n\n\n\nThe visual pattern that these points make on the plot tells us something about the data - it looks like the older participants tended to have a greater stroop-effect.\nBut we can also have relationships between two numeric variables that look the opposite, or have no obvious pattern, or have a more consistent patterning (see Figure @ref(fig:numnumrels))\n\n\n\n\nRelationships between two numeric variables can look very different\n\n\n\n\nAs a means of summarising these different types of relationships, we can calculate the covariance to describe in what direction, and how strong (i.e., how clear and consistent) the pattern is.\nCovariance\nWe know that variance is the measure of how much a single numeric variable varies around its mean.Covariance is a measure of how two numeric variables vary together, and can express the directional relationship between them.\n\n\n\n\n\n\nCovariance is the measure of how two variables vary together.\nFor samples, covariance is calculated using the following formula:\n\\[\\mathrm{cov}(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})\\]\nwhere:\n\n\n\\(x\\) and \\(y\\) are two variables;\n\n\\(i\\) denotes the observational unit, such that \\(x_i\\) is value that the \\(x\\) variable takes on the \\(i\\)th observational unit, and similarly for \\(y_i\\);\n\n\\(n\\) is the sample size.\n\n\n\n\n\n\n\nIt often helps to understand covariance by working through a visual explanation. Consider the following scatterplot:\n\n\n\n\n\n Now let’s superimpose a vertical dashed line at the mean of \\(x\\) (\\(\\bar{x}\\)) and a horizontal dashed line at the mean of \\(y\\) (\\(\\bar{y}\\)):\n\n\n\n\n\n Now let’s pick one of the points, call it \\(x_i\\), and show \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\).\nNotice that this makes a rectangle.\nAs \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\) are both positive values, their product - \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) - is positive.\n\n\n\n\n\n In fact, for all these points in red, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is positive (remember that a negative multiplied by a negative gives a positive):\n\n\n\n\n\n And for these points in blue, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is negative:\n\n\n\n\n\n Now take another look at the formula for covariance:\n\\[\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}\\]\nIt is the sum of all these products divided by \\(n-1\\). It is the average of the products! We can easily calculate the covariance between variables in R using the cov() function. cov() takes two variables cov(x = , y = ).\nWe can either use the $ to pull out the variables from the datset:\n\ncov(stroopdata$age, stroopdata$stroop_effect)\n\n[1] 23.9597\n\n\nOr we can specify the dataframe, use the %>% symbol, and call cov() inside summarise():\n\nstroopdata %>%\n  summarise(\n    mean_age = mean(age),\n    mean_stroop = mean(stroop_effect),\n    cov_agestroop = cov(age, stroop_effect)\n  )\n\n# A tibble: 1 × 3\n  mean_age mean_stroop cov_agestroop\n     <dbl>       <dbl>         <dbl>\n1     42.8        2.40          24.0"
  },
  {
    "objectID": "rd1_04.html#categorical-and-categorical",
    "href": "rd1_04.html#categorical-and-categorical",
    "title": "Visualising and describing relationships",
    "section": "\n4 Categorical and Categorical",
    "text": "4 Categorical and Categorical\nWhat if we are interested in the relationship between two variables that are both categorical?\nAs a quick example, let’s read in a dataset containing information on passengers from the Titanic. We can see from the first few rows of the dataset that there are quite a few categorical variables here:\n\ntitanic <- read_csv(\"https://uoepsy.github.io/data/titanic.csv\")\nhead(titanic)\n\n# A tibble: 6 × 5\n   ...1 class     age    sex   survived\n  <dbl> <chr>     <chr>  <chr> <chr>   \n1     1 1st class adults man   yes     \n2     2 1st class adults man   yes     \n3     3 1st class adults man   yes     \n4     4 1st class adults man   yes     \n5     5 1st class adults man   yes     \n6     6 1st class adults man   yes     \n\n\nRecall that we summarised one categorical variable using a frequency table:\n\ntitanic %>%\n  count(survived)\n\n# A tibble: 2 × 2\n  survived     n\n  <chr>    <int>\n1 no         817\n2 yes        499\n\n\nWe can also achieve this using the table() function:\n\n#thes two lines of code do exactly the same thing!\ntable(titanic$survived)\ntitanic %>% select(survived) %>% table()\n\n\n\n\n no yes \n817 499 \n\n\nContingency Tables\nLet’s suppose we are interested in how the Class of passengers’ tickets (1st Class, 2nd Class, 3rd Class) can be used to understand their survival.\nWe can create two-way table, where we have each variable on either dimension of the table:\n\n#Either this:\ntable(titanic$class, titanic$survived)\n# or this:\ntitanic %>% select(class, survived) %>% table()\n\n\n\n           \n             no yes\n  1st class 122 203\n  2nd class 167 118\n  3rd class 528 178\n\n\nAnd we can pass this to prop.table() to turn these into proportions.\nWe can turn them into:\n\nproportions of the total:\n\n\ntitanic %>% \n    select(class, survived) %>% \n    table() %>%\n    prop.table()\n\n           survived\nclass               no        yes\n  1st class 0.09270517 0.15425532\n  2nd class 0.12689970 0.08966565\n  3rd class 0.40121581 0.13525836\n\n\n\nproportions of each row:\n\n\ntitanic %>% \n    select(class, survived) %>% \n    table() %>%\n    prop.table(margin = 1)\n\n           survived\nclass              no       yes\n  1st class 0.3753846 0.6246154\n  2nd class 0.5859649 0.4140351\n  3rd class 0.7478754 0.2521246\n\n\n\nproportions of each column:\n\n\ntitanic %>% \n    select(class, survived) %>% \n    table() %>%\n    prop.table(margin = 2)\n\n           survived\nclass              no       yes\n  1st class 0.1493268 0.4068136\n  2nd class 0.2044064 0.2364729\n  3rd class 0.6462668 0.3567134\n\n\nMosaic Plots\nThe equivalent way to visualise a contingency table is in the form of a mosaic plot.\n\ntitanic %>% \n  select(class, survived) %>% \n  table() %>%\n  plot()\n\n\n\n\nYou can think of the prop.table(margin = ) as scaling the areas of one of the variables to be equal:\n\ntitanic %>% \n  select(class, survived) %>% \n  table() %>%\n  prop.table(margin = 1)\n\n           survived\nclass              no       yes\n  1st class 0.3753846 0.6246154\n  2nd class 0.5859649 0.4140351\n  3rd class 0.7478754 0.2521246\n\n\nIn the table above, each row (representing each level of the “Class” variable) sums to 1. The equivalent plot would make each of level of the “Class” variable as the same area:\n\ntitanic %>% \n  select(class, survived) %>% \n  table() %>%\n  prop.table(margin = 1) %>%\n  plot()"
  },
  {
    "objectID": "rd1_04.html#glossary",
    "href": "rd1_04.html#glossary",
    "title": "Visualising and describing relationships",
    "section": "\n5 Glossary",
    "text": "5 Glossary\n\n\nExplanatory variable: A variable used to understand or predict values of an outcome variable.\n\nOutcome variable: A variable which we are aiming to understand or predict via some explanatory variable(s).\n\nScatterplot: A plot in which the values of two variables are plotted along the two axes, the pattern of the resulting points revealing any relationship which is present.\n\nCovariance: A measure of the extent to which two variables vary together. \n\n\ncov() To calculate the covariance between two variables.\n\ngroup_by() To apply a grouping in a dataframe for each level of a given variable. Grouped dataframes will retain their grouping, so that if we use summarise() it will provide a summary calculation for each group.\n\ngeom_point() To add points/dots to a ggplot.\n\nfacet_wrap() To split a ggplot into multiple plots (facets) for each level of a given variable."
  },
  {
    "objectID": "rd1_05.html",
    "href": "rd1_05.html",
    "title": "Types of relationships",
    "section": "",
    "text": "You have seen by now how to visualise the distribution of a variable, and how to visualise a relationship between two variables. Relationships between two variables can look very different, and can follow different patterns. These patterns can be expressed mathematically in the form of functions.\n\n\n\n\n\n\nFunctions\nA function is a mapping between two sets of numbers (\\(x\\) and \\(y\\)) - associating every element of \\(x\\) with an element in \\(y\\).\nWe often denote functions using the letter \\(f\\), in that we state that \\(y = f(x)\\) (“y equals f of x”).\nFor example, there is a mapping between these two sets:\n\\[x=\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\ 10 \\\\ \\end{bmatrix}\\]\nAnd we can write this mapping as:\\[f(x) = x + 2\\]\nAnd we could visualise this relationship between \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\nIn statistics, we often attempt to summarise the pattern that is present in the data using linear functions.\nImagine that we plant 10 trees, and measure their heights each year for 10 years. We could visualise this data (the relationship between time and tree height) on a scatterplot (we have added some lines to the plot to show which tree is which):\n\n\n\n\n\nWe might sensibly choose to describe this pattern as a line: \n\n\n\n\n\nAnd in order to describe a line like this, we require two things:\n\nThe starting point (i.e., where it crosses the y-axis)\nThe amount it goes up every year.\n\nWhen we planted the trees (at year 0), they were on average about 5cm tall. So this is where our line starts.\nFor every year, the trees grew by about 10cm on average. So we can now describe tree height as a function of time:\\[\\textrm{Tree height} = 5 + (10 \\times \\textrm{Years})\\]\nWe can write this in terms of \\(x\\) and \\(y\\):\n\n\n\\(y = f(x)\\)         “\\(y\\) is some function \\(f\\) of \\(x\\)”\n\n\\(f(x) = 5 + 10x\\)         “the function \\(f\\) maps each value \\(x_i\\) to \\(5 + (10 \\times x_i)\\)”\n\nFunctions don’t have to be linear. Often, we might want to describe relationships which appear to be more complex than a straight line.\nFor example, it is often suggested that for difficult tasks, some amount of stress may improve performance (but not too little or too much). We might think of the relationship between performance and stress as a curve (Figure 1). \n\n\n\n\nFigure 1: Yerkes Dodson Law\n\n\n\n\n One way to describe curves is to use polynomials (\\(x^2\\), \\(x^3\\), etc.).\nFor example, in the following two sets, \\(y\\) can be described as \\(f(x)\\) where \\(f(x)=x^2\\):\n\\[x=\\begin{bmatrix} -5 \\\\ -4 \\\\ -3 \\\\ -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 25 \\\\ 16 \\\\ 9 \\\\ 4 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25 \\end{bmatrix}\\]\nand when we plot each value of \\(x\\) against the corresponding value of \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\nThe code to create the above plot\n\n\n\n\n\n\n# the tibble() function can be used to create a dataframe\n# here, we create one with two variables, x and y.\n# x is the sequence from -5 to 5,\n# y is equal to x squared. \n# we save this data as \"poly\"\npoly <- \n    tibble(\n        x = c(-5,-4,-3,-2,-1,0,1,2,3,4,5),\n        y = x^2\n    )\n# create a plot with \"poly\", with variable \"x\" on the x-axis,\n# and variable \"y\" on the y-axis. Add geom_points for each entry\nggplot(data = poly, aes(x = x, y = y)) + \n    geom_point()"
  },
  {
    "objectID": "rd1_05.html#transformations",
    "href": "rd1_05.html#transformations",
    "title": "Types of relationships",
    "section": "\n2 Transformations",
    "text": "2 Transformations\nWe have seen previously how we might change all the values in a variable, for instance if we want to turn heights from centimetres to metres:\n\n# read in the starwars dataset and assign it the name \"starwars2\"\nstarwars2 <- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\n\n# take the starwars2 dataframe %>%\n# mutate it such that there is a new variable called \"height_m\",\n# the values of which are equal to the \"height\" variable divided by 100.\n# then, select only the \"height\" and \"height_m\" columns (this is just \n# to make it easier to see without all the other variables)\nstarwars2 %>%\n    mutate(\n        height_m = height/100\n    ) %>% \n    select(height, height_m)\n\n# A tibble: 75 × 2\n   height height_m\n    <dbl>    <dbl>\n 1    172     1.72\n 2    167     1.67\n 3     96     0.96\n 4    202     2.02\n 5    150     1.5 \n 6    178     1.78\n 7    165     1.65\n 8     97     0.97\n 9    183     1.83\n10    182     1.82\n# … with 65 more rows\n\n\nWhat we have done here, can be described as a transformation, in that we have applied a mathematical function to the values in the height variable.\n\n\n\n\n\n\nTransformation\nData transformation is when we apply a deterministic function to map each value of a variable to a transformed value.\nWe transform for various reasons. For instance, we can use it to change the units we are interpreting (e.g., cm to m), or to change the shape of a distribution (e.g., make it less skewed).\n\n\n\nWe could even plot the heights in cm and heights in m against one another (note what units are on each axis):\n\n\n\n\n\n The relationship between a variable and a transformed variable need be linear, for example, log transformation: \n\n\n\n\n\n\n2.1 A recap of Logarithms and Natural Logarithms\n\n\n\n\n\n\nLogarithm\nA logarithm (log) is the power to which a number must be raised in order to get some other number.\nTake as examples \\(10^2\\) and \\(10^3\\):\n\\[\n10^2 = 100 \\qquad \\Longleftrightarrow \\qquad \\log_{10} (100) = 2 \\\\\n10^3 = 1000 \\qquad \\Longleftrightarrow \\qquad \\log_{10} (1000) = 3\n\\]\nWe refer to \\(\\log_{10}\\) as “Log base 10”.\n\n\n\n\n\n\n\n\n\nNatural log\nA special case of the logarithm is referred to as the natural logarithm, and is denoted by \\(ln\\) or \\(log_e\\), where \\(e = 2.718281828459...\\)\n\\(e\\) is a special number for which \\(log_e(e) = 1\\).\nIn R, this is just the log() function, which uses the base \\(e\\) by default.\n\n# Natural logarithm of e is 1:\nlog(2.718281828459, base = 2.718281828459)\n\n[1] 1\n\nlog(2.718281828459)\n\n[1] 1"
  },
  {
    "objectID": "rd1_05.html#centering-and-standardisation",
    "href": "rd1_05.html#centering-and-standardisation",
    "title": "Types of relationships",
    "section": "\n3 Centering and Standardisation",
    "text": "3 Centering and Standardisation\nRecall our dataset from our introduction to handling numerical data, in which we had data on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale, WAIS), their ages, and their scores on 2 other tests. We know how to calculate the mean and standard deviation of the IQ scores:\n\n# read in the data\nwechsler <- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n\n# calculate the mean and sd of IQs\nwechsler %>% \n  summarise(\n    mean_iq = mean(iq),\n    sd_iq = sd(iq)\n  )\n\n# A tibble: 1 × 2\n  mean_iq sd_iq\n    <dbl> <dbl>\n1    99.3  15.4\n\n\nTwo very useful transformations we can apply to a variable are centering and standardisation.\n\n\nCentering A transformation which re-expresses each value as the distance from a given number (e.g., the mean).\n\n\nStandardising A transformation which re-expresses each value as the distance from the mean in units of standard deviations.\n\n\n\n\n\n\n\n\nMean-centering\nTo Mean-center a variable, we simply subtract the mean from each value, \\(x_i - \\bar{x}\\): \\[\n\\textrm{raw IQ} = \\begin{bmatrix} 71 \\\\ 103 \\\\ 74 \\\\ 108 \\\\ 118 \\\\ 129 \\\\ ... \\end{bmatrix}, \\qquad\n\\textrm{mean centered IQ} = \\begin{bmatrix} 71-99.3 \\\\ 103-99.3 \\\\ 74-99.3 \\\\ 108-99.3 \\\\ 118-99.3 \\\\ 129-99.3 \\\\ ... \\end{bmatrix} = \\begin{bmatrix} -28.3 \\\\ 3.7 \\\\ -25.3 \\\\ 8.7 \\\\ 18.7 \\\\ 29.7 \\\\ ... \\end{bmatrix}\n\\]\n\n\n\nTo mean-center in R, we can simply add a new variable using mutate() and subtract the mean IQ from the IQ variable:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_meancenter\" for which\n# the entries are equal to the \"iq\" variable entries minus the \n# mean of the \"iq\" variable\nwechsler %>%\n  mutate(\n    iq_meancenter = iq - mean(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2 iq_meancenter\n   <chr>       <dbl> <dbl> <dbl> <dbl>         <dbl>\n 1 ppt_1          71    27    46    50        -28.3 \n 2 ppt_2         103    38    42    29          3.67\n 3 ppt_3          74    20    50    77        -25.3 \n 4 ppt_4         108    46    50    62          8.67\n 5 ppt_5         118    45    60    29         18.7 \n 6 ppt_6         129    33    45    45         29.7 \n 7 ppt_7         103    49    42    41          3.67\n 8 ppt_8         120    27    63    33         20.7 \n 9 ppt_9          96    37    53    44         -3.33\n10 ppt_10         80    26    53    21        -19.3 \n# … with 110 more rows\n\n\n\n\n\n\n\n\nStandardisation\nWhen we standardise a variable, we call the transformed values z-scores. To transform a given value \\(x_i\\) into a z-score, we simply calculate the distance from \\(x_i\\) to the mean, \\(\\bar{x}\\), and divide this by the standard deviation, \\(s\\)\n\\[\nz_i = \\frac{x_i - \\bar{x}}{s}\n\\]\nSo for each of the raw IQ scores, we can transform them to z-scores by subtracting the mean and then dividing by the standard deviation. The resulting values tell us how low/high each participant’s IQ score is compared to observed distribution of scores:\\[\n\\textrm{raw IQ} = \\begin{bmatrix} 71 \\\\ 103 \\\\ 74 \\\\ 108 \\\\ 118 \\\\ 129 \\\\ ... \\end{bmatrix}, \\qquad\n\\textrm{standardised IQ} = \\begin{bmatrix} \\frac{71-99.3}{15.43} \\\\ \\frac{103-99.3}{15.43} \\\\ \\frac{74-99.3}{15.43} \\\\ \\frac{108-99.3}{15.43} \\\\ \\frac{118-99.3}{15.43} \\\\ \\frac{129-99.3}{15.43} \\\\ ... \\end{bmatrix} = \\begin{bmatrix} -1.84 \\\\ 0.238 \\\\ -1.64 \\\\ 0.562 \\\\ 1.21 \\\\ 1.92 \\\\ ... \\end{bmatrix}\n\\]\n\n\n\nWe can achieve this in R either by manually performing the calculation:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_z\" for which\n# the entries are equal to the \"iq\" variable entries minus the mean of the \"iq\"\n# variable, divided by the standard deviation of the \"iq\" variable.  \nwechsler %>% \n  mutate(\n    iq_z = (iq - mean(iq)) / sd(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2   iq_z\n   <chr>       <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1 ppt_1          71    27    46    50 -1.84 \n 2 ppt_2         103    38    42    29  0.238\n 3 ppt_3          74    20    50    77 -1.64 \n 4 ppt_4         108    46    50    62  0.562\n 5 ppt_5         118    45    60    29  1.21 \n 6 ppt_6         129    33    45    45  1.92 \n 7 ppt_7         103    49    42    41  0.238\n 8 ppt_8         120    27    63    33  1.34 \n 9 ppt_9          96    37    53    44 -0.216\n10 ppt_10         80    26    53    21 -1.25 \n# … with 110 more rows\n\n\nOr we can use the scale() function:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_std\" for which\n# the entries are equal to the scaled values of the \"iq\" variable.  \nwechsler %>%\n  mutate(\n    iq_std = scale(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2 iq_std[,1]\n   <chr>       <dbl> <dbl> <dbl> <dbl>      <dbl>\n 1 ppt_1          71    27    46    50     -1.84 \n 2 ppt_2         103    38    42    29      0.238\n 3 ppt_3          74    20    50    77     -1.64 \n 4 ppt_4         108    46    50    62      0.562\n 5 ppt_5         118    45    60    29      1.21 \n 6 ppt_6         129    33    45    45      1.92 \n 7 ppt_7         103    49    42    41      0.238\n 8 ppt_8         120    27    63    33      1.34 \n 9 ppt_9          96    37    53    44     -0.216\n10 ppt_10         80    26    53    21     -1.25 \n# … with 110 more rows\n\n\nWe can also use the scale() function to mean-center a variable, by setting scale(variable, center = TRUE, scale = FALSE):\n\n# create two new variables in the \"wechsler\" dataframe, one which is \n# mean centered iq, and one which is standardised iq:\nwechsler %>%\n  mutate(\n    iq_mc = scale(iq, center = TRUE, scale = FALSE),\n    iq_std = scale(iq, center = TRUE, scale = TRUE) # these are the default settings\n  )\n\n# A tibble: 120 × 7\n   participant    iq   age test1 test2 iq_mc[,1] iq_std[,1]\n   <chr>       <dbl> <dbl> <dbl> <dbl>     <dbl>      <dbl>\n 1 ppt_1          71    27    46    50    -28.3      -1.84 \n 2 ppt_2         103    38    42    29      3.67      0.238\n 3 ppt_3          74    20    50    77    -25.3      -1.64 \n 4 ppt_4         108    46    50    62      8.67      0.562\n 5 ppt_5         118    45    60    29     18.7       1.21 \n 6 ppt_6         129    33    45    45     29.7       1.92 \n 7 ppt_7         103    49    42    41      3.67      0.238\n 8 ppt_8         120    27    63    33     20.7       1.34 \n 9 ppt_9          96    37    53    44     -3.33     -0.216\n10 ppt_10         80    26    53    21    -19.3      -1.25 \n# … with 110 more rows\n\n\n\n3.1 Test norms\nMany neuropsychological tests will have norms - parameters which describe the distribution of scores on the test in a normal population. For instance, in a normal adult population, scores on the WAIS have a mean of 100 and a standard deviation of 15.  What this means is that rather than calculating a standardised score against the observed mean of our sample, we might calculate a standardised score against the test norms. In the formula for \\(z\\), we replace our sample mean \\(\\bar{x}\\) with the population mean \\(\\mu\\), and the sample standard deviation \\(s\\) with the population standard deviation \\(\\sigma\\):\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nThe resulting values tell us how low/high each participant’s IQ score is compared to the distribution of IQ scores in the population."
  },
  {
    "objectID": "rd1_05.html#glossary",
    "href": "rd1_05.html#glossary",
    "title": "Types of relationships",
    "section": "\n4 Glossary",
    "text": "4 Glossary\n\n\nFunction: A mapping between two sets of numbers, associating every element of the first set with an elemet in the second.\n\n\nTransformation: Applying a function to a variable to map each value to a transformed value.\n\nLogarithm: The power to which a number must be raised in order to get some other number.\n\nCentering: Transformation which re-expresses each value as the distance from a given number (e.g., the mean).\n\n\nStandardisation: Transformation which re-expresses each value as the distance from the mean in units of standard deviations. \n\n\nscale() To mean center or standardise a variable (depending upon whether center=TRUE/FALSE and scale=TRUE/FALSE)."
  },
  {
    "objectID": "rd1_07.html",
    "href": "rd1_07.html",
    "title": "Probability 1",
    "section": "",
    "text": "Think about flipping a coin once. Can you predict the outcome?\nThink about flipping a coin many times, one million say. Are you able to predict roughly how many heads or tails will show up?\nIt’s hard to guess the outcome of just one coin flip because the outcome could be one of two possible outcomes. Hence, we say that flipping a coin is a random experiment or random process.\nIf you flip it over and over, however, you can predict the proportion of heads you’re likely to see in the long run. In the long run simply means if you were to repeat the same experiment over and over many times under the same conditions.\nThis discussion leads us to define the specific type of randomness that we will be studying in this course.\n\n\n\n\n\n\nWhat is randomness?\nWe will say that a repeatable process is random if its outcome is\n\nunpredictable in the short run, and\npredictable in the long run.\n\n\n\n\nIt is this long-term predictability of randomness that we will use throughout the rest of the course. To do that, we will need to talk about the probabilities of different outcomes and learn some rules for dealing with them."
  },
  {
    "objectID": "rd1_07.html#video-activity",
    "href": "rd1_07.html#video-activity",
    "title": "Probability 1",
    "section": "\n2 Video activity",
    "text": "2 Video activity\nPlease watch the following video, explaining you to the concept of “randomness”."
  },
  {
    "objectID": "rd1_07.html#random-experiments-and-probability",
    "href": "rd1_07.html#random-experiments-and-probability",
    "title": "Probability 1",
    "section": "\n3 Random experiments and probability",
    "text": "3 Random experiments and probability\n\n3.1 Example 1: Flipping a coin\nThe process of flipping a coin is an example of a random experiment as its outcome is uncertain. We do not know beforehand whether the coin will land heads (H) or tails (T).\nThe collection of all possible outcomes is known as the outcome space or sample space. We typically denote the sample space by \\(S\\). In the coin example, this is: \\[\nS = \\{ H, T \\}\n\\]\nOne particular repetition (i.e. instance) of such experiment is known as a trial.\n\n3.2 Example 2: Throwing a die\nAnother example of a random experiment is throwing a six-faced die as, for each trial, we can not exactly predict which face will appear.\nThe list of possible outcomes for the die experiment is \\(1, 2, ..., 6\\). Hence, the sample space can be written: \\[\nS = \\{1, 2, 3, 4, 5, 6 \\}\n\\]\n\n3.3 Events\nConsider again the die experiment. Often, we are not interested in the probability of observing a particular outcome, such as 3, but rather in a collection of outcomes together. For example, we might be interested in the probability of observing an even number.\nSuch collections of outcomes are called events. More formally, an event is a set of outcomes.\nEach individual outcome is also considered an event. To distinguish, some people call simple events the individual outcomes, and compound events a collection of two or more outcomes.\nThe event “an even number appears” is simply the collection of even outcomes. We could call it “E” for “even” and write it as: \\[E = \\{ 2, 4, 6\\}\\]\nTwo important events are:\n\nthe empty set: the set of no outcomes, denoted \\(\\emptyset\\);\nthe sample space: the set of all outcomes, denoted \\(S\\).\n\nIn general, a finite sample space is written \\[\nS = \\{s_1, s_2, ..., s_n\\}\n\\]\nwhere each \\(s_i\\) represents an outcome or simple event.\n\n3.4 Example 3: Flipping 2 coins\nConsider flipping 2 coins simultaneously. The sample space is: \\[\nS = \\{ (H,H), (H,T), (T,H), (T,T)\\}\n\\]\nTypical events could be:\n\nObserving tails at least once, \\(A = \\{(H,T), (T,H), (T,T)\\}\\)\n\nObserving the same face twice, \\(B = \\{(H,H), (T,T)\\}\\)"
  },
  {
    "objectID": "rd1_07.html#defining-probability",
    "href": "rd1_07.html#defining-probability",
    "title": "Probability 1",
    "section": "\n4 Defining probability",
    "text": "4 Defining probability\nConsider repeating the random process of throwing a die many times, 500 say, and recording whether or not an even number appears. We will now create a table listing the result of each trial.\nFirst, we load the tidyverse package:\n\nlibrary(tidyverse)\n\nNext, we create the sample space:\n\nS <- 1:6\nS\n\n[1] 1 2 3 4 5 6\n\n\nThe following code defines the event “an even number appears”:\n\nE <- c(2, 4, 6)\nE\n\n[1] 2 4 6\n\n\nSay, now, that the outcome of a roll is 3. How can we check whether the outcome belongs to the event of interest \\(E\\)?\nWe can use the %in% function to ask R: “Is 3 in E?” The answer can be TRUE or FALSE.\n\n3 %in% E\n\n[1] FALSE\n\n\nSuppose instead, that the outcome of a roll is 2. Is 2 in E?\n\n2 %in% E\n\n[1] TRUE\n\n\nWe now specify how many trials we will be performing:\n\nnum_trials <- 500\n\nNext, we repeat the experiment num_trials times and compute the accumulated percentage of even outcomes:\n\nexperiment <- tibble(\n    trial = 1:num_trials,\n    outcome = sample(S, num_trials, replace = TRUE),\n    is_even = outcome %in% E,\n    cumul_even = cumsum(is_even),\n    cumul_perc_even = 100 * cumsum(is_even) / trial\n)\n\nThe following code displays the top 10 rows of the experiment:\n\nhead(experiment, n = 10)\n\n# A tibble: 10 × 5\n   trial outcome is_even cumul_even cumul_perc_even\n   <int>   <int> <lgl>        <int>           <dbl>\n 1     1       5 FALSE            0             0  \n 2     2       2 TRUE             1            50  \n 3     3       4 TRUE             2            66.7\n 4     4       4 TRUE             3            75  \n 5     5       2 TRUE             4            80  \n 6     6       3 FALSE            4            66.7\n 7     7       4 TRUE             5            71.4\n 8     8       2 TRUE             6            75  \n 9     9       5 FALSE            6            66.7\n10    10       2 TRUE             7            70  \n\n\n\nUnderstanding the code. Let’s inspect each column in turn:\n\n\ntrial records the number of each trial: 1, 2, …, 500;\n\noutcome lists the result of each trial: 1, or 2, …, or 6;\n\nis_even checks whether the outcome of each trial belongs to the event \\(E\\) (=TRUE) or not (=FALSE);\n\ncumul_even computes the cumulative sum of is_even.\n\nAs we saw, the is_even column contains either TRUE or FALSE. This was created with the function %in%, which is equivalent to asking a question: Is the outcome in \\(E\\)? The result will be either TRUE or FALSE. We note that, when summed, R considers a TRUE as 1, and a FALSE as 0. For example, the cumulative sum of c(TRUE, FALSE, TRUE, TRUE, FALSE) is c(1, 1, 2, 3, 3). Finally, the column cumul_perc_even computes the accumulate percentage of even outcomes.\n\nThe first trial’s outcome was 5, which is not an even number. Hence the cumulative percentage of even outcomes is 0 out of 100, or 0%. The next four trials lead to 2, 4, 4, and 2 respectively, which all are even numbers. The cumulative percentages will be 1 out of 2 (50%), 2 out of 3 (66.67%), 3 out of 4 (75%), and 4 out of 5 (80%). Next, we observe 3, which is odd, hence the cumulative percentage of even outcomes is now 4 out of 66.67%, and so on.\nFinally, we plot the accumulated percentage of even numbers against the trial number:\n\nggplot(experiment, aes(x = trial, \n                       y = cumul_perc_even)) +\n    geom_point(color = 'darkolivegreen4') +\n    geom_line(color = 'darkolivegreen4') +\n    geom_hline(aes(yintercept = 50), color = 'red', linetype = 2) +\n    labs(x = \"Trial number\", y = \"Accumulated percent even\")\n\n\n\n\nAs the number of trials increase, we see that the curve approaches 50%, which is 0.5, and that the cumulative percentage of even outcomes keeps fluctuating around 50%.\n\n\n\n\n\n\nCheckpoint\n\n\n\nWhat’s the probability of obtaining an even number when throwing a fair die?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf you answered 0.5 (or 50%), then you are on the right track!\n\n\n\nBased on the graph, it looks like the relative frequency of an even number settles down to about 50%, so saying that the probability is about 0.5 seems like a reasonable answer.\nBut do random experiments always behave well enough for this definition of probability to always apply? Perhaps the relative frequency of an event can bounce back and forth between two values forever, never settling on just one number?"
  },
  {
    "objectID": "rd1_07.html#the-law-of-large-numbers",
    "href": "rd1_07.html#the-law-of-large-numbers",
    "title": "Probability 1",
    "section": "\n5 The law of large numbers",
    "text": "5 The law of large numbers\nFortunately, Jacob Bernoulli proved the Law of large numbers (LLN) in the 18th century, giving us the peace of mind that we need.\n\n\n\n\n\n\nLaw of large numbers\nThe law of large numbers (LLN) states that as we repeat a random experiment over and over, the proportion of times that an event occurs does settle down to a single number. We call this number the probability of that event.\n\n\n\nHowever, it is not that simple. The LLN requires two key assumptions:\n\n\nIdentical distribution: The outcomes of the random experiment must have the same probabilities of occurring in each trial. That is, we can not have in trial 1 a 50% chance of observing an even number, and then a 80% chance in trial 2. The underlying chance needs to be the same. This is accomplished by not changing the random experiment we are studying, the repeated trials must happen in the same conditions.\n\n\nIndependence: The outcome of one trial must not affect the outcomes of other trials.\n\nFor the die experiment, we can now write that the probability of observing an even number is 0.5 as follows. First, we need to define the event of interest, \\(E = \\{2, 4, 6\\}\\), and then we can write: \\[\nP(E) = 0.5\n\\]\nIf you do not give a name to the event, you must specify it inside of the parentheses. Note the use of round parentheses for probability \\(P()\\) and the curly brackets to list the outcomes of interest. \\[\nP(\\{2, 4, 6\\}) = 0.5\n\\]\nWe reached this definition of the probability of the event \\(E\\). In the long run, \\[\nP(E) = \\frac{\\text{number of times outcome was in the event } E}{\\text{total number of trials}}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nWe typically use the first few capital letters of the alphabet to name events. The letter \\(P\\) will always be reserved for probability.\nWhen we write \\(P(A) = 0.5\\) we mean “the probability of the event \\(A\\) is 0.5”.\nWe use proportions (or decimal numbers) when reporting probability values in a formal situation like writing a report or a paper. However, when discussing probability informally, we often use percentages."
  },
  {
    "objectID": "rd1_07.html#law-of-averages-unicorn",
    "href": "rd1_07.html#law-of-averages-unicorn",
    "title": "Probability 1",
    "section": "\n6 Law of averages = unicorn",
    "text": "6 Law of averages = unicorn\nYou might have heard from friends or TV shows that sometimes the random experiment “owes” you a particular outcome. Let’s try to entangle this in more detail and understand where the pitfall of this reasoning is.\nThe law of large numbers tells us that the probability of an event is the proportion of times we would observe it in the long run. The long run is really long - infinitely long. We, as humans and finite entities, can not generate an infinitely long sequence of trials and memorise it.\nMany people believe that if you flip a fair coin, where fair means that the chance of getting heads is the same as the chance of getting tails (0.5) we expect the coin to “even out” the results in the coming trials if heads has not appeared in the recent ones.\nSay, for example, that in 10 trials you only observed 1 head. This is quite a low proportion, 0.1 (1 out of 10) compared to the 0.5 (5 out of 10) that the player expected. Does this mean that the coin due to show heads in the near future, as the coin “owes” us some heads to even out the proportions?\nThe answer is no.\nThe long run means that the proportions will eventually even out in the infinite sequence of trials, but you will not know when this happens and there is absolutely no requirement for the coin to show heads again in the upcoming trials in order to keep a probability of 0.5."
  },
  {
    "objectID": "rd1_07.html#modelling-probability",
    "href": "rd1_07.html#modelling-probability",
    "title": "Probability 1",
    "section": "\n7 Modelling probability",
    "text": "7 Modelling probability\nTo assign a probability value to different events, we should make sure that these coherence principles are satisfied:\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event not A is written \\(\\sim A\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(\\sim A) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]\n\n7.1 Probability in the case of equally likely outcomes\nConsider a sample space of \\(n\\) outcomes\n\\[\nS = \\{s_1, s_2, \\dots, s_n \\}\n\\]\nand suppose these are all equally likely, with \\(p\\) denoting the probability of each outcome: \\[\nP(\\{s_1\\}) = P(\\{s_2\\}) = \\cdots = P(\\{s_n\\}) = p\n\\]\nAs the outcomes in the sample space are mutually exclusive events,1 we can compute the probability of the sample space as\n\\[\n1 = P(S) = P(\\{s_1\\}) + P(\\{s_2\\}) + \\cdots + P(\\{s_n\\}) = p + p + \\cdots + p = n p\n\\]\nwhich leads to \\[\np = P(\\{s_i\\}) = \\frac{1}{n}\n\\]\nNext, consider an event \\(A\\) comprising a few of the outcomes from \\(S\\)\n\\[\nA = \\{s_2, s_5, s_9\\}\n\\] which can be also written as the union of disjoint events\n\\[\nA = \\{s_2\\} \\cup \\{s_5\\} \\cup \\{s_9\\}\n\\]\nWe can compute the probability of \\(A\\) as follows\n\\[\n\\begin{aligned}\nP(A) &= P(\\{s_2\\}) + P(\\{s_5\\}) + P(\\{s_9\\}) \\\\\n&= p + p + p \\\\\n&= 3 p \\\\\n&= 3 \\left(\\frac{1}{n}\\right) \\\\\n&= \\frac{3}{n} \\\\\n&= \\frac{n_A}{n} \\\\\n&= \\frac{\\text{number of outcomes within }A}{\\text{number of possible outcomes}}\n\\end{aligned}\n\\] where \\(n_A\\) is the number of outcomes within \\(A\\) and \\(n\\) is the total number of possible outcomes in \\(S\\)."
  },
  {
    "objectID": "rd1_07.html#glossary",
    "href": "rd1_07.html#glossary",
    "title": "Probability 1",
    "section": "\n8 Glossary",
    "text": "8 Glossary\n\n\nRandom experiment. A process or phenomenon which can have two ore more possible outcomes.\n\nTrial. A single repetition of the experiment\n\nOutcome. The value observed after running a trial\n\nSample space. The collection of all possible outcomes. The sample space is denoted \\(S\\) and has probability 1.\n\nEvent. Either a single outcome (simple event) or a collection of outcomes (compound event).\n\nProbability. A number reporting how likely is an even to occur when performing a trial (that is, obtaining an outcome within that event or satisfying the proposition of the event)\n\nDisjoint (or mutually exclusive) events. Two events \\(A\\) and \\(B\\) are disjoint if they share no outcomes in common. For disjoint events, knowing that one event occurs tells us that the other cannot occur.\n\nIndependent events. Two events are independent if learning that one event occurs does not change the probability that the other event occurs.\n\nProbability assignment rule. Says that the probability of any event must be between 0 (the probability of an impossible event) and 1 (the probability of a certain event).\n\nTotal probability rule. The probability that the experiment’s outcome is in the sample space must be 1.\n\nComplement rule. If \\(P(A)\\) denotes the probability of the event \\(A\\) occurring, the probability of “not A” is \\(P(\\sim A) = 1 - P(A)\\).\n\nAddition rule for disjoint events. If \\(A\\) and \\(B\\) are disjoint events, then \\(P(A \\cup B) = P(A) + P(B)\\).\n\nMultiplication rule for independent events. If A and B are independent events, then \\(P(A \\cap B) = P(A) \\times P(B)\\)."
  },
  {
    "objectID": "rd1_08.html",
    "href": "rd1_08.html",
    "title": "Probability 2",
    "section": "",
    "text": "Let’s recap the probability rules discussed last week.\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event not A is written \\(\\sim A\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(\\sim A) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#general-addition-rule",
    "href": "rd1_08.html#general-addition-rule",
    "title": "Probability 2",
    "section": "\n2 General addition rule",
    "text": "2 General addition rule\nConsider throwing a six-sided die, for which we already know that the sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). What’s the probability of observing an odd number or a number which is less than 4?\nThe relevant events are:\n\n\\(A = \\text{result is odd} = \\{1, 3, 5\\}\\)\n\\(B = \\text{result is less than four} = \\{1, 2, 3\\}\\)\n\nand we know that:\n\n\\(P(A) = 3/6 = 0.5\\)\n\\(P(B) = 3/6 = 0.5\\)\n\nHowever, the probability of \\(A\\) or \\(B\\), written \\(P(A \\cup B)\\), is not simply \\(P(A) + P(B)\\), because the events \\(A\\) and \\(B\\) are not disjoint; they overlap. The outcomes 1 and 3 play a crucial role here as they are both odd and are less than four, i.e. they are in both sets. They are both in \\(A\\) and \\(B\\), which places them in the intersection of the two circles.\nThe diagram below represents the sample space as \\(S\\). Notice that the outcomes 4 and 6 are neither odd nor less than four, so they sit outside both circles.\n\n\n\n\n\n\n\n\nThe reason we can’t simply add the probabilities of \\(A\\) and \\(B\\) is that we would count the outcomes 1 and 3 twice. If we did add the two probabilities, we could compensate by subtracting the probability of the outcomes 1 and 3.\n\\[\n\\begin{aligned}\nP(\\text{odd or less than 4}) &= P(\\text{odd}) + P(\\text{less than 4}) - P(\\text{odd and less than 4}) \\\\\n&= P(\\{1, 3, 5\\}) + P(\\{1, 2, 3\\}) - P(\\{1, 3\\}) \\\\\n&= 3/6 + 3/6 - 2/6 \\\\\n&= 0.667\n\\end{aligned}\n\\]\nThis is true in general and is called the general addition rule, which does not require disjoint events. Consider two generic events \\(A\\) and \\(B\\) such that\n\n\n\n\n\n\n\n\nThen, the probability of observing \\(A\\) or \\(B\\) is: \\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nIntuitively, \\(P(A) + P(B)\\) counts \\(A \\cap B\\) twice, so we have to subtract \\(P(A \\cap B)\\) to make the net number of times \\(A \\cap B\\) is counted equal to 1.\nWhen the two events have no outcomes in common, i.e. \\(P(A \\cap B) = 0\\) as the two events can’t happen together, the general addition rule reduces to the addition rule for disjoint events.\n\n\n\n\n\n\nTERMINOLOGY\n\n\n\n\nWould you like fruit or dessert?\n\nEveryday language can be ambiguous. It’s not clear if by answering “yes” to the question above our choice is only one of the two options, or we can actually have both.\nTypically, everyday language uses “or” meaning the exclusive version. That is, you can have one or the other, but not both. In other words, you can have fruit or dessert, but not both.\nWhen talking about probability, being imprecise and ambiguous can get us into trouble. For this reason, we will here define once for all what we mean by “or” in a mathematical sense.\nIn statistics, when we say “or” we always mean the inclusive version. In other words, the probability of \\(A\\) or \\(B\\) means the probability of either \\(A\\) or \\(B\\) or both.\n\n\nLet’s consider another example. Suppose that 25% of people have an electric scooter, 29% of people have a bike, and 12% of people own both. What is the probability that someone owns an electric scooter or a bike?\nThis question concerns the following two events\n\n\\(A = \\text{owning an electric scooter}\\)\n\\(B = \\text{owning a bike}\\)\n\nand we are told that\n\n\\(P(A) = 0.25\\)\n\\(P(B) = 0.29\\)\n\\(P(A \\cap B) = 0.12\\)\n\nClearly, \\(A\\) and \\(B\\) are not mutually exclusive (or disjoint) events. Having a scooter doesn’t exclude owning a bike, and vice versa. In fact, the problem statements tells us exactly the percentage of people having both an electric scooter and a bike, 12%.\nSo… how do we calculate the probability of owning an electric scooter or a bike?\nIf we simply did \\(P(A) + P(B)\\), we would count twice those having both a scooter and a bike. Hence, we need to subtract \\(P(A \\cap B)\\): \\[\n\\begin{aligned}\nP(A \\cup B) &= P(A) + P(B) - P(A \\cap B) \\\\\n&= 0.25 + 0.29 - 0.12 \\\\\n&= 0.42\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rd1_08.html#conditional-probability",
    "href": "rd1_08.html#conditional-probability",
    "title": "Probability 2",
    "section": "\n3 Conditional probability",
    "text": "3 Conditional probability\nThe following contingency table displays the counts of passengers who did or did not survive the Titanic sinking by travelling class.\n\n\n\n\n\n\n\nSurvived\n\n\n\n Class \n    No \n    Yes \n    Total \n  \n\n\n\n 1st \n    122 \n    203 \n    325 \n  \n\n 2nd \n    167 \n    118 \n    285 \n  \n\n 3rd \n    528 \n    178 \n    706 \n  \n\n Total \n    817 \n    499 \n    1316 \n  \n\n\n\n\nYou already encountered contingency tables when studying the relationship between two categorical variables in week 4.\nIf you divide the frequency table by the total number of passengers, 1316, you obtain a table of relative frequencies. It is just a small step from these relative frequencies to probabilities.\nLet;s focus on the Titanic survival study and make the sample space just the set of these 1316 passengers. If we select a passenger at random from this study, the probability we select a first class passenger is just the corresponding relative frequency (since we are equally likely to select any of the 1316 passengers). There are 325 first class passengers in the data out of a total of 1316, giving a probability of \\[\nP( \\text{1st} ) = \\frac{325}{1316} = 0.247\n\\] The same method works for more complicated events like intersections. For example, what’s the probability of selecting a 1st class passenger who survived? Well, 203 first class passengers survived, so the probability is \\[\nP( \\text{1st} \\cap \\text{survived} ) = \\frac{203}{1316} = 0.154\n\\]\nThe probability of selecting a passenger who survived is \\[\nP( \\text{survived} ) = \\frac{499}{1316} = 0.379\n\\] What if we are given the information that the selected passenger was in first class? Would that change the probability that the selected passenger survived? You bet it would!\nWhen we restrict our focus to first class passengers, we look only at the row of the table where “Class” is “1st”. Of the 325 first class passengers, only 203 of them said survived. We write the probability that a selected passenger survived given that we have selected a first class passenger as \\[\nP(\\text{survived} \\mid \\text{1st}) = \\frac{203}{325} = 0.625\n\\]\nNow, imagine dividing numerator and denominator by the total, 1316: \\[\n\\begin{aligned}\nP(\\text{survived} \\mid \\text{1st})\n= \\frac{203 / 1316}{325 / 1316}\n= \\frac{P(\\text{1st} \\cap \\text{survived})}{P(\\text{1st})}\n\\end{aligned}\n\\]\nA probability that takes into account a given condition, such as being a first class passenger, is called a conditional probability. For generic events \\(A\\) and \\(B\\), we write the conditional probability as \\(P(B | A)\\) and pronounce it “the probability of \\(B\\) given \\(A\\)”.\nSuppose we are told that the event \\(A\\) with \\(P(A) > 0\\) occurs. The conditional probability of \\(B\\) given \\(A\\) is given by: \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)}\n\\] Intuitively, we need to divide the formula by \\(P(A)\\) to make sure that \\(P(A|A) = 1\\).\nThe formula wouldn’t work if \\(P(A) = 0\\), but this makes sense as we couldn’t be told that \\(A\\) occurred if it was an impossible event!\n\n\n\n\n\n\nIMPORTANT NOTE\n\n\n\nThe conditional probability \\(P(\\text{some event} | A)\\) satisfies the standard rules of probability. You can think of it as the same as \\(P(\\text{some event})\\) but after having replaced the sample space from \\(S\\) to \\(A\\), which is the event that we have been told happened. As the event happened, we know that the new sample space is now \\(A\\), so that \\(P(A) = 1\\).\nTo see this, remember that \\(P(S) = 1\\) and we can then rewrite\n\\[P(A) = \\frac{P(A)}{1} = \\frac{P(A)}{P(S)} = \\frac{P(A \\cap S)}{P(S)} = P(A | S)\\]\nThe rule \\(P(A) + P({} \\sim A) = 1\\) can be written as \\(P(A | S) + P({} \\sim A | S) = 1\\).\nComplement rule for conditional probabilities\nIn general, according to the complement rule, \\(P({}\\sim B|A) = 1 - P(B|A)\\).\nProduct rule for conditionally independent events\nWe also have a similar rule for independence. Two events \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if:\n\\[\nP(A \\cap B | C) = P(A | C) \\times P(B | C)\n\\]\n\n\nConsider the following plot:\n\n\n\n\n\n\n\n\nOn the y-axis we see the conditional probabilities of survival given the passenger class. That is \\(P(\\text{survived} | \\text{1st}) = 0.62\\), \\(P(\\text{survived} | \\text{2nd}) = 0.41\\), and \\(P(\\text{survived} | \\text{3rd}) = 0.25\\).\nThe x-axis displays the probability of a randomly selected passenger to be in each travelling class. For example, \\(P(\\text{1st}) = 0.25\\), \\(P(\\text{2nd}) = (0.46 - 0.25) = 0.21\\), and \\(P(\\text{3rd}) = 1 - (P(\\text{1st}) + P(\\text{2nd})) = 1 - 0.25 - 0.21 = 1 - 0.46 = 0.54\\)"
  },
  {
    "objectID": "rd1_08.html#general-multiplication-rule",
    "href": "rd1_08.html#general-multiplication-rule",
    "title": "Probability 2",
    "section": "\n4 General multiplication rule",
    "text": "4 General multiplication rule\nConsider two generic events \\(A\\) and \\(B\\). Before, we saw that \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nRearranging the conditional probability formula we obtain the general multiplication rule: \\[\nP(A \\cap B) = P(A) P(B|A)\n\\]\nIf you were to start from \\(P(A | B)\\) you would reach to the equivalent version: \\[\nP(A \\cap B) = P(B) P(A|B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#independence",
    "href": "rd1_08.html#independence",
    "title": "Probability 2",
    "section": "\n5 Independence",
    "text": "5 Independence\nWe say that two events \\(A\\) and \\(B\\) are independent, if knowing that one occurred doesn’t change the probability of the other occurring:\n\\[\nP(B|A) = P(B)\n\\]\nFrom this we have that\n\\[\nP(B) = P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nLeading to\n\\[\nP(A \\cap B) = P(A) P(B)\n\\] At the same time,\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) P(B)}{P(B)} = P(A)\n\\]\nSo, we have three equivalent definitions of independent events! If one holds, the remaining ones hold as well. Two events \\(A\\) and \\(B\\) are independent if and only if \\[\n\\begin{aligned}\nP(A | B) &= P(A) \\\\\nP(B | A) &= P(B) \\\\\nP(A \\cap B) &= P(A) P(B)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nIn pictures: Independence vs Dependence\n\n\n\n\n\nIndependence happens when the probability of “Survived = Yes” is the same regardless of whether the passenger was in 1st, 2nd, or 3rd class.\n\n\n\n\n\n\n\n\nPerfect dependence happens when knowing the passenger class leads to a perfect prediction of whether a passenger survived or not.\n\n\n\n\n\n\n\n\n\n\n\nNote that, for independent events \\(A\\) and \\(B\\) such that \\(P(B | A) = P(B)\\), the general multiplication rule reduces to the multiplication rule for independent events: \\[\nP(A \\cap B) = P(B|A)P(A) = P(B) P(A)\n\\]"
  },
  {
    "objectID": "rd1_08.html#bayes-rule-reversing-the-conditioning",
    "href": "rd1_08.html#bayes-rule-reversing-the-conditioning",
    "title": "Probability 2",
    "section": "\n6 Bayes’ rule: reversing the conditioning",
    "text": "6 Bayes’ rule: reversing the conditioning\nSuppose we have \\(P(A | B)\\) but we are interested in \\(P(B | A)\\). That is, we want to reverse the conditioning.\nBy applying the rule for conditional probability and the general multiplication rule, we have that: \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{P(A|B)P(B)}{P(A)}\n\\]"
  },
  {
    "objectID": "rd1_08.html#glossary",
    "href": "rd1_08.html#glossary",
    "title": "Probability 2",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nGeneral addition rule. For any events \\(A\\) and \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nGeneral multiplication rule. For any events \\(A\\) and \\(B\\), \\(P(A \\cap B) = P(B|A)P(A) = P(A | B) P(B)\\).\n\nIndependent events. Two events \\(A\\) and \\(B\\) are independent if and only if \\(P(A \\cap B) = P(A) P(B)\\)\n\n\nBayes’ rule. \\(P(B | A) = \\frac{P(A | B)P(B)}{P(A)}\\)"
  },
  {
    "objectID": "rd1_09.html",
    "href": "rd1_09.html",
    "title": "Discrete random variables",
    "section": "",
    "text": "Consider throwing three fair coins. The sample space of this random experiment is\n\\[\nS = \\{\n    TTT, \\\n    TTH, \\\n    THT, \\\n    HTT, \\\n    THH, \\\n    HTH, \\\n    HHT, \\\n    HHH\n\\}\n\\]\nEach outcome has an equal chance of occurring of \\(1 / 8 = 0.125\\), computed as one outcome divided by the total number of possible outcomes.\nOften, we are only interested in a numerical summary of the random experiment. One such summary could be the total number of heads.\n\nRandom variable\nWe call a numerical summary of a random process a random variable.\nRandom variables are typically denoted using the last uppercase letters of the alphabet (\\(X, Y, Z\\)). Sometimes we might also use an uppercase letter with a subscript to distinguish them, e.g. \\(X_1, X_2, X_3\\).\n\nA random variable, like a random experiment, also has a sample space and this is called the support or range of \\(X\\), written \\(R_X\\). This represents the set of possible values that the random variable can take."
  },
  {
    "objectID": "rd1_09.html#discrete-vs-continuous-random-variables",
    "href": "rd1_09.html#discrete-vs-continuous-random-variables",
    "title": "Discrete random variables",
    "section": "\n2 Discrete vs continuous random variables",
    "text": "2 Discrete vs continuous random variables\nThere are two different types of random variables, and the type is defined by their range.\nWe call a variable discrete or continuous depending on the “gappiness” of its range, i.e. depending on whether or not there are gaps between successive possible values of a random variable.\n\n\n\n\n\n\n\n\n\nA discrete random variable has gaps in between its possible values. An example is the number of children in a randomly chosen family (0, 1, 2, 3, …). Clearly, you can’t have 2.3 children…\nA continuous random variable has no gaps in between the its possible values. An example is the height in cm of a randomly chosen individual.\n\nIn this week’s exercises we will study discrete random variables."
  },
  {
    "objectID": "rd1_09.html#three-coins-example-continued",
    "href": "rd1_09.html#three-coins-example-continued",
    "title": "Discrete random variables",
    "section": "\n3 Three coins example (continued)",
    "text": "3 Three coins example (continued)\nIn the 3 coins example, the possible values of the random variable \\(X\\) = “number of heads in 3 tosses” are \\[\nR_X = \\{0, 1, 2, 3\\}\n\\]\nmeaning that \\(X\\) is a discrete random variable.\nWe denote a potential value of the random variable using a lowercase \\(x\\) and a subscript to number the possible values.\nThis is obtained as follows:\n\n\n\n\n\n\n\n\nAs we can see, each value of the random variable is computed from the underlying random experiment. There is one outcome only (TTT) leading to zero heads, i.e. \\(X = 0\\). There are three outcomes (TTH, THT, HTT) leading to one head, i.e. \\(X = 1\\). And so on…\nThe experiment’s outcomes are all equally likely, each having a \\(1/8\\) chance of occurring. However, since the random variable aggregates the experiment’s outcomes, the probability of the random variable taking a particular value is computed by summing the probabilities of the outcomes leading to that value.\nLet’s try and obtain the same diagram as that shown above using R. We will be using the function expand_grid, which creates the sample space by listing all possible combinations.\n\nlibrary(tidyverse)\n\nexperiment <- expand_grid(coin1 = c('T', 'H'),\n                          coin2 = c('T', 'H'),\n                          coin3 = c('T', 'H'))\nexperiment\n\n# A tibble: 8 × 3\n  coin1 coin2 coin3\n  <chr> <chr> <chr>\n1 T     T     T    \n2 T     T     H    \n3 T     H     T    \n4 T     H     H    \n5 H     T     T    \n6 H     T     H    \n7 H     H     T    \n8 H     H     H    \n\nexperiment <- experiment %>% \n    mutate(\n        prob = rep( 1/n(), n() )\n    )\nexperiment\n\n# A tibble: 8 × 4\n  coin1 coin2 coin3  prob\n  <chr> <chr> <chr> <dbl>\n1 T     T     T     0.125\n2 T     T     H     0.125\n3 T     H     T     0.125\n4 T     H     H     0.125\n5 H     T     T     0.125\n6 H     T     H     0.125\n7 H     H     T     0.125\n8 H     H     H     0.125\n\nrv <- experiment %>%\n    mutate(\n        value = (coin1 == 'H') + (coin2 == 'H') + (coin3 == 'H')\n    ) %>%\n    group_by(value) %>%\n    summarise(prob = sum(prob))\nrv\n\n# A tibble: 4 × 2\n  value  prob\n  <int> <dbl>\n1     0 0.125\n2     1 0.375\n3     2 0.375\n4     3 0.125\n\n\nwhere \\(1/8 = 0.125\\) and \\(3/8 = 0.375\\).\n\nWe can provide a concise representation of a random variable \\(X\\), the set of all its possible values, and the probabilities of those values by providing the probability distribution of \\(X\\). You can think of the probability distribution of a random variable as a succinct way to provide a global picture of the random variable.\n\n\n\n\n\n\nProbability distribution\n\n\n\nThe probability distribution of a discrete random variable \\(X\\) provides the possible values of the random variable and their corresponding probabilities.\nA probability distribution can be in the form of a table, graph, or mathematical formula.\n\n\nWe visualise the distribution of a discrete random variable via a line graph. This graph gives us, with just a glance, an immediate representation of the distribution of that random variable.\n\nggplot(data = rv) +\n    geom_segment(aes(x = value, xend = value, y = 0, yend = prob)) +\n    geom_point(aes(x = value, y = prob)) +\n    labs(x = \"Possible values, x\", y = \"Probabilities, P(X = x)\")\n\n\n\n\n\n\n\nAs you can see, a line graph has gaps in between the possible values the random variable can take, exactly to remind us that the random variable can’t take values that are different from 0, 1, 2, and 3.\nAlternatively, you could provide the probability distribution of the random variable in tabular form:\n\n\n\n\n\n x \n    P(X = x) \n  \n\n\n 0 \n    1/8 \n  \n\n 1 \n    3/8 \n  \n\n 2 \n    3/8 \n  \n\n 3 \n    1/8 \n  \n\n\n\n\nStatisticians have also spent lots of time trying to find a mathematical formula for that probability distribution. The formula is the most concise way to obtain the probabilities as it gives you a generic rule which you can use to compute the probability of any possible value of that random variable. All you have to do is substitute to \\(x\\) the value you are interested in, e.g. 0, 1, 2, or 3.\n\n\n\n\n\n\nProbability mass function\n\n\n\nThe probability mass function (pmf) of \\(X\\) assigns a probability, between 0 and 1, to every value of the discrete random variable \\(X\\).\nEither of the following symbols are often used: \\[\nf(x) = P(x) = P(X = x) \\qquad \\text{for all possible }x\n\\] where \\(P(X = x)\\) reads as “the probability that the random variable \\(X\\) equals \\(x\\)”.\nThe sum of all of these probabilities must be one, i.e. \\[\n\\sum_{i} f(x_i) = \\sum_{i} P(X = x_i) = 1\n\\]\n\n\nBefore we define the mathematical function, I need to tell you what a number followed by an exclamation mark means.\nIn mathematics \\(n!\\), pronounced “\\(n\\) factorial”, is the product of all the integers from 1 to \\(n\\). For example, \\(4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\), and \\(3! = 3 \\cdot 2 \\cdot 1 = 6\\). By convention, mathematician have decided that \\(0! = 1\\).\nThe probability function of \\(X\\) = “number of heads in 3 tosses” makes use of the following numbers:\n\n\n\\(3\\), representing the number of coin flips\n\n\\(\\frac{1}{2}\\), the probability of observing heads in a single flip of a fair coin\n\nFor the three coins example, the probability function of \\(X\\) is \\[\nP(X = x) = \\frac{3!}{x!\\ (3-x)!} \\cdot (1/2)^x \\cdot (1/2)^{3-x}\n\\]\nLet’s see if the formula gives back the table we created above.\n\n\nFor \\(x=0\\) we have:\n\\[\nP(X = 0) = \\frac{3!}{0!\\ 3!} \\cdot (1/2)^0 \\cdot (1/2)^3 = \\frac{6}{6} \\cdot 1 \\cdot (1/8) = 1/8\n\\]\n\n\nAnd so on… If you want to see the rest, check the optional box below.\n\n\n\n\n\n\nOptional: I want to see the other probabilities\n\n\n\n\n\n\n\nFor \\(x = 1\\) we have\n\\[\nP(X = 1) = \\frac{3!}{1!\\ 2!} \\cdot (1/2)^1 \\cdot (1/2)^2 = \\frac{6}{2} \\cdot (1/2) \\cdot (1/4) = 3 \\cdot (1/8) = 3/8\n\\]\n\n\nFor \\(x = 2\\) we have\n\\[\nP(X = 2) = \\frac{3!}{2!\\ 1!} \\cdot (1/2)^2 \\cdot (1/2)^1= \\frac{6}{2} \\cdot (1/4) \\cdot (1/2) = 3 \\cdot (1/8) = 3/8\n\\]\n\n\nFor \\(x=3\\) we have\n\\[\nP(X = 3) = \\frac{3!}{3!\\ 0!} \\cdot (1/2)^3 \\cdot (1/2)^0 = \\frac{6}{6} \\cdot (1/8) \\cdot 1 = 1/8\n\\]\n\n\n\n\n\nAs you can see, this formula will provide you the same values that are listed in the tabular representation of the probability distribution."
  },
  {
    "objectID": "rd1_09.html#centre-the-expected-value",
    "href": "rd1_09.html#centre-the-expected-value",
    "title": "Discrete random variables",
    "section": "\n4 Centre: the expected value",
    "text": "4 Centre: the expected value\nConsider a discrete random variable with range \\(R_X = \\{x_1, x_2, \\dots, x_n\\}\\)\nThe expected value (or mean) of a random variable \\(X\\), denoted by \\(E(X)\\), \\(\\mu\\), or \\(\\mu_X\\), describes where the probability distribution of \\(X\\) is centred.\nWe tend to prefer the name “expected value” to “mean” as the random variable is not something the has happened yet, it’s a potentially observable value. So, the expected value is the typical value we expect to observe.\nThe expected value of \\(X\\) is computed by multiplying each value by its probability and then summing everything:\n\\[\n\\begin{aligned}\n\\mu = E(X) &= x_1 \\cdot P(x_1) + x_2 \\cdot P(x_2) + \\cdots + x_n \\cdot P(x_n) \\\\\n&= \\sum_{i} x_i \\cdot P(x_i)\n\\end{aligned}\n\\]\nFor the three coins, the expected value is: \\[\n\\mu = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{3}{2} = 1.5\n\\]\nAs you can see, 1.5 is not one of the possible values that \\(X\\) can take in that case, as it lies in the gap between the values 1 and 2. However, it is a fictitious number which seems to well represent the centre of that distribution and hence a typical value from that distribution."
  },
  {
    "objectID": "rd1_09.html#spread-the-standard-deviation",
    "href": "rd1_09.html#spread-the-standard-deviation",
    "title": "Discrete random variables",
    "section": "\n5 Spread: the standard deviation",
    "text": "5 Spread: the standard deviation\nThe variability of a random variable \\(X\\) is measured by its standard deviation.\n\n\n\n\n\n\nVariance and standard deviation\n\n\n\nIf \\(X\\) has expected value \\(\\mu\\), the variance of \\(X\\) is \\[\n\\sigma^2 = \\sum_i (x_i - \\mu)^2 \\cdot P(x_i)\n\\] and the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]"
  },
  {
    "objectID": "rd1_09.html#underlying-random-experiments",
    "href": "rd1_09.html#underlying-random-experiments",
    "title": "Discrete random variables",
    "section": "\n6 Underlying random experiments",
    "text": "6 Underlying random experiments\nAs we saw, each random variable is a numerical summary of a random experiment and, as such, it arises from an underlying random experiment.\nIn this section we will analyse different random experiments, also called models, commonly arising in every day situations.\n\n6.1 Binomial model\n\n\n\n\n\n\nNotation\n\n\n\n\\(p\\) is the probability of a success on any one trial, and \\(n\\) is the number of trials.\n\n\nSuppose you have a series of trials that satisfy these conditions:\n\nB: They are Bernoulli — that is, each trial must have one of two different outcomes, one called a “success” and the other a “failure.”\nI: Each trial is independent of the others — that is, the probability of a success doesn’t change depending on what has happened before.\nN: There is a fixed number, \\(n\\), of trials.\nS: The probability, \\(p\\), of a success is the same on each trial, with \\(0 \\leq p \\leq 1\\).\n\nThen the distribution of the random variable \\(X\\) that counts the number of successes in \\(n\\) trials (each with a probability of success = \\(p\\)) is called a binomial distribution.\nThe numbers \\(n\\) and \\(p\\) are called the parameters of the binomial distribution. We write that \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\) as follows: \\[\nX \\sim \\text{Binomial}(n,p)\n\\]\nFurther, the probability that you get exactly \\(X = x\\) successes is \\[\nP(X = x) = \\frac{n!}{x!\\ (n-x)!} \\cdot p^x \\cdot (1-p)^{n-x}, \\qquad R_X = \\{0, 1, 2, ..., n\\}\n\\] where \\(n! = n (n-1) (n-2) \\cdots 3 \\cdot 2 \\cdot 1\\).\nDo you recognise it from the coins example?\nVisual exploration\nThe figure below displays different binomial distributions as \\(n\\) and \\(p\\) vary:\n\n\n\n\nThe binomial probability distribution as n and p vary.\n\n\n\n\nCentre and spread\nFor a random variable \\(X\\) having a binomial distribution with \\(n\\) trials and probability of success \\(p\\), the mean (expected value) and standard deviation for the distribution are given by \\[\n\\mu_X = n p \\qquad \\text{and} \\qquad \\sigma_X = \\sqrt{n p (1 - p)}\n\\]\nBinomial distribution in R\nThe function to compute the binomial probability distribution is\ndbinom(x, size, prob)\nwhere:\n\n\nx is the values for which we want to compute the probabilities\n\nsize is \\(n\\) in our notation, the number of trials\n\nprob is \\(p\\) in our notation, the probability of success in each trial.\nExample\nA student is attempting a 10-questions multiple choice test. Each question has four different options. If the student answers at random, what is the chance that they correctly answers 2 out of the 10 questions?\nAs we know that the student is randomly guessing the answers, the probability of a correct answer is \\(p = 1/4\\). The probability of answering 2 questions correctly out of the 10 in the test is \\(P(X = 2)\\):\n\ndbinom(x = 2, size = 10, prob = 1/4)\n\n[1] 0.2815676\n\n\nIn a multiple choice test comprising 10 questions having each 4 possible answers, there is a 28% chance of answering exactly 2 questions out of 10 correctly just by random guessing.\n\nNote that you can also compute the probabilities for all possible values of \\(X\\) at once:\n\ntibble(\n    values = 0:10,\n    prob = dbinom(x = 0:10, size = 10, prob = 1/4)\n)\n\n# A tibble: 11 × 2\n   values        prob\n    <int>       <dbl>\n 1      0 0.0563     \n 2      1 0.188      \n 3      2 0.282      \n 4      3 0.250      \n 5      4 0.146      \n 6      5 0.0584     \n 7      6 0.0162     \n 8      7 0.00309    \n 9      8 0.000386   \n10      9 0.0000286  \n11     10 0.000000954\n\n\n\n6.2 Geometric model\nSuppose you have a series of trials that satisfy these conditions:\n\nThey are Bernoulli — that is, each trial must have one of two different outcomes, one called a “success” and the other a “failure”.\nEach trial is independent of the others; that is, the probability of a success doesn’t change depending on what has happened before.\nThe trials continue until the first success.\nThe probability, \\(p\\), of a success is the same on each trial, \\(0 \\leq p \\leq 1\\).\n\nThen the distribution of the random variable \\(X\\) that counts the number of failures before the first “success” is called a geometric distribution.\nThe probability that the first success occurs after \\(X = x\\) failures is \\[\nP(X = x) = (1 - p)^{x} p, \\qquad R_X = \\{0, 1, 2, ...\\}\n\\]\nWe write that \\(X\\) follow a geometric distribution with parameter \\(p\\) as follows: \\[\nX \\sim \\text{Geometric}(p)\n\\]\nVisual exploration\nThe figure below displays different geometric distributions as \\(p\\) varies:\n\n\n\n\n\n\n\n\nCentre and spread\nA random variable \\(X\\) that has a geometric distribution with probability of success \\(p\\) has an expected value (mean) and standard deviation of \\[\n\\mu_X = \\frac{1 - p}{p} \\qquad \\text{and} \\qquad \\sigma_X = \\sqrt{\\frac{1-p}{p^2}}\n\\]\nGeometric distribution in R\nThe function to compute the geometric probability distribution is\ndgeom(x, prob)\nwhere:\n\n\nx is the number of failures before the first success\n\nprob is \\(p\\) in our notation, the probability of success in each trial.\nExample\nConsider rolling a fair six-sided die until a five appears. What is the probability of rolling the first five on the third roll?\nFirst, note that the probability of “success” (observing a five) is \\(p = 1/6\\). We are asked to compute the probability of having the first “success” on the 3rd trial. We want to compute \\(P(X = 2)\\) because we need to have 2 failures followed by a success:\n\ndgeom(x = 2, prob = 1/6)\n\n[1] 0.1157407\n\n\nThus, there is a 12% chance of obtaining the first five on the 3rd roll of a die."
  },
  {
    "objectID": "rd1_09.html#glossary",
    "href": "rd1_09.html#glossary",
    "title": "Discrete random variables",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nRandom variable. A numerical summary of a random experiment.\n\nRange of a random variable. The set of possible values the random variable can take.\n\nProbability distribution. A table, graph, or formula showing how likely each possible value of a random variable is to occur.\n\nProbability (mass) function. A function providing the probabilities, between 0 and 1, for each value that the random variable can take. These probabilities must sum to 1.\n\nBinomial random variable. \\(X\\) represents the number of successes in \\(n\\) trials where the probability of success, \\(p\\), is constant from trial to trial. It has range \\(R_X = \\{0, 1, 2, ..., n\\}\\).\n\nGeometric random variable. \\(X\\) represents the number of failures until the first success, where the probability of success, \\(p\\), is constant from trial to trial. It has range \\(R_X = \\{0, 1, 2, ...\\}\\)."
  }
]