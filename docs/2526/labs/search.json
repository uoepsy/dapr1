[
  {
    "objectID": "zf_formatting_resources.html",
    "href": "zf_formatting_resources.html",
    "title": "Formatting resources",
    "section": "",
    "text": "This page contains formatting resources that will help you finalise your report formatting prior to submission."
  },
  {
    "objectID": "zf_formatting_resources.html#successful-knitting-checklist",
    "href": "zf_formatting_resources.html#successful-knitting-checklist",
    "title": "Formatting resources",
    "section": "\n1 Successful knitting checklist",
    "text": "1 Successful knitting checklist\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist"
  },
  {
    "objectID": "zf_formatting_resources.html#apa-style-reporting",
    "href": "zf_formatting_resources.html#apa-style-reporting",
    "title": "Formatting resources",
    "section": "\n2 APA style reporting",
    "text": "2 APA style reporting\nCheck the following guide for reporting numbers and statistics in APA style (7th edition).\n\nNumbers and statistics guide, APA style 7th edition"
  },
  {
    "objectID": "zf_formatting_resources.html#hiding-code-andor-output",
    "href": "zf_formatting_resources.html#hiding-code-andor-output",
    "title": "Formatting resources",
    "section": "\n3 Hiding code and/or output",
    "text": "3 Hiding code and/or output\n\n\nHide R code\nHide R output\nHide both R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "zf_formatting_resources.html#figures",
    "href": "zf_formatting_resources.html#figures",
    "title": "Formatting resources",
    "section": "\n4 Figures",
    "text": "4 Figures\n\n4.1 Combining plots\nYou can combine multiple plots into a single figure using the functions | and / from library(patchwork).\nSuppose you stored four plots into objects named plt1, plt2, plt3, plt4. To combine the four plots into a single figure with 2 rows and 2 columns, you can use:\n(plt1 | plt2) / (plt3 | pl4)\nThe vertical bar | places figures side by side. The forward slash / starts a new row.\n\n4.2 Reducing figure size\nYou can adjust the figure height and width with the following code chunk options: fig.height = ?, fig.width = ?.\nTrying different numbers by trial-and-error, substitute numbers where the ? are, and adjust as needed: for example, 5 and 4, 12 and 8, etc. Always keep in mind, that the figure labels should still be legible in your resulting plots.\n```{r, fig.height = 5, fig.width = 4}\n# your code to display the figure here\n```\n\n4.3 Referencing figures\nStep 1. Create a unique label for the code chunk that displays the figure, in this case UniqueFigureLabel but you should use a more descriptive name.\n```{r UniqueFigureLabel, fig.cap = \"Write a figure caption here\"}\nlibrary(tidyverse)\nlibrary(palmerpenguins)   # for the penguins example data we are using here\npltSpecies &lt;- ggplot(penguins, aes(x = species)) + \n    geom_bar()\npltSpecies\n```\n\n\n\n\nWrite a figure caption here\n\n\n\nTo reference a figure in the Rmd file, for example the one above, you would write:\n\nFigure \\@ref(fig:UniqueFigureLabel) displays...\n\nwhich, when you Knit to PDF, becomes:\n\nFigure 1 displays…"
  },
  {
    "objectID": "zf_formatting_resources.html#tables",
    "href": "zf_formatting_resources.html#tables",
    "title": "Formatting resources",
    "section": "\n5 Tables",
    "text": "5 Tables\nWe will use example data from library(palmerpenguins) about the body_mass_g of different species of penguins.\n\n5.1 Pretty PDF tables\nSuppose you created a descriptives table and stored it into tbl:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)   # for the penguins example data we are using here\n\ntbl &lt;- penguins |&gt;\n    group_by(species) |&gt;\n    summarise(M = mean(body_mass_g, na.rm = TRUE),\n              SD = sd(body_mass_g, na.rm = TRUE))\ntbl\n\n# A tibble: 3 × 3\n  species       M    SD\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    3701.  459.\n2 Chinstrap 3733.  384.\n3 Gentoo    5076.  504.\n\n\nIt wouldn’t be appropriate to show this R printout in a paper/report/dissertation. Instead, you can create a properly formatted table for a PDF report using the kbl() or kable() functions from library(kableExtra).\n```{r, echo=FALSE}\nlibrary(kableExtra)\ntbl |&gt;\n    kbl(digits = 2, booktabs = TRUE, \n        caption = \"Write a table caption here\")\n```\n\n\n\nWrite a table caption here\n\nspecies\nM\nSD\n\n\n\nAdelie\n3700.66\n458.57\n\n\nChinstrap\n3733.09\n384.34\n\n\nGentoo\n5076.02\n504.12\n\n\n\n\n\nThe provided options are:\n\n\ndigits = 2, to format numbers to only have 2 decimal places\n\nbooktabs = TRUE, to create “book-style” tables, i.e. tables with horizontal rows only\n\ncaption = \"Write a table caption here\", to provide a table caption\n\n5.2 Referencing tables\nThis is a continuation of the previous example, where we created a descriptives table and stored it into tbl. Please note that tbl is not a kbl()/kable() yet.\nTo reference a table you need to pick a unique label for the code chunk that displays the table, in this case UniqueTableLabel but you should use a more descriptive name.\nYou must also ensure that the table has a caption for the referencing to work.\n```{r UniqueTableLabel, echo=FALSE}\nlibrary(kableExtra)\ntbl |&gt;\n    kbl(digits = 2, booktabs = TRUE, \n        caption = \"Write here a table caption\")\n```\n\n\n\nWrite here a table caption\n\nspecies\nM\nSD\n\n\n\nAdelie\n3700.66\n458.57\n\n\nChinstrap\n3733.09\n384.34\n\n\nGentoo\n5076.02\n504.12\n\n\n\n\n\nIn the Rmd file, the table is referenced as:\n\nTable \\@ref(tab:UniqueTableLabel) displays...\n\nwhich, when you knit to PDF, is displayed as:\n\nTable 1 displays…\n\nFor details on styling PDF tables, see this link."
  },
  {
    "objectID": "rd1_10.html",
    "href": "rd1_10.html",
    "title": "Continuous random variables",
    "section": "",
    "text": "Recall that a random variable is continuous if there are no gaps between the possible values it can take.\n\n\n\n\n\n\n\n\nImagine a spinner numbered with the hours of the day. If you were to spin it with some strength, what’s the probability of the hand stopping exactly at 3.5173847294 hours?\nWell, there is only one way to observe 3.5173847294, while there are an infinite number of possibilities between 0 and 24 hours (you could have as many decimals as you wish).\nDefine the event \\(A = \\{ 3.5173847294 \\}\\). If you were to calculate the probability of observing \\(A\\) using the standard formula you would get: \\[\nP(A) = \\frac{n_A}{n} = \\frac{1}{\\infty} = 0\n\\]\nThe probability of the hand stopping exactly at 3.5173847294 hours is 0.\nCheck this with R:\n\n1 / Inf\n\n[1] 0\n\n\nFor this reason, we cannot compute probabilities for continuous random variables in the same way as we did in the previous weeks (number of outcomes in the event / number of possible outcomes).\n\n\n\n\n\n\nKey point\nFor continuous random variables we can only compute the probability of observing a value in a given interval. For example, the probability that the hand stops in between 3 and 3.5 hours.\n\n\n\nConsider the histogram shown below. The height of each rectangle will tell you the number of people having a value within a specific interval. For example, the number of people having a value of \\(x\\) between 20 and 22.5."
  },
  {
    "objectID": "rd1_10.html#continuous-random-variables",
    "href": "rd1_10.html#continuous-random-variables",
    "title": "Continuous random variables",
    "section": "",
    "text": "Recall that a random variable is continuous if there are no gaps between the possible values it can take.\n\n\n\n\n\n\n\n\nImagine a spinner numbered with the hours of the day. If you were to spin it with some strength, what’s the probability of the hand stopping exactly at 3.5173847294 hours?\nWell, there is only one way to observe 3.5173847294, while there are an infinite number of possibilities between 0 and 24 hours (you could have as many decimals as you wish).\nDefine the event \\(A = \\{ 3.5173847294 \\}\\). If you were to calculate the probability of observing \\(A\\) using the standard formula you would get: \\[\nP(A) = \\frac{n_A}{n} = \\frac{1}{\\infty} = 0\n\\]\nThe probability of the hand stopping exactly at 3.5173847294 hours is 0.\nCheck this with R:\n\n1 / Inf\n\n[1] 0\n\n\nFor this reason, we cannot compute probabilities for continuous random variables in the same way as we did in the previous weeks (number of outcomes in the event / number of possible outcomes).\n\n\n\n\n\n\nKey point\nFor continuous random variables we can only compute the probability of observing a value in a given interval. For example, the probability that the hand stops in between 3 and 3.5 hours.\n\n\n\nConsider the histogram shown below. The height of each rectangle will tell you the number of people having a value within a specific interval. For example, the number of people having a value of \\(x\\) between 20 and 22.5."
  },
  {
    "objectID": "rd1_10.html#proportions-as-areas-the-density-histogram",
    "href": "rd1_10.html#proportions-as-areas-the-density-histogram",
    "title": "Continuous random variables",
    "section": "\n2 Proportions as areas: the density histogram",
    "text": "2 Proportions as areas: the density histogram\nThe natural gestation period for human births has a mean of about 266 days and a standard deviation of about 16 days. A group of researchers recorded the gestation period (in days) of 10,000 babies and the distribution is shown below as a frequency histogram (left panel) and relative frequency histogram (right panel).\n\n\n\n\nFrequency histogram (left panel) and relative frequency histogram (right panel) of gestation period.\n\n\n\nThe histogram is unimodal, with the modal interval extending from 266 to 270 days. The distribution is symmetric around the centre (266). This means that if you were the fold the histogram at 266 days, the two halves would match when superimposed.\nThe histogram also highlights some variability in gestation days from person to person, with some women giving birth after as low as 200 days or as high as 320 days.\nUsually, histograms are drawn with the height of the rectangle for the \\(j\\)th interval being either the frequency (count) \\(n_j\\) or the relative frequency \\(n_j / n\\) of observations falling into that interval.\nFor example, for roughly 400 women the gestation period was between 240 and 245 days. If we use the relative frequency, instead, the height of the rectangle tells us the proportion of observations in that interval. In this case, the proportion of women with a gestation period between 240 and 245 days is \\(400 / 10,000 = 0.04\\).\nWe now discuss a third type of histogram called the density histogram, in which we adjust the heigh of the rectangle so that the area is equal to the relative frequency. This is accomplished by changing the height to \\(n_j / (n w)\\) where \\(w\\) is the interval width.\nWhy does this work? \\[\narea = width \\cdot height = w \\cdot \\frac{n_j}{n w} = \\frac{n_j}{n}\n\\]\nSo now it is the area of the \\(j\\)th rectangle that tells us the proportion of observations in the \\(j\\)th interval.\n\n\n\n\n\n\n\n\nFigure 1: Density histogram of the gestation period for the 10,000 women in the study.\n\n\n\n\n\nIn Figure 1(a) the proportion of women under study whose gestation period falls within any class interval is the area of the corresponding rectangle. For example the proportion of women with a gestation period between 240 and 245 days is equal to \\(area = width \\cdot height = 5 \\cdot 0.008 = 0.04\\), which matches the relative frequency histogram.\nFurthermore, we can obtain the proportion of women whose gestation period falls between the limits \\(a = 240\\) days and \\(b = 255\\) days by adding the areas of all the rectangles between these limits. As the graph in Figure 1(b) tells us, the area of the shaded part is .196. Thus, the proportion of women with a gestation period between 240 and 255 days is 0.196 or 19.6% of the women in the study population.\nThis correspondence between area and proportions is the foundation on which the probability for continuous random variables is built. In fact, we can say that the probability that a randomly picked woman from the study population has a gestation period between 240 and 255 days is 0.196.\nAlso, note that the total area under a density histogram is \\[\n\\sum \\left( \\frac{n_j}{n w} \\cdot w \\right) = \\frac{\\sum  n_j}{n} = 1\n\\]\n\n\n\n\n\n\n\nLet’s now summarise what we have learned about density histograms:\n\nthe vertical scale is relative frequency / interval width\nthe total area under the histogram is 1\nthe proportion of data between \\(a\\) and \\(b\\) is the area under the histogram between \\(a\\) and \\(b\\)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Density curve of the gestation period for the 10,000 women in the sample.\n\n\n\n\n\nIn Figure 2(c) we have superimposed an approximating smooth curve on top of the density histogram. Figure 2(d) displays only the approximating smooth curve. Furthermore, we have also highlighted in green the area under that curve in between the same limit as the histogram (\\(a = 240\\) and \\(b = 255\\)) and calculated the highlighted area. That area turned out to be 0.194, which is very close to the area of 0.196 computed from the density histogram.\nFrom this, it is quite easy to see how areas under the smooth curve in between two particular limits are in good agreement with the same areas computed using a density histogram, and thus with proportions of people in the study.\nSuch smooth curve is called a density curve or density function. The probability distribution of a continuous random variable \\(X\\) is specified in terms of its density function. We calculate the probability that the random variable \\(X\\) is between \\(a\\) and \\(b\\) by finding the area under the density curve between \\(a\\) and \\(b\\).\n\n\n\n\n\n\nLet’s now summarise what we have learned about density functions:\n\nThe probability distribution of a continuous random variable is specified in terms of a density function (i.e., a density curve).\nProbabilities are obtained as areas under the density curve.\n\nThe total area under the curve is one.\nThis is because the total probability must be one: \\(P(R_X) = 1\\).\n\n\nThe curve is always greater than or equal to zero for all possible values of the random variable.\nThis is similar to saying that probabilities cannot be negative.\n\n\n\n\n\nLast week we said that the probability distribution of a random variable \\(X\\) provides a concise representation of the random variable in terms of the set of all the possible values it can take and the corresponding probabilities. Clearly, this gives an immediate global picture of the random variable.\n\n\n\n\n\n\nProbability distribution for a continuous random variable\nThe probability distribution of a continuous random variable \\(X\\) provides the possible values of the random variable and their corresponding probability densities.\nA probability distribution can be in the form of a graph or mathematical formula.\n\n\n\nThe density curve is the graphical representation of the probability distribution, while the density function is the mathematical formula of the curve.\n\n\n\n\n\n\nHelp! I am slightly lost…\n\n\n\n\n\nTo revise the following topics click on the links below:\n\nSemester 1, Week 2: Frequency distributions\nSemester 1, Week 3: Histograms and densities"
  },
  {
    "objectID": "rd1_10.html#the-normal-distribution",
    "href": "rd1_10.html#the-normal-distribution",
    "title": "Continuous random variables",
    "section": "\n3 The normal distribution",
    "text": "3 The normal distribution\nA symmetric and bell-shaped density curve is called a normal density curve.\n\n\n\n\n\n\nNormal distribution\nA continuous random variable is said to be normally distributed, or to have a normal probability distribution, if its distribution has the shape of a normal curve (it is symmetric and bell-shaped).\n\n\n\nThe normal density function is the function used to calculate probabilities (as areas under the density curve) for a normally distributed random variable.\n\n\n\n\n\n\nKey question\nHow can we make sure that the normal curve can adapt to any symmetric and bell-shaped density histogram?\n\n\n\nEvery histogram can be centred at a different value and have a different spread. Hence, we need to specify:\n\nwhere the normal curve should be centred at;\nwhat should the spread of the normal curve be.\n\nFrom this we can see that the normal curve depends on two quantities called the parameters of the normal distribution (see Figure 3):\n\nthe mean \\(\\mu\\), specifying the centre of the distribution;\nthe standard deviation \\(\\sigma\\), specifying the spread of the distribution.\n\n\n\n\n\n\n\n\nFigure 3: Normal curves for different means and standard deviations.\n\n\n\n\nAs we can see, changing \\(\\mu\\) shifts the curve along the axis, while increasing \\(\\sigma\\) increases the spread and flattens the curve.\nThe formula for the normal density curve which we plotted above is provided below. However, you do not have to memorise it as this is already implemented in R: \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}\n\\] for some values of \\(\\mu\\) and \\(\\sigma\\).\nThe same function in R is:\ndnorm(x, mean = &lt;mean&gt;, sd = &lt;sd&gt;)\nwhere:\n\n\nx represents the value of the random variable\n\nmean the mean of the normal curve\n\nsd the standard deviation of the curve\n\nWe write that a random variable \\(X\\) follows a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as \\[\nX \\sim N(\\mu, \\sigma)\n\\]\nClearly, the mean and standard deviation of \\(X\\) will be \\(E(X) = \\mu\\) and \\(SD(X) = \\sigma\\) respectively.\nWe use lowercase letters such as \\(x, a, b\\) to denote an observed value, or a particular value, of the random variable \\(X\\)."
  },
  {
    "objectID": "rd1_10.html#the-68-95-99.7-rule",
    "href": "rd1_10.html#the-68-95-99.7-rule",
    "title": "Continuous random variables",
    "section": "\n4 The 68-95-99.7% rule",
    "text": "4 The 68-95-99.7% rule\nA variable which is normally distributed has:\n\n68% of the observations (values) within \\(1\\sigma\\) of the \\(\\mu\\)\n95% of its observations (values) within \\(2\\sigma\\) of the \\(\\mu\\)\n99.7% of its observations (values) within \\(3\\sigma\\) of the \\(\\mu\\)"
  },
  {
    "objectID": "rd1_10.html#obtaining-probabilities",
    "href": "rd1_10.html#obtaining-probabilities",
    "title": "Continuous random variables",
    "section": "\n5 Obtaining probabilities",
    "text": "5 Obtaining probabilities\nBefore describing how to compute the probabilities for a continuous random variable using R, we need to discuss an important property that only holds for continuous random variables:\n\n\n\n\n\n\nInterval endpoints for continuous random variables\nWhen performing calculations involving a continuous random variable, we do not have to worry about whether interval endpoints are included or not in the calculation: \\[\nP(a \\leq X \\leq b) = P(a \\leq X &lt; b) = P(a &lt; X \\leq b) = P(a &lt; X &lt; b)\n\\]\nThis is due to the fact that the probability of \\(X\\) being equal to a specific value is 0: \\[\n\\begin{aligned}\nP(X \\leq x) &= P(X &lt; x) + P(X = x) \\\\\n&= P(X &lt; x) + 0 \\\\\n&= P(X &lt; x)\n\\end{aligned}\n\\]\n\n\n\nWe compute areas under the normal curve using the function pnorm(x, mean, sd). This returns the area to the left of x in a normal curve centred at mean and having standard deviation sd: \\[\nP(X \\leq x) = P(X &lt; x) = \\texttt{pnorm}(x, \\texttt{ mean = } \\mu, \\texttt{ sd = } \\sigma)\n\\]\nThe following figure shows the type of areas returned by the pnorm function:\n\n\n\n\nR computes lower-tail probabilities.\n\n\n\nFor example, the probability that the random variable \\(X \\sim N(\\mu = 5, \\sigma = 2)\\) is less than 1 is:\n\npnorm(1, mean = 5, sd = 2)\n\n[1] 0.02275013\n\n\n\nThere are three types of probabilities that we might want to compute:\n\nThe probability that \\(X\\) is less than a given value: \\(P(X &lt; x)\\)\nThe probability that \\(X\\) is between a lower and an upper value: \\(P(x_l &lt; X &lt; x_u)\\)\nThe probability that \\(X\\) is greater than a given value: \\(P(X &gt; x)\\)\n\nWe will now analyse each case in turn and show how each of these is computed.\n\n5.1 1. Area to the left of a value \\(x\\)\n\nThe probability that \\(X &lt; x\\) is directly returned by pnorm():\n\n\n\n\n\n\n\n\n\n5.2 2. Area between the values \\(x_l\\) and \\(x_u\\)\n\nThe probability that \\(X\\) is between a lower (\\(x_l\\)) and an upper (\\(x_u\\)) value can be obtained as: \\[\nP(x_l &lt; X &lt; x_u) = P(X &lt; x_u) - P(X &lt; x_l)\n\\]\n\n\n\n\n\n\n\n\n\n5.3 3. Area to the right of \\(x\\)\n\nThe probability to the right of \\(x\\) is one minus the probability to the left of \\(x\\): \\[\nP(X &gt; x) = 1 - P(X &lt; x)\n\\]"
  },
  {
    "objectID": "rd1_10.html#the-inverse-problem-quantiles-and-percentiles",
    "href": "rd1_10.html#the-inverse-problem-quantiles-and-percentiles",
    "title": "Continuous random variables",
    "section": "\n6 The inverse problem: quantiles and percentiles",
    "text": "6 The inverse problem: quantiles and percentiles\nIn this section we will examine the inverse problem to the one of calculating the probability that \\(X &lt; x\\).\nGiven a probability \\(p\\), what’s the value \\(x\\) that cuts to its left a probability of \\(p\\)?\n\n\n\n\n\n\n\n\nThat value, called the \\(p\\)-quantile, is the value \\(x_p\\) for which the lower-tail probability is \\(p\\):\n\\[\n\\text{value }x_p\\text{ such that} \\qquad P(X \\leq x_p) = p\n\\]\nPercentiles and quantiles are two words used to express the same idea. We use percentile when speaking in terms of percentages and quantile when speaking in terms of proportions (or probabilities) between 0 and 1.\nThe 50th percentile (or \\(0.5\\)-quantile) is the value \\(x_{0.5}\\) that cuts a 0.5 probability to its left. You actually know that the value such that 50% of the observations are lower and 50% are higher is called the median.\n In R, you calculate the \\(p\\)-quantile with the function\nqnorm(p, mean, sd)\nThis will return the value \\(x\\) such that \\(P(X \\leq x) = p\\). That value is typically written \\(x_p\\).\n For example, if \\(X \\sim N(0, 1)\\), the value cutting an area of 0.025 to its left is\n\nqnorm(0.025, 0, 1)\n\n[1] -1.959964\n\n\nwhile that cutting an area of 0.975 to its left is\n\nqnorm(0.975, 0, 1)\n\n[1] 1.959964\n\n\nThe \\(0.025\\)-quantile is \\(x_{0.025} = -1.96\\), and the \\(0.975\\)-quantile is \\(x_{0.975} = 1.96\\)."
  },
  {
    "objectID": "rd1_10.html#working-in-standard-units",
    "href": "rd1_10.html#working-in-standard-units",
    "title": "Continuous random variables",
    "section": "\n7 Working in standard units",
    "text": "7 Working in standard units\nInstead of always specifying the mean and the standard deviation of the normal distribution, we can work with a reference normal distribution that has a mean equal to 0 and standard deviation equal to 1. This is known as the standard normal distribution.\n\n\n\n\n\n\nStandard normal distribution\nThe standard normal distribution, denoted \\(Z \\sim N(0, 1)\\), is a normal distribution with mean = 0 and standard deviation = 1.\n\n\n\nThe R functions dnorm(x, mean, sd) and pnorm(x, mean, sd) assume that mean = 0 and sd = 1 if not provided. In other words, they assume a standard normal distribution by default.\nFor example, the following two functions return the same output\n\ndnorm(2)\n\n[1] 0.05399097\n\ndnorm(2, mean = 0, sd = 1)\n\n[1] 0.05399097\n\n\nand similarly for pnorm().\nIn order to transform a value \\(x\\) from a normal distribution with mean = \\(\\mu\\) and sd = \\(\\sigma\\) to a score on the standard normal scale, we must use the z-score transformation.\nThe z-score measures distance in terms of the number of standard deviations from the mean: \\[\nz = \\frac{x - \\mu}{\\sigma} \\qquad (\\text{standard units or } z\\text{-}scores)\n\\]\n\n\n\n\n\n\nz-score\nThe z-score of \\(x\\) is the number of standard deviations \\(x\\) is from the mean.\n\n\n\n\nLet \\(X \\sim N(\\mu, \\sigma)\\) and \\(Z \\sim N(0, 1)\\). The following relation holds: \\[\nP(X &lt; x) = P \\left( \\frac{X - \\mu}{\\sigma} &lt; \\frac{x - \\mu}{\\sigma} \\right) = P(Z &lt; z)\n\\]\nThis means that the probability, when computed on the original normal distribution or on the standard normal distribution, will be the same as long as you use the z-score of \\(x\\) for the standard normal distribution.\n\nConsider the following cases:\n\nYou observe \\(x = 4.3\\) from a \\(N(3, 2)\\) distribution. The z-score is \\(z = (4.3 - 3) / 2 = 0.65\\)\n\nYou observe \\(x = 4.3\\) from a \\(N(3, 0.2)\\) distribution. The z-score is \\(z = (4.3 - 3) / 0.2 = 6.5\\)\n\n\nIn case (a), your observed value 4.3 is 0.65 standard deviations away from the mean. In case (b), the observed value 4.3 is 6.5 standard deviations away from the mean.\nObserving a value of 4.3 from a \\(N(3,2)\\) population is very likely, as 68% of the observations are within 1 \\(\\sigma\\) of the mean. However, observing a value of 4.3 from a \\(N(3,0.2)\\) distribution is very unlikely, as almost all the values will be within 3 \\(\\sigma\\) of the mean. Such a high z-score provides us with an element of surprise. We might want to inspect that value further as it could be an outlier or a typing error of who has entered the data.\nAs you can see, z-scores are particularly useful for comparing observations across distributions having a different mean and standard deviation."
  },
  {
    "objectID": "rd1_10.html#summary",
    "href": "rd1_10.html#summary",
    "title": "Continuous random variables",
    "section": "\n8 Summary",
    "text": "8 Summary\nWhen a histogram shows that a variable’s distribution is symmetric and bell-shaped, we can say that the variable is normally distributed and we can model the distribution with a mathematical curve called the normal density function.\nWe saw that the normal distribution depends on two parameters that control the centre and spread of the normal density curve:\n\nthe mean;\nthe standard deviation.\n\nThese two parameters make sure that the normal curve can fit histograms that have different centres and spreads.\nIn other words, if you know that a distribution is normal (shape), then the mean (centre) and standard deviation (spread) tell you everything else about the distribution.\nIn R, the equation of the normal density curve is given by the function dnorm(x, mean, sd).\nWe can use the normal curve to compute the probability of intervals as the area under the curve in that interval. The function to compute the area under a normal curve to the left of x is pnorm(x, mean, sd).\n\n\n\n\n\n\nProbability\nR command\n\n\n\nProbability of observing a value less than or equal x\n\npnorm(x, mean, sd)\n\n\nProbability of observing a value between xl and xu\n\npnorm(xu, mean, sd) - pnorm(xl, mean, sd)\n\n\nProbability of observing a value greater than xu\n\n1 - pnorm(xu, mean, sd)\n\n\n\nThe value \\(x_p\\) cutting an area = \\(p\\) to its left is returned by qnorm(p, mean, sd). This is called the \\(p\\)-quantile of \\(X\\)."
  },
  {
    "objectID": "rd1_10.html#glossary",
    "href": "rd1_10.html#glossary",
    "title": "Continuous random variables",
    "section": "\n9 Glossary",
    "text": "9 Glossary\n\n\nContinuous random variable. A variable that takes on numerical values determined by a chance process which can have any number of decimals.\n\nDensity function. The function which provides the probability that a random variable is within an interval in terms of the area under that curve between that interval.\n\nNormal distribution. The probability distribution of a random variable which has a symmetric and bell shaped density curve.\n\n\\(p\\)-quantile. The \\(x_p\\) value cutting an area = \\(p\\) to its left. In other words, the \\(x_p\\) value such that \\(P(X \\leq x_p) = p\\)."
  },
  {
    "objectID": "rd1_08.html",
    "href": "rd1_08.html",
    "title": "Probability 2",
    "section": "",
    "text": "Let’s recap the probability rules discussed last week.\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event “not A” is the complement of A and is written \\(A^c\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(A^c) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#recap",
    "href": "rd1_08.html#recap",
    "title": "Probability 2",
    "section": "",
    "text": "Let’s recap the probability rules discussed last week.\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event “not A” is the complement of A and is written \\(A^c\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(A^c) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#general-addition-rule",
    "href": "rd1_08.html#general-addition-rule",
    "title": "Probability 2",
    "section": "\n2 General addition rule",
    "text": "2 General addition rule\nConsider throwing a six-sided die, for which we already know that the sample space is \\(S = \\{1, 2, 3, 4, 5, 6\\}\\). What’s the probability of observing an odd number or a number which is less than 4?\nThe relevant events are:\n\n\\(A = \\text{result is odd} = \\{1, 3, 5\\}\\)\n\\(B = \\text{result is less than four} = \\{1, 2, 3\\}\\)\n\nand we know that:\n\n\\(P(A) = 3/6 = 0.5\\)\n\\(P(B) = 3/6 = 0.5\\)\n\nHowever, the probability of \\(A\\) or \\(B\\), written \\(P(A \\cup B)\\), is not simply \\(P(A) + P(B)\\), because the events \\(A\\) and \\(B\\) are not disjoint; they overlap. The outcomes 1 and 3 play a crucial role here as they are both odd and are less than four, i.e. they are in both sets. They are both in \\(A\\) and \\(B\\), which places them in the intersection of the two circles.\nThe diagram below represents the sample space as \\(S\\). Notice that the outcomes 4 and 6 are neither odd nor less than four, so they sit outside both circles.\n\n\n\n\n\n\n\n\nThe reason we can’t simply add the probabilities of \\(A\\) and \\(B\\) is that we would count the outcomes 1 and 3 twice. If we did add the two probabilities, we could compensate by subtracting the probability of the outcomes 1 and 3.\n\\[\n\\begin{aligned}\nP(\\text{odd or less than 4}) &= P(\\text{odd}) + P(\\text{less than 4}) - P(\\text{odd and less than 4}) \\\\\n&= P(\\{1, 3, 5\\}) + P(\\{1, 2, 3\\}) - P(\\{1, 3\\}) \\\\\n&= 3/6 + 3/6 - 2/6 \\\\\n&= 0.667\n\\end{aligned}\n\\]\nThis is true in general and is called the general addition rule, which does not require disjoint events. Consider two generic events \\(A\\) and \\(B\\) such that\n\n\n\n\n\n\n\n\nThen, the probability of observing \\(A\\) or \\(B\\) is: \\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nIntuitively, \\(P(A) + P(B)\\) counts \\(A \\cap B\\) twice, so we have to subtract \\(P(A \\cap B)\\) to make the net number of times \\(A \\cap B\\) is counted equal to 1.\nWhen the two events have no outcomes in common, i.e. \\(P(A \\cap B) = 0\\) as the two events can’t happen together, the general addition rule reduces to the addition rule for disjoint events.\n\n\n\n\n\n\nTERMINOLOGY\n\n\n\n\nWould you like fruit or dessert?\n\nEveryday language can be ambiguous. It’s not clear if by answering “yes” to the question above our choice is only one of the two options, or we can actually have both.\nTypically, everyday language uses “or” meaning the exclusive version. That is, you can have one or the other, but not both. In other words, you can have fruit or dessert, but not both.\nWhen talking about probability, being imprecise and ambiguous can get us into trouble. For this reason, we will here define once for all what we mean by “or” in a mathematical sense.\nIn statistics, when we say “or” we always mean the inclusive version. In other words, the probability of \\(A\\) or \\(B\\) means the probability of either \\(A\\) or \\(B\\) or both.\n\n\nLet’s consider another example. Suppose that 25% of people have an electric scooter, 29% of people have a bike, and 12% of people own both. What is the probability that someone owns an electric scooter or a bike?\nThis question concerns the following two events\n\n\\(A = \\text{owning an electric scooter}\\)\n\\(B = \\text{owning a bike}\\)\n\nand we are told that\n\n\\(P(A) = 0.25\\)\n\\(P(B) = 0.29\\)\n\\(P(A \\cap B) = 0.12\\)\n\nClearly, \\(A\\) and \\(B\\) are not mutually exclusive (or disjoint) events. Having a scooter doesn’t exclude owning a bike, and vice versa. In fact, the problem statements tells us exactly the percentage of people having both an electric scooter and a bike, 12%.\nSo… how do we calculate the probability of owning an electric scooter or a bike?\nIf we simply did \\(P(A) + P(B)\\), we would count twice those having both a scooter and a bike. Hence, we need to subtract \\(P(A \\cap B)\\): \\[\n\\begin{aligned}\nP(A \\cup B) &= P(A) + P(B) - P(A \\cap B) \\\\\n&= 0.25 + 0.29 - 0.12 \\\\\n&= 0.42\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rd1_08.html#conditional-probability",
    "href": "rd1_08.html#conditional-probability",
    "title": "Probability 2",
    "section": "\n3 Conditional probability",
    "text": "3 Conditional probability\nThe following contingency table displays the counts of passengers who did or did not survive the Titanic sinking by travelling class.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\nClass\nNo\nYes\nTotal\n\n\n\n\n1st\n122\n203\n325\n\n\n2nd\n167\n118\n285\n\n\n3rd\n528\n178\n706\n\n\nTotal\n817\n499\n1316\n\n\n\n\n\nYou already encountered contingency tables when studying the relationship between two categorical variables in week 4.\nIf you divide the frequency table by the total number of passengers, 1316, you obtain a table of relative frequencies. It is just a small step from these relative frequencies to probabilities.\nLet;s focus on the Titanic survival study and make the sample space just the set of these 1316 passengers. If we select a passenger at random from this study, the probability we select a first class passenger is just the corresponding relative frequency (since we are equally likely to select any of the 1316 passengers). There are 325 first class passengers in the data out of a total of 1316, giving a probability of \\[\nP( \\text{1st} ) = \\frac{325}{1316} = 0.247\n\\] The same method works for more complicated events like intersections. For example, what’s the probability of selecting a 1st class passenger who survived? Well, 203 first class passengers survived, so the probability is \\[\nP( \\text{1st} \\cap \\text{survived} ) = \\frac{203}{1316} = 0.154\n\\]\nThe probability of selecting a passenger who survived is \\[\nP( \\text{survived} ) = \\frac{499}{1316} = 0.379\n\\] What if we are given the information that the selected passenger was in first class? Would that change the probability that the selected passenger survived? You bet it would!\nWhen we restrict our focus to first class passengers, we look only at the row of the table where “Class” is “1st”. Of the 325 first class passengers, only 203 of them said survived. We write the probability that a selected passenger survived given that we have selected a first class passenger as \\[\nP(\\text{survived} \\mid \\text{1st}) = \\frac{203}{325} = 0.625\n\\]\nNow, imagine dividing numerator and denominator by the total, 1316: \\[\n\\begin{aligned}\nP(\\text{survived} \\mid \\text{1st})\n= \\frac{203 / 1316}{325 / 1316}\n= \\frac{P(\\text{1st} \\cap \\text{survived})}{P(\\text{1st})}\n\\end{aligned}\n\\]\nA probability that takes into account a given condition, such as being a first class passenger, is called a conditional probability. For generic events \\(A\\) and \\(B\\), we write the conditional probability as \\(P(B | A)\\) and pronounce it “the probability of \\(B\\) given \\(A\\)”.\nSuppose we are told that the event \\(A\\) with \\(P(A) &gt; 0\\) occurs. The conditional probability of \\(B\\) given \\(A\\) is given by: \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)}\n\\] Intuitively, we need to divide the formula by \\(P(A)\\) to make sure that \\(P(A|A) = 1\\).\nThe formula wouldn’t work if \\(P(A) = 0\\), but this makes sense as we couldn’t be told that \\(A\\) occurred if it was an impossible event!\n\n\n\n\n\n\nIMPORTANT NOTE\n\n\n\nThe conditional probability \\(P(\\text{some event} | A)\\) satisfies the standard rules of probability. You can think of it as the same as \\(P(\\text{some event})\\) but after having replaced the sample space from \\(S\\) to \\(A\\), which is the event that we have been told happened. As the event happened, we know that the new sample space is now \\(A\\), so that \\(P(A) = 1\\).\nTo see this, remember that \\(P(S) = 1\\) and we can then rewrite\n\\[P(A) = \\frac{P(A)}{1} = \\frac{P(A)}{P(S)} = \\frac{P(A \\cap S)}{P(S)} = P(A | S)\\]\nThe rule \\(P(A) + P(A^c) = 1\\) can be written as \\(P(A | S) + P(A^c | S) = 1\\).\nComplement rule for conditional probabilities\nIn general, according to the complement rule, \\(P(B^c|A) = 1 - P(B|A)\\).\nProduct rule for conditionally independent events\nWe also have a similar rule for independence. Two events \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if:\n\\[\nP(A \\cap B | C) = P(A | C) \\times P(B | C)\n\\]\n\n\nConsider the following plot:\n\n\n\n\n\n\n\n\nOn the y-axis we see the conditional probabilities of survival given the passenger class. That is \\(P(\\text{survived} | \\text{1st}) = 0.62\\), \\(P(\\text{survived} | \\text{2nd}) = 0.41\\), and \\(P(\\text{survived} | \\text{3rd}) = 0.25\\).\nThe x-axis displays the probability of a randomly selected passenger to be in each travelling class. For example, \\(P(\\text{1st}) = 0.25\\), \\(P(\\text{2nd}) = (0.46 - 0.25) = 0.21\\), and \\(P(\\text{3rd}) = 1 - (P(\\text{1st}) + P(\\text{2nd})) = 1 - 0.25 - 0.21 = 1 - 0.46 = 0.54\\)"
  },
  {
    "objectID": "rd1_08.html#general-multiplication-rule",
    "href": "rd1_08.html#general-multiplication-rule",
    "title": "Probability 2",
    "section": "\n4 General multiplication rule",
    "text": "4 General multiplication rule\nConsider two generic events \\(A\\) and \\(B\\). Before, we saw that \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nRearranging the conditional probability formula we obtain the general multiplication rule: \\[\nP(A \\cap B) = P(A) P(B|A)\n\\]\nIf you were to start from \\(P(A | B)\\) you would reach to the equivalent version: \\[\nP(A \\cap B) = P(B) P(A|B)\n\\]"
  },
  {
    "objectID": "rd1_08.html#independence",
    "href": "rd1_08.html#independence",
    "title": "Probability 2",
    "section": "\n5 Independence",
    "text": "5 Independence\nWe say that two events \\(A\\) and \\(B\\) are independent, if knowing that one occurred doesn’t change the probability of the other occurring:\n\\[\nP(B|A) = P(B)\n\\]\nFrom this we have that\n\\[\nP(B) = P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nLeading to\n\\[\nP(A \\cap B) = P(A) P(B)\n\\] At the same time,\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A) P(B)}{P(B)} = P(A)\n\\]\nSo, we have three equivalent definitions of independent events! If one holds, the remaining ones hold as well. Two events \\(A\\) and \\(B\\) are independent if and only if \\[\n\\begin{aligned}\nP(A | B) &= P(A) \\\\\nP(B | A) &= P(B) \\\\\nP(A \\cap B) &= P(A) P(B)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nIn pictures: Independence vs Dependence\n\n\n\n\n\nIndependence happens when the probability of “Survived = Yes” is the same regardless of whether the passenger was in 1st, 2nd, or 3rd class.\n\n\n\n\n\n\n\n\nPerfect dependence happens when knowing the passenger class leads to a perfect prediction of whether a passenger survived or not.\n\n\n\n\n\n\n\n\n\n\n\nNote that, for independent events \\(A\\) and \\(B\\) such that \\(P(B | A) = P(B)\\), the general multiplication rule reduces to the multiplication rule for independent events: \\[\nP(A \\cap B) = P(B|A)P(A) = P(B) P(A)\n\\]"
  },
  {
    "objectID": "rd1_08.html#bayes-rule-reversing-the-conditioning",
    "href": "rd1_08.html#bayes-rule-reversing-the-conditioning",
    "title": "Probability 2",
    "section": "\n6 Bayes’ rule: reversing the conditioning",
    "text": "6 Bayes’ rule: reversing the conditioning\nSuppose we have \\(P(A | B)\\) but we are interested in \\(P(B | A)\\). That is, we want to reverse the conditioning.\nBy applying the rule for conditional probability and the general multiplication rule, we have that: \\[\nP(B | A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{P(A|B)P(B)}{P(A)}\n\\]"
  },
  {
    "objectID": "rd1_08.html#glossary",
    "href": "rd1_08.html#glossary",
    "title": "Probability 2",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nGeneral addition rule. For any events \\(A\\) and \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nGeneral multiplication rule. For any events \\(A\\) and \\(B\\), \\(P(A \\cap B) = P(B|A)P(A) = P(A | B) P(B)\\).\n\nIndependent events. Two events \\(A\\) and \\(B\\) are independent if and only if \\(P(A \\cap B) = P(A) P(B)\\)\n\n\nBayes’ rule. \\(P(B | A) = \\frac{P(A | B)P(B)}{P(A)}\\)"
  },
  {
    "objectID": "rd1_05.html",
    "href": "rd1_05.html",
    "title": "Types of relationships",
    "section": "",
    "text": "You have seen by now how to visualise the distribution of a variable, and how to visualise a relationship between two variables. Relationships between two variables can look very different, and can follow different patterns. These patterns can be expressed mathematically in the form of functions.\n\n\n\n\n\n\nFunctions\nA function is a mapping between two sets of numbers (\\(x\\) and \\(y\\)) - associating every element of \\(x\\) with an element in \\(y\\).\nWe often denote functions using the letter \\(f\\), in that we state that \\(y = f(x)\\) (“y equals f of x”).\nFor example, there is a mapping between these two sets:\n\\[x=\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\ 10 \\\\ \\end{bmatrix}\\]\nAnd we can write this mapping as:\\[f(x) = x + 2\\]\nAnd we could visualise this relationship between \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\n\nIn statistics, we often attempt to summarise the pattern that is present in the data using linear functions.\nImagine that we plant 10 trees, and measure their heights each year for 10 years. We could visualise this data (the relationship between time and tree height) on a scatterplot (we have added some lines to the plot to show which tree is which):\n\n\n\n\n\n\n\n\nWe might sensibly choose to describe this pattern as a line: \n\n\n\n\n\n\n\n\nAnd in order to describe a line like this, we require two things:\n\nThe starting point (i.e., where it crosses the y-axis)\nThe amount it goes up every year.\n\nWhen we planted the trees (at year 0), they were on average about 5cm tall. So this is where our line starts.\nFor every year, the trees grew by about 10cm on average. So we can now describe tree height as a function of time:\\[\\textrm{Tree height} = 5 + (10 \\times \\textrm{Years})\\]\nWe can write this in terms of \\(x\\) and \\(y\\):\n\n\n\\(y = f(x)\\)         “\\(y\\) is some function \\(f\\) of \\(x\\)”\n\n\\(f(x) = 5 + 10x\\)         “the function \\(f\\) maps each value \\(x_i\\) to \\(5 + (10 \\times x_i)\\)”\n\nFunctions don’t have to be linear. Often, we might want to describe relationships which appear to be more complex than a straight line.\nFor example, it is often suggested that for difficult tasks, some amount of stress may improve performance (but not too little or too much). We might think of the relationship between performance and stress as a curve (Figure 1). \n\n\n\n\n\n\n\nFigure 1: Yerkes Dodson Law\n\n\n\n\n One way to describe curves is to use polynomials (\\(x^2\\), \\(x^3\\), etc.).\nFor example, in the following two sets, \\(y\\) can be described as \\(f(x)\\) where \\(f(x)=x^2\\):\n\\[x=\\begin{bmatrix} -5 \\\\ -4 \\\\ -3 \\\\ -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 25 \\\\ 16 \\\\ 9 \\\\ 4 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25 \\end{bmatrix}\\]\nand when we plot each value of \\(x\\) against the corresponding value of \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to create the above plot\n\n\n\n\n\n\n# the tibble() function can be used to create a dataframe\n# here, we create one with two variables, x and y.\n# x is the sequence from -5 to 5,\n# y is equal to x squared. \n# we save this data as \"poly\"\npoly &lt;- \n    tibble(\n        x = c(-5,-4,-3,-2,-1,0,1,2,3,4,5),\n        y = x^2\n    )\n# create a plot with \"poly\", with variable \"x\" on the x-axis,\n# and variable \"y\" on the y-axis. Add geom_points for each entry\nggplot(data = poly, aes(x = x, y = y)) + \n    geom_point()"
  },
  {
    "objectID": "rd1_05.html#functions",
    "href": "rd1_05.html#functions",
    "title": "Types of relationships",
    "section": "",
    "text": "You have seen by now how to visualise the distribution of a variable, and how to visualise a relationship between two variables. Relationships between two variables can look very different, and can follow different patterns. These patterns can be expressed mathematically in the form of functions.\n\n\n\n\n\n\nFunctions\nA function is a mapping between two sets of numbers (\\(x\\) and \\(y\\)) - associating every element of \\(x\\) with an element in \\(y\\).\nWe often denote functions using the letter \\(f\\), in that we state that \\(y = f(x)\\) (“y equals f of x”).\nFor example, there is a mapping between these two sets:\n\\[x=\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\ 10 \\\\ \\end{bmatrix}\\]\nAnd we can write this mapping as:\\[f(x) = x + 2\\]\nAnd we could visualise this relationship between \\(x\\) and \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\n\nIn statistics, we often attempt to summarise the pattern that is present in the data using linear functions.\nImagine that we plant 10 trees, and measure their heights each year for 10 years. We could visualise this data (the relationship between time and tree height) on a scatterplot (we have added some lines to the plot to show which tree is which):\n\n\n\n\n\n\n\n\nWe might sensibly choose to describe this pattern as a line: \n\n\n\n\n\n\n\n\nAnd in order to describe a line like this, we require two things:\n\nThe starting point (i.e., where it crosses the y-axis)\nThe amount it goes up every year.\n\nWhen we planted the trees (at year 0), they were on average about 5cm tall. So this is where our line starts.\nFor every year, the trees grew by about 10cm on average. So we can now describe tree height as a function of time:\\[\\textrm{Tree height} = 5 + (10 \\times \\textrm{Years})\\]\nWe can write this in terms of \\(x\\) and \\(y\\):\n\n\n\\(y = f(x)\\)         “\\(y\\) is some function \\(f\\) of \\(x\\)”\n\n\\(f(x) = 5 + 10x\\)         “the function \\(f\\) maps each value \\(x_i\\) to \\(5 + (10 \\times x_i)\\)”\n\nFunctions don’t have to be linear. Often, we might want to describe relationships which appear to be more complex than a straight line.\nFor example, it is often suggested that for difficult tasks, some amount of stress may improve performance (but not too little or too much). We might think of the relationship between performance and stress as a curve (Figure 1). \n\n\n\n\n\n\n\nFigure 1: Yerkes Dodson Law\n\n\n\n\n One way to describe curves is to use polynomials (\\(x^2\\), \\(x^3\\), etc.).\nFor example, in the following two sets, \\(y\\) can be described as \\(f(x)\\) where \\(f(x)=x^2\\):\n\\[x=\\begin{bmatrix} -5 \\\\ -4 \\\\ -3 \\\\ -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\qquad\ny=\\begin{bmatrix} 25 \\\\ 16 \\\\ 9 \\\\ 4 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 4 \\\\ 9 \\\\ 16 \\\\ 25 \\end{bmatrix}\\]\nand when we plot each value of \\(x\\) against the corresponding value of \\(y\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to create the above plot\n\n\n\n\n\n\n# the tibble() function can be used to create a dataframe\n# here, we create one with two variables, x and y.\n# x is the sequence from -5 to 5,\n# y is equal to x squared. \n# we save this data as \"poly\"\npoly &lt;- \n    tibble(\n        x = c(-5,-4,-3,-2,-1,0,1,2,3,4,5),\n        y = x^2\n    )\n# create a plot with \"poly\", with variable \"x\" on the x-axis,\n# and variable \"y\" on the y-axis. Add geom_points for each entry\nggplot(data = poly, aes(x = x, y = y)) + \n    geom_point()"
  },
  {
    "objectID": "rd1_05.html#transformations",
    "href": "rd1_05.html#transformations",
    "title": "Types of relationships",
    "section": "\n2 Transformations",
    "text": "2 Transformations\nWe have seen previously how we might change all the values in a variable, for instance if we want to turn heights from centimetres to metres:\n\n# read in the starwars dataset and assign it the name \"starwars2\"\nstarwars2 &lt;- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\n\n# take the starwars2 dataframe |&gt;\n# mutate it such that there is a new variable called \"height_m\",\n# the values of which are equal to the \"height\" variable divided by 100.\n# then, select only the \"height\" and \"height_m\" columns (this is just \n# to make it easier to see without all the other variables)\nstarwars2 |&gt;\n    mutate(\n        height_m = height/100\n    ) |&gt; \n    select(height, height_m)\n\n# A tibble: 75 × 2\n   height height_m\n    &lt;dbl&gt;    &lt;dbl&gt;\n 1    172     1.72\n 2    167     1.67\n 3     96     0.96\n 4    202     2.02\n 5    150     1.5 \n 6    178     1.78\n 7    165     1.65\n 8     97     0.97\n 9    183     1.83\n10    182     1.82\n# ℹ 65 more rows\n\n\nWhat we have done here, can be described as a transformation, in that we have applied a mathematical function to the values in the height variable.\n\n\n\n\n\n\nTransformation\nData transformation is when we apply a deterministic function to map each value of a variable to a transformed value.\nWe transform for various reasons. For instance, we can use it to change the units we are interpreting (e.g., cm to m), or to change the shape of a distribution (e.g., make it less skewed).\n\n\n\nWe could even plot the heights in cm and heights in m against one another (note what units are on each axis):\n\n\n\n\n\n\n\n\n The relationship between a variable and a transformed variable need be linear, for example, log transformation: \n\n\n\n\n\n\n\n\n\n2.1 A recap of Logarithms and Natural Logarithms\n\n\n\n\n\n\nLogarithm\nA logarithm (log) is the power to which a number must be raised in order to get some other number.\nTake as examples \\(10^2\\) and \\(10^3\\):\n\\[\n10^2 = 100 \\quad \\Longleftrightarrow \\quad \\log_{10} (100) = 2\n\\]\n\\[\n10^3 = 1000 \\quad \\Longleftrightarrow \\quad \\log_{10} (1000) = 3\n\\]\nWe refer to \\(\\log_{10}\\) as “Log base 10”.\n\n\n\n\n\n\n\n\n\nNatural log\nA special case of the logarithm is referred to as the natural logarithm, and is denoted by \\(ln\\) or \\(log_e\\), where \\(e = 2.718281828459...\\)\n\\(e\\) is a special number for which \\(log_e(e) = 1\\).\nIn R, this is just the log() function, which uses the base \\(e\\) by default.\n\n# Natural logarithm of e is 1:\nlog(2.718281828459, base = 2.718281828459)\n\n[1] 1\n\nlog(2.718281828459)\n\n[1] 1"
  },
  {
    "objectID": "rd1_05.html#centering-and-standardisation",
    "href": "rd1_05.html#centering-and-standardisation",
    "title": "Types of relationships",
    "section": "\n3 Centering and Standardisation",
    "text": "3 Centering and Standardisation\nRecall our dataset from our introduction to handling numerical data, in which we had data on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale, WAIS), their ages, and their scores on 2 other tests. We know how to calculate the mean and standard deviation of the IQ scores:\n\n# read in the data\nwechsler &lt;- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\n\n# calculate the mean and sd of IQs\nwechsler |&gt; \n  summarise(\n    mean_iq = mean(iq),\n    sd_iq = sd(iq)\n  )\n\n# A tibble: 1 × 2\n  mean_iq sd_iq\n    &lt;dbl&gt; &lt;dbl&gt;\n1    99.3  15.4\n\n\nTwo very useful transformations we can apply to a variable are centering and standardisation.\n\n\nCentering A transformation which re-expresses each value as the distance from a given number (e.g., the mean).\n\n\nStandardising A transformation which re-expresses each value as the distance from the mean in units of standard deviations.\n\n\n\n\n\n\n\n\nMean-centering\nTo Mean-center a variable, we simply subtract the mean from each value, \\(x_i - \\bar{x}\\): \\[\n\\textrm{raw IQ} = \\begin{bmatrix} 71 \\\\ 103 \\\\ 74 \\\\ 108 \\\\ 118 \\\\ 129 \\\\ ... \\end{bmatrix}, \\qquad\n\\textrm{mean centered IQ} = \\begin{bmatrix} 71-99.3 \\\\ 103-99.3 \\\\ 74-99.3 \\\\ 108-99.3 \\\\ 118-99.3 \\\\ 129-99.3 \\\\ ... \\end{bmatrix} = \\begin{bmatrix} -28.3 \\\\ 3.7 \\\\ -25.3 \\\\ 8.7 \\\\ 18.7 \\\\ 29.7 \\\\ ... \\end{bmatrix}\n\\]\n\n\n\nTo mean-center in R, we can simply add a new variable using mutate() and subtract the mean IQ from the IQ variable:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_meancenter\" for which\n# the entries are equal to the \"iq\" variable entries minus the \n# mean of the \"iq\" variable\nwechsler |&gt;\n  mutate(\n    iq_meancenter = iq - mean(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2 iq_meancenter\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n 1 ppt_1          71    27    46    50        -28.3 \n 2 ppt_2         103    38    42    29          3.67\n 3 ppt_3          74    20    50    77        -25.3 \n 4 ppt_4         108    46    50    62          8.67\n 5 ppt_5         118    45    60    29         18.7 \n 6 ppt_6         129    33    45    45         29.7 \n 7 ppt_7         103    49    42    41          3.67\n 8 ppt_8         120    27    63    33         20.7 \n 9 ppt_9          96    37    53    44         -3.33\n10 ppt_10         80    26    53    21        -19.3 \n# ℹ 110 more rows\n\n\n\n\n\n\n\n\nStandardisation\nWhen we standardise a variable, we call the transformed values z-scores. To transform a given value \\(x_i\\) into a z-score, we simply calculate the distance from \\(x_i\\) to the mean, \\(\\bar{x}\\), and divide this by the standard deviation, \\(s\\)\n\\[\nz_i = \\frac{x_i - \\bar{x}}{s}\n\\]\nSo for each of the raw IQ scores, we can transform them to z-scores by subtracting the mean and then dividing by the standard deviation. The resulting values tell us how low/high each participant’s IQ score is compared to observed distribution of scores:\\[\n\\textrm{raw IQ} = \\begin{bmatrix} 71 \\\\ 103 \\\\ 74 \\\\ 108 \\\\ 118 \\\\ 129 \\\\ ... \\end{bmatrix}, \\qquad\n\\textrm{standardised IQ} = \\begin{bmatrix} \\frac{71-99.3}{15.43} \\\\ \\frac{103-99.3}{15.43} \\\\ \\frac{74-99.3}{15.43} \\\\ \\frac{108-99.3}{15.43} \\\\ \\frac{118-99.3}{15.43} \\\\ \\frac{129-99.3}{15.43} \\\\ ... \\end{bmatrix} = \\begin{bmatrix} -1.84 \\\\ 0.238 \\\\ -1.64 \\\\ 0.562 \\\\ 1.21 \\\\ 1.92 \\\\ ... \\end{bmatrix}\n\\]\n\n\n\nWe can achieve this in R either by manually performing the calculation:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_z\" for which\n# the entries are equal to the \"iq\" variable entries minus the mean of the \"iq\"\n# variable, divided by the standard deviation of the \"iq\" variable.  \nwechsler |&gt; \n  mutate(\n    iq_z = (iq - mean(iq)) / sd(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2   iq_z\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 ppt_1          71    27    46    50 -1.84 \n 2 ppt_2         103    38    42    29  0.238\n 3 ppt_3          74    20    50    77 -1.64 \n 4 ppt_4         108    46    50    62  0.562\n 5 ppt_5         118    45    60    29  1.21 \n 6 ppt_6         129    33    45    45  1.92 \n 7 ppt_7         103    49    42    41  0.238\n 8 ppt_8         120    27    63    33  1.34 \n 9 ppt_9          96    37    53    44 -0.216\n10 ppt_10         80    26    53    21 -1.25 \n# ℹ 110 more rows\n\n\nOr we can use the scale() function:\n\n# Take the \"wechsler\" dataframe, and mutate it,\n# such that there is a variable called \"iq_std\" for which\n# the entries are equal to the scaled values of the \"iq\" variable.  \nwechsler |&gt;\n  mutate(\n    iq_std = scale(iq)\n  )\n\n# A tibble: 120 × 6\n   participant    iq   age test1 test2 iq_std[,1]\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 ppt_1          71    27    46    50     -1.84 \n 2 ppt_2         103    38    42    29      0.238\n 3 ppt_3          74    20    50    77     -1.64 \n 4 ppt_4         108    46    50    62      0.562\n 5 ppt_5         118    45    60    29      1.21 \n 6 ppt_6         129    33    45    45      1.92 \n 7 ppt_7         103    49    42    41      0.238\n 8 ppt_8         120    27    63    33      1.34 \n 9 ppt_9          96    37    53    44     -0.216\n10 ppt_10         80    26    53    21     -1.25 \n# ℹ 110 more rows\n\n\nWe can also use the scale() function to mean-center a variable, by setting scale(variable, center = TRUE, scale = FALSE):\n\n# create two new variables in the \"wechsler\" dataframe, one which is \n# mean centered iq, and one which is standardised iq:\nwechsler |&gt;\n  mutate(\n    iq_mc = scale(iq, center = TRUE, scale = FALSE),\n    iq_std = scale(iq, center = TRUE, scale = TRUE) # these are the default settings\n  )\n\n# A tibble: 120 × 7\n   participant    iq   age test1 test2 iq_mc[,1] iq_std[,1]\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 ppt_1          71    27    46    50    -28.3      -1.84 \n 2 ppt_2         103    38    42    29      3.67      0.238\n 3 ppt_3          74    20    50    77    -25.3      -1.64 \n 4 ppt_4         108    46    50    62      8.67      0.562\n 5 ppt_5         118    45    60    29     18.7       1.21 \n 6 ppt_6         129    33    45    45     29.7       1.92 \n 7 ppt_7         103    49    42    41      3.67      0.238\n 8 ppt_8         120    27    63    33     20.7       1.34 \n 9 ppt_9          96    37    53    44     -3.33     -0.216\n10 ppt_10         80    26    53    21    -19.3      -1.25 \n# ℹ 110 more rows\n\n\n\n3.1 Test norms\nMany neuropsychological tests will have norms - parameters which describe the distribution of scores on the test in a normal population. For instance, in a normal adult population, scores on the WAIS have a mean of 100 and a standard deviation of 15.  What this means is that rather than calculating a standardised score against the observed mean of our sample, we might calculate a standardised score against the test norms. In the formula for \\(z\\), we replace our sample mean \\(\\bar{x}\\) with the population mean \\(\\mu\\), and the sample standard deviation \\(s\\) with the population standard deviation \\(\\sigma\\):\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nThe resulting values tell us how low/high each participant’s IQ score is compared to the distribution of IQ scores in the population."
  },
  {
    "objectID": "rd1_05.html#glossary",
    "href": "rd1_05.html#glossary",
    "title": "Types of relationships",
    "section": "\n4 Glossary",
    "text": "4 Glossary\n\n\nFunction: A mapping between two sets of numbers, associating every element of the first set with an elemet in the second.\n\n\nTransformation: Applying a function to a variable to map each value to a transformed value.\n\nLogarithm: The power to which a number must be raised in order to get some other number.\n\nCentering: Transformation which re-expresses each value as the distance from a given number (e.g., the mean).\n\n\nStandardisation: Transformation which re-expresses each value as the distance from the mean in units of standard deviations. \n\n\nscale() To mean center or standardise a variable (depending upon whether center=TRUE/FALSE and scale=TRUE/FALSE)."
  },
  {
    "objectID": "rd1_03.html",
    "href": "rd1_03.html",
    "title": "Numeric data",
    "section": "",
    "text": "In the previous week, we looked into describing and visualising categorical data. We looked at using the mode and median as measures of central tendency, before discussing how for ordered categorical data we could look at the quartiles (the points in rank-ordered data below which falls 25%, 50%, 75% and 100% of the data) to gain an understanding of how spread out the data are.\nWe now move to looking at measures of central tendency and of spread for numeric data."
  },
  {
    "objectID": "rd1_03.html#introduction",
    "href": "rd1_03.html#introduction",
    "title": "Numeric data",
    "section": "",
    "text": "In the previous week, we looked into describing and visualising categorical data. We looked at using the mode and median as measures of central tendency, before discussing how for ordered categorical data we could look at the quartiles (the points in rank-ordered data below which falls 25%, 50%, 75% and 100% of the data) to gain an understanding of how spread out the data are.\nWe now move to looking at measures of central tendency and of spread for numeric data."
  },
  {
    "objectID": "rd1_03.html#central-tendency",
    "href": "rd1_03.html#central-tendency",
    "title": "Numeric data",
    "section": "\n2 Central tendency",
    "text": "2 Central tendency\nIn the following examples, we are going to use some data on 120 participants’ IQ scores (measured on the Wechsler Adult Intelligence Scale (WAIS)), their ages, and their scores on 2 other tests.\nIt is available at https://uoepsy.github.io/data/wechsler.csv\n\nwechsler &lt;- read_csv(\"https://uoepsy.github.io/data/wechsler.csv\")\nsummary(wechsler)\n\n participant              iq              age            test1      \n Length:120         Min.   : 58.00   Min.   :20.00   Min.   :30.00  \n Class :character   1st Qu.: 88.00   1st Qu.:29.50   1st Qu.:45.75  \n Mode  :character   Median :100.50   Median :37.00   Median :49.00  \n                    Mean   : 99.33   Mean   :36.63   Mean   :49.33  \n                    3rd Qu.:109.00   3rd Qu.:44.00   3rd Qu.:53.25  \n                    Max.   :137.00   Max.   :71.00   Max.   :67.00  \n     test2      \n Min.   : 2.00  \n 1st Qu.:42.00  \n Median :52.50  \n Mean   :51.24  \n 3rd Qu.:62.00  \n Max.   :80.00  \n\n\nMode and median revisited\nWe saw for categorical data two different measures of central tendency:\n\n\nMode: The most frequent value (the value that occurs the greatest number of times).\n\n\nMedian: The value for which 50% of observations a lower and 50% are higher. It is the mid-point of the values when they are rank-ordered.\n\nWe applied both of these to categorical data, but we can also use them for numeric data.\n\n\n\n\n\n\n\n\n\nMode\nMedian\nMean\n\n\n\nNominal (unordered categorical)\n✔\n✘\n✘\n\n\nOrdinal (ordered categorical)\n✔\n✔\n\n?(you may see it sometimes for certain types of ordinal data - there’s no consensus)\n\n\nNumeric Continuous\n✔\n✔\n✔\n\n\n\n The mode of numeric variables is not frequently used. Unlike categorical variables where there are a distinct set of possible values which the data can take, for numeric variables, data can take a many more (or infinitely many) different values. Finding the “most common” is sometimes not possible. The most frequent value (the mode) of the iq variable is 97:\n\n# take the \"wechsler\" dataframe |&gt;\n# count() the values in the \"iq\" variable (creates an \"n\" column), and\n# from there, arrange() the data so that the \"n\" column is descending - desc()\nwechsler |&gt;\n    count(iq) |&gt;\n    arrange(desc(n))\n\n# A tibble: 53 × 2\n      iq     n\n   &lt;dbl&gt; &lt;int&gt;\n 1    97     7\n 2   108     6\n 3    92     5\n 4   103     5\n 5   105     5\n 6   110     5\n 7    85     4\n 8    99     4\n 9   107     4\n10   113     4\n# ℹ 43 more rows\n\n\nRecall that the median is found by ordering the data from lowest to highest, and finding the mid-point. In the wechsler dataset we have IQ scores for 120 participants. We find the median by ranking them from lowest to highest IQ, and finding the mid-point between the \\(60^{th}\\) and \\(61^{st}\\) participants’ scores.\nWe can also use the median() function with which we are already familiar:\n\nmedian(wechsler$iq)\n\n[1] 100.5\n\n\nMean\nOne of the most frequently used measures of central tendency for numeric data is the mean.\n\n\n\n\n\n\nMean: \\(\\bar{x}\\)\nThe mean is calculated by summing all of the observations together and then dividing by the total number of obervations (\\(n\\)).\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the mean is:\n\\[\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\]\n\n\n\n\n\n\nHelp reading mathematical formulae\n\n\n\n\n\nThis might be the first mathematical formula you have seen in a while, so let’s unpack it.\nThe \\(\\sum\\) symbol is used to denote a series of additions - a “summation”.\nWhen we include the bits around it: \\(\\sum\\limits_{i = 1}^{n}x_i\\) we are indicating that we add together all the terms \\(x_i\\) for values of \\(i\\) between \\(1\\) and \\(n\\): \\[\\sum\\limits_{i = 1}^{n}x_i \\qquad = \\qquad x_1+x_2+x_3+...+x_n\\]\nSo in order to calculate the mean, we do the summation (adding together) of all the values from the \\(1^{st}\\) to the \\(n^{th}\\) (where \\(n\\) is the total number of values), and we divide that by \\(n\\).\n\n\n\n\n\n\n\n\n\nSamples and populations\n\n\n\n\n\nStatistics is all about drawing inferences from some sampled data about the larger population from which it is sampled.\nA statistic which we calculate from our sample provides us with an estimate of something in the population (for instance, we might take the average age of students at Edinburgh University as an estimate of the age of all students).\nBecause of this, statisticians have different notations for when we are talking about populations vs talking about samples:\n\n\n\n\n\n\n\n\nSample\nPopulation\n\n\n\nNumber of observations\n\\(n\\)\n\\(N\\)\n\n\nMean\n\\(\\bar{x} = \\frac{\\sum\\limits_{i = 1}^{n}x_i}{n}\\)\n\\(\\mu = \\frac{\\sum\\limits_{i = 1}^{N}x_i}{N}\\)\n\n\n\n\n\n\n\n\n\nWe can do the calculation by summing the iq variable, and dividing by the number of observations (in our case we have 120 participants):\n\n# get the values in the \"iq\" variable from the \"wechsler\" dataframe, and\n# sum them all together. Then divide this by 120\nsum(wechsler$iq)/120\n\n[1] 99.33333\n\n\nOr, more easily, we can use the mean() function:\n\nmean(wechsler$iq)\n\n[1] 99.33333\n\n\nSummarising variables\nFunctions such as mean(), median(), min() and max() can quickly summarise data, and we can use them together really easily in combination with summarise().\n\n\n\n\n\n\nsummarise()\nThe summarise() function is used to reduce variables down to a single summary value.\n\n# take the data |&gt;\n# summarise() it, such that there is a value called \"summary_value\", which\n# is the sum() of \"variable1\" column, and a value called \n# \"summary_value2\" which is the mean() of the \"variable2\" column.\ndata |&gt;\n  summarise(\n    summary_value = sum(variable1),\n    summary_value2 = mean(variable2)\n  )\n\nNote: Just like with mutate() we don’t have to keep using the dollar sign $, as we have already told it what dataframe to look for the variables in.\n\n\n\nSo if we want to show the mean IQ score and the mean age of our participants:\n\n# take the \"wechsler\" dataframe |&gt;\n# summarise() it, such that there is a value called \"mean_iq\", which\n# is the mean() of the \"iq\" variable, and a value called \n# \"mean_age\" which is the mean() of the \"age\" variable. \nwechsler |&gt;\n    summarise(\n        mean_iq = mean(iq),\n        mean_age = mean(age)\n    )\n\n# A tibble: 1 × 2\n  mean_iq mean_age\n    &lt;dbl&gt;    &lt;dbl&gt;\n1    99.3     36.6"
  },
  {
    "objectID": "rd1_03.html#spread",
    "href": "rd1_03.html#spread",
    "title": "Numeric data",
    "section": "\n3 Spread",
    "text": "3 Spread\nInterquartile range\nIf we are using the median as our measure of central tendency and we want to discuss how spread out the spread are around it, then we will want to use quartiles (recall that these are linked: the \\(2^{nd}\\) quartile = the median).\nWe have already briefly introduced how for ordinal data, the 1st and 3rd quartiles give us information about how spread out the data are across the possible response categories. For numeric data, we can likewise find the 1st and 3rd quartiles in the same way - we rank-order all the data, and find the point at which 25% and 75% of the data falls below.\nThe difference between the 1st and 3rd quartiles is known as the interquartile range (IQR).( Note, we couldn’t take the difference for ordinal data, because “difference” would not be quantifiable - the categories are ordered, but intervals are between categories are unknown)\nIn R, we can find the IQR as follows:\n\nIQR(wechsler$age)\n\n[1] 14.5\n\n\nAlternatively, we can use this inside summarise():\n\n# take the \"wechsler\" dataframe |&gt;\n# summarise() it, such that there is a value called \"median_age\", which\n# is the median() of the \"age\" variable, and a value called \"iqr_age\", which\n# is the IQR() of the \"age\" variable.\nwechsler |&gt; \n  summarise(\n    median_age = median(age),\n    iqr_age = IQR(age)\n  )\n\n# A tibble: 1 × 2\n  median_age iqr_age\n       &lt;dbl&gt;   &lt;dbl&gt;\n1         37    14.5\n\n\nVariance\nIf we are using the mean as our as our measure of central tendency, we can think of the spread of the data in terms of the deviations (distances from each value to the mean).\nRecall that the mean is denoted by \\(\\bar{x}\\). If we use \\(x_i\\) to denote the \\(i^{th}\\) value of \\(x\\), then we can denote deviation for \\(x_i\\) as \\(x_i - \\bar{x}\\).\nThe deviations can be visualised by the red lines in Figure @ref(fig:deviations).\n\n\n\n\nDeviations from the mean\n\n\n\n\n\n\n\n\n\nThe sum of the deviations from the mean, \\(x_i - \\bar x\\), is always zero\n\\[\n\\sum\\limits_{i = 1}^{n} (x_i - \\bar{x}) = 0\n\\]\nThe mean is like a center of gravity - the sum of the positive deviations (where \\(x_i &gt; \\bar{x}\\)) is equal to the sum of the negative deviations (where \\(x_i &lt; \\bar{x}\\)).\n\n\n\nBecause deviations around the mean always sum to zero, in order to express how spread out the data are around the mean, we must we consider squared deviations.\nSquaring the deviations makes them all positive. Observations far away from the mean in either direction will have large, positive squared deviations. The average squared deviation is known as the variance, and denoted by \\(s^2\\)\n\n\n\n\n\n\nVariance: \\(s^2\\)\nThe variance is calculated as the average of the squared deviations from the mean.\nWhen we have sampled some data, we denote the mean of our sample with the symbol \\(\\bar{x}\\) (sometimes referred to as “x bar”). The equation for the variance is:\n\\[s^2 = \\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nWhy n minus 1?\n\n\n\n\n\nThe top part of the equation \\(\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2\\) can be expressed in \\(n-1\\) terms, so we divide by \\(n-1\\) to get the average.Example: If we only have two observations \\(x_1\\) and \\(x_2\\), then we can write out the formula for variance in full quite easily. The top part of the equation would be: \\[\n\\sum\\limits_{i=1}^{2}(x_i - \\bar{x})^2 \\qquad = \\qquad (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2\n\\]\nThe mean for only two observations can be expressed as \\(\\bar{x} = \\frac{x_1 + x_2}{2}\\), so we can substitute this in to the formula above. \\[\n(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 \\qquad = \\qquad \\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2\n\\] Which simplifies down to one value: \\[\n\\left(x_1 - \\frac{x_1 + x_2}{2}\\right)^2 + \\left(x_2 - \\frac{x_1 + x_2}{2}\\right)^2 \\qquad = \\qquad  \\left(\\frac{x_1 - x_2}{\\sqrt{2}}\\right)^2\n\\]  So although we have \\(n=2\\) datapoints (\\(x_1\\) and \\(x_2\\)), the top part of the equation for the variance has only 1 (\\(n-1\\)) units of information. In order to take the average of these bits of information, we divide by \\(n-1\\).\n\n\n\n\n\n\nWe can get R to calculate this for us using the var() function:\n\nwechsler |&gt;\n  summarise(\n    variance_iq = var(iq)\n  )\n\n# A tibble: 1 × 1\n  variance_iq\n        &lt;dbl&gt;\n1        238.\n\n\nStandard deviation\nOne difficulty in interpreting variance as a measure of spread is that it is in units of squared deviations. It relects the typical squared distance from a value to the mean.\nConveniently, by taking the square root of the variance, we can translate the measure back into the units of our original variable. This is known as the standard deviation.\n\n\n\n\n\n\nStandard Deviation: \\(s\\)\nThe standard deviation, denoted by \\(s\\), is a rough estimate of the typical distance from a value to the mean.\nIt is the square root of the variance (the typical squared distance from a value to the mean).\n\\[\ns = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}}\n\\]\n\n\n\nWe can get R to calculate the standard deviation of a variable sd() function:\n\nwechsler |&gt;\n  summarise(\n    variance_iq = var(iq),\n    sd_iq = sd(iq)\n  )\n\n# A tibble: 1 × 2\n  variance_iq sd_iq\n        &lt;dbl&gt; &lt;dbl&gt;\n1        238.  15.4"
  },
  {
    "objectID": "rd1_03.html#visualisations",
    "href": "rd1_03.html#visualisations",
    "title": "Numeric data",
    "section": "\n4 Visualisations",
    "text": "4 Visualisations\nBoxplots\nBoxplots provide a useful way of visualising the interquartile range (IQR). You can see what each part of the boxplot represents in Figure @ref(fig:boxplotdesc).\n\n\n\n\nAnatomy of a boxplot\n\n\n\nWe can create a boxplot of our age variable using the following code:\n\n# Notice, we put age on the x axis, making the box plot vertical. \n# If we had set aes(y = age) instead, then it would simply be rotated 90 degrees \nggplot(data = wechsler, aes(x = age)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nHistograms\nNow that we have learned about the different measures of central tendency and of spread, we can look at how these influence visualisations of numeric variables.\nWe can visualise numeric data using a histogram, which shows the frequency of values which fall within bins of an equal width.\n\n# make a ggplot with the \"wechsler\" data. \n# on the x axis put the possible values in the \"iq\" variable,\n# add a histogram geom (will add bars representing the count \n# in each bin of the variable on the x-axis)\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram()\n\n\n\n\n\n\n\nWe can specifiy the width of the bins:\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\nLet’s take a look at the means and standard deviations of participants’ scores on the other tests (the test1 and test2 variables):\n\nwechsler |&gt; \n  summarise(\n    mean_test1 = mean(test1),\n    sd_test1 = sd(test1),\n    mean_test2 = mean(test2),\n    sd_test2 = sd(test2)\n  )\n\n# A tibble: 1 × 4\n  mean_test1 sd_test1 mean_test2 sd_test2\n       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1       49.3     7.15       51.2     14.4\n\n\nTests 1 and 2 have similar means (around 50), but the standard deviation of Test 2 is almost double that of Test 1. We can see this distinction in the visualisation below - the histograms are centered at around the same point (50), but the one for Test 2 is a lot wider than that for Test 1.\n\n\n\n\n\n\n\n\nDensity curves\nIn addition to grouping numeric data into bins in order to produce a histogram, we can also visualise a density curve.\nFor the time being, you can think of the density as a bit similar to the notion of relative frequency, in that for a density curve, the values on the y-axis are scaled so that the total area under the curve is equal to 1. Because there are infinitely many values that numeric variables could take (e.g., 50, 50.1, 50.01, 5.001, …), we could group the data into infinitely many bins. In creating a curve for which the total area underneath is equal to one, we can use the area under the curve in a range of values to indicate the proportion of values in that range.\n\nggplot(data = wechsler, aes(x = iq)) + \n  geom_density()+\n  xlim(50,150)"
  },
  {
    "objectID": "rd1_03.html#skew",
    "href": "rd1_03.html#skew",
    "title": "Numeric data",
    "section": "\n5 Skew",
    "text": "5 Skew\nSkewness is a measure of asymmetry in a distribution. Distributions can be positively skewed or negatively skewed, and this influences our measures of central tendency and of spread to different degrees (see Figure @ref(fig:skewplot)).\n\n\n\n\nSkew influences the mean and median to different degrees."
  },
  {
    "objectID": "rd1_03.html#glossary",
    "href": "rd1_03.html#glossary",
    "title": "Numeric data",
    "section": "\n6 Glossary",
    "text": "6 Glossary\n\n\nInterquartile Range (IQR): The \\(3^{rd}\\) quartile minus the \\(1^{st}\\) quartile.\n\nMean: The sum of all observations divided by the total number of observations. The center of gravity of a variable.\n\nDeviation: The distance from an observation to the mean value.\n\nVariance: The average squared distance from observations to the mean value.\n\nStandard deviation: Square root of variance - can be thought of as the average distance from observations to the mean value.\n\nBoxplot: Displays the median and the IQR, and any extreme values.\n\n\nHistogram: Shows the frequency of values which fall within bins of an equal width.\n\nDensity curve: A curve for reflecting the distribution of a variable, for which the area under the curve sums to 1.\n\nSkew: A measure of asymmetry in a distribution. \n\n\nsummarise() To summarise variables into a single value according to whatever calculation we give it.\n\nIQR() To calculate the interquartile range for a given variable.\n\nmean() To calculate the mean of a given variable.\n\nsd() To calculate the standard deviation of a given variable.\n\nvar() To calculate the variance of a given variable.\n\ngeom_boxplot() To add a boxplot to a ggplot.\n\ngeom_histogram() To add a histogram to a ggplot.\n\ngeom_density() To add a density curve to a ggplot.\n\ngeom_vline() To add a vertical line to a ggplot."
  },
  {
    "objectID": "rd1_01.html",
    "href": "rd1_01.html",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Recommended option: access it via the RStudio Server Online:\n\nGo to the course LEARN page.\nClick “Quick Links”\nClick “RStudio Server Online (by Noteable)”\nSelect “RStudio” from the dropdown menu\nClick “Start”\n\nAlternative: install it locally on your PC following these instructions.\n\n\nCreate a new folder on the server. Give it a useful name like the name of the course: DAPR1. (In the picture we have used “USMR” but please use “DAPR1”).\n\n\n\nIn the top right, click Project &gt; New Project\n\n\n\nClick “Existing Directory”\n\n\n\nClick “Browse” and then choose the folder you just created. Click “Choose” and then click “Create Project”.\n\n\n\nYou should now be able to tell that you have the project open, because it shows you in the top right.\n\n\nYou’re ready to go!"
  },
  {
    "objectID": "rd1_01.html#getting-r-and-rstudio",
    "href": "rd1_01.html#getting-r-and-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Recommended option: access it via the RStudio Server Online:\n\nGo to the course LEARN page.\nClick “Quick Links”\nClick “RStudio Server Online (by Noteable)”\nSelect “RStudio” from the dropdown menu\nClick “Start”\n\nAlternative: install it locally on your PC following these instructions.\n\n\nCreate a new folder on the server. Give it a useful name like the name of the course: DAPR1. (In the picture we have used “USMR” but please use “DAPR1”).\n\n\n\nIn the top right, click Project &gt; New Project\n\n\n\nClick “Existing Directory”\n\n\n\nClick “Browse” and then choose the folder you just created. Click “Choose” and then click “Create Project”.\n\n\n\nYou should now be able to tell that you have the project open, because it shows you in the top right.\n\n\nYou’re ready to go!"
  },
  {
    "objectID": "rd1_01.html#a-first-look-at-rstudio",
    "href": "rd1_01.html#a-first-look-at-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "\n2 A first look at RStudio",
    "text": "2 A first look at RStudio\nOkay, now you should have RStudio and a project open, and you should see something which looks more or less like the image below, where there are several little windows.\n\nWe are going to explore what each of these little windows offer by just diving in and starting to do things.\n\n2.1 R as a calculator\nStarting in the left-hand window, you’ll notice the blue sign &gt; which is where we R code gets executed.\nType 2+2, and hit Enter ↵. You should discover that R is a calculator.\nLet’s work through some of the basic operations (adding, subtracting, etc). Try these commands yourself:\n\n2 + 5\n10 - 4\n2 * 5\n10 - (2 * 5)\n(10 - 2) * 5\n10 / 2\n\n3^2 (Hint, interpret the ^ symbol as “to the power of”)\n\n\n\n\n\n\n\nWhenever you see the blue sign &gt;, it means R is ready and waiting for you to provide a command.\nIf you type 10 + and press Enter, you’ll see that instead of a blue &gt; you are left with a blue +. This means that R is waiting for more. Either give it more, or cancel the command by pressing the escape key on your keyboard.\n\n\n\nAs well as performing calculations, we can ask R things, such as “Is 3 less than 5?”:\n\n3 &lt; 5\n\n[1] TRUE\n\n\nAs the computation above returns TRUE, we notice that such questions return either TRUE or FALSE. These are not numbers and are called logical values.\nTry the following:\n\n\n3 &gt; 5 “is 3 greater than 5?”\n\n3 &lt;= 5 “is 3 less than or equal to 5?”\n\n3 &gt;= 3 “is 3 greater than or equal to 3?”\n\n3 == 5 “is 3 equal to 5?”\n\n(2 * 5) == 10 “is 2 times 5 equal to 10?”\n\n(2 * 5) != 11 “is 2 times 5 NOT equal to 11?”\n\n2.2 R as a calculator with a memory\nWe can also store things in R’s memory, and to do that we just need to give them a name. Type x &lt;- 5 and press Enter.\nWhat has happened? We’ve just stored something named x which has the value 5. We can now refer to the name and it will give us the value! Try typing x and hitting Enter. It should give you the number 5. What about x * 3?\n\n\n2.2.1 Storing things in R\nThe &lt;- symbol, pronounced arrow, is used to assign a value to a named object:\n[name] &lt;- [value]\nNote, there are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the &lt;- don’t matter):\n\n\nlucky_number &lt;- 5 ✔   lucky number &lt;- 5 ❌\n\n\n\nNames must start with a letter:\n\n\nlucky_number &lt;- 5 ✔   1lucky_number &lt;- 5 ❌\n\n\n\nCase sensitive:\n\n\nlucky_number is different from Lucky_Number\n\n\n\nReserved words - there is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function\n(Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\nYou might have noticed that something else happened when you executed the code x &lt;- 5. The thing we named x with a value of 5 suddenly appeared in the top-right window. This is known as the environment, and it shows everything that we store things in R:\n\nWe’ve now used a couple of the windows - we’ve been executing R code in the console, and learned about how we can store things in R’s memory (the environment) by assigning a name to them:\n\nNotice that in the screenshot above, we have moved the console down to the bottom-left, and introduced a new window above it. This is the one that we’re going to talk about next.\n\n3 R scripts and Rmarkdown\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWell, we can write and edit our code in a separate place before sending it to the console to be executed!!\n\n3.1 R scripts\n\n\n\n\n\n\nTask\n\n\n\n\nOpen an R script\n\nFile &gt; New File &gt; R script\n\n\nCopy and paste the following into the R script\n\n\nx &lt;- 210\ny &lt;- 15\nx / y\n\n\nPosition your text-cursor (blinking vertical line) on the top line and press:\n\nCtrl + Enter on Windows\nCmd + Enter on macOS\n\n\n\n\n\nNotice what has happened - it has sent the command x &lt;- 210 to the console, where it has been executed, and x is now in your environment. Additionally, it has moved the text-cursor to the next line.\n\n\n\n\n\n\n\nTask\n\n\n\nPress Ctrl + Enter (Windows) or Cmd + Enter (macOS) again. Do it twice (this will run the next two lines).\nThen, change x to some other number in your R script, and run the lines again (starting at the top).\n\n\n\n\n\n\n\n\nTask\n\n\n\nAdd the following line to your R script and execute it (send it to the console pressing Ctrl/Cmd + Enter):\n\nplot(1,5)\n\n\n\nA very basic plot should have appeared in the bottom-right of RStudio. The bottom-right window actually does some other useful things.\n\n\n\n\n\n\nTask\n\n\n\n\nSave the R script you have been working with:\n\nFile &gt; Save\ngive it an appropriate name, and click save.\n\n\nCheck that you can now see that file in the file pane, by clicking on the “Files” tab of the bottom-right window.\n\n\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n3.2 Rmarkdown\n\n\n\n\nArtwork by @allison_horst\n\n\n\nIn addition to R scripts, there is another type of document we can create, known as an “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\nWe’re going to use Rmarkdown documents throughout this course. We’ll get into it how to write them lower down, but it basically involves writing normal text interspersed with “code-chunks” (i.e., chunks of code!). In the example below, you can see the grey boxes indicating the R code, with text in between. We can then compile the document into either a .pdf or a .html file.\n\n\n4 Recap\nOkay, so we’ve now seen all of the different windows in RStudio in action:\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code and Rmarkdown documents. You can then send this to the console for it to be executed.\nThe bottom-right window shows you the plots that you create, the files in your project, and some other things (we’ll get to these later).\n\n\n\n5 Take a breather\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to learning about how we store data in R.\n\n\n\n\n\n\nUseful Settings 1: Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools &gt; Global Options…\n\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n\n\n\n\n\n\n\nUseful Settings 2: Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx &lt;- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools &gt; Global Options…\n\nIn the left menu of the box, click “Code”\n\nTick the box for “Soft-wrap R source files”\n\n\n\n\n\n6 R Packages\n\n6.1 Installing R packages\nAlongside the basic installation of R and RStudio, there are many add-on packages which the R community create and maintain.\nThe thousands of packages are part of what makes R such a powerful and useful tool - there is a package for almost everything you could want to do in R.\n\n\n\n\n\n\nTask\n\n\n\nIn the console, type install.packages(\"cowsay\") and hit Enter.\nLots of red text will come up, and it will take a bit of time.\nWhen it has finished, and R is ready for you to use again, you will see the blue sign &gt;.\n\n\n\n6.2 Using R packages\nIt’s not enough just to install a package - to actually use the package, we need to load it using library().\nWe install a package only once. But each time we open RStudio, we have to load the packages we need.\n\n\n\n\n\n\n\nTask\n\n\n\nIn the console again, type library(cowsay) and hit enter. This loads the package for us to use it.\nThen, type say(\"hello world\", by = \"cow\") and hit enter.\nHopefully you got a similar result to ours:\n\nlibrary(cowsay)\nsay(\"Hi Folks!\", by = \"cow\")\n\n\n ___________ \n&lt; Hi Folks! &gt;\n ----------- \n      \\\n       \\\n\n        ^__^ \n        (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||\n\n\n\n\n\n7 Your first .Rmd file\nIn order to be able to write and compile Rmarkdown documents (and do a whole load of other things which we are going to need throughout the course) we are now going to install a set of packages known collectively as the “tidyverse” (this includes the “rmarkdown” package).\n\n\nIf you installed R/Rstudio on your own computer, then in the console, type install.packages(\"tidyverse\") and hit Enter. You may have to wait a while.\n\nIf you are using rstudio.ppls.ed.ac.uk, then we have already installed “tidyverse” and a few other useful packages for you, so you don’t have to do anything!\n\n\n\n\n\n\n\nTask\n\n\n\nOpen a new Rmarkdown document: File &gt; New File &gt; R Markdown…\nWhen the box pops-up, give a title of your choice (“Intro lab”, maybe?) and your name as the author.\n\n\n\n7.1 Writing code in a .Rmd file\nThe file which you have just created will have some template stuff in it. Delete everything below the first code chunk to start with a fresh document:\n\n\n\n\n\n\n\nTask\n\n\n\nInsert a new code chunk by either using the Insert button in the top right of the document and selecting R, or by typing Ctrl + Alt + i on Windows or Option + Cmd + i on MacOS.\nInside the chunk, type:\nprint(\"Hello world! My name is ?\")\nTo execute the code inside the chunk, you can either:\n\ndo as you did in the R script - put the text-cursor on the first line, and hit Ctrl/Cmd + Enter to run the lines sequentially;\nclick the little green arrow at the top right of your code-chunk to run all of the code inside the chunk;\nwhile your cursor is inside the code chunk, press Cmd + Shift + Enter to run all of the code inside the chunk.\n\nYou can see that the output gets printed below.\n\n\n\nWe’re going to use some functions which are in the tidyverse package, which we already installed above (or which we installed for you on the server).\nTo use the package, we need to load it.\nWhen writing analysis code, we want it to be reproducible - we want to be able to give somebody else our code and the data, and ensure that they can get the same results. To do this, we need to show what packages we use.\nIt is good practice to load any packages you use at the top of your code, so that users of your code will know what packages they will need to install to run your code.\nIn your first code chunk, type:\n\n# I'm going to use these packages in this document:\nlibrary(tidyverse)\n\nand run the chunk.\nNOTE: You might get various messages popping up below when you run this chunk, that is fine.\nComments in code\nNote that using # in R code makes that line a comment, which basically means that R will ignore the line. Comments are useful for you to remind yourself of what your code is doing.\n\n7.2 Writing text in a .Rmd file\nPlace your cursor outside the code chunk, and below the code chunk add a new line with the following:\n# R code examples\nNote that when the # is used in a Rmarkdown file outside of a code-chunk, it will make that line a heading when we finally get to compiling the document. Below, what you see on the left will be compiled to look like those on the right:\n\n\n7.2.1 RECALL:\n\n\nInside a code-chunk, one or more #s will create a comment\n\n\nOutside a code-chunk, one ore more #s will create headings\n\nIn your Rmarkdown document, choose a few of the symbols below, and write an explanation of what it does, giving an example in a code chunk. You can see an example of the first few below.\n\n+\n-\n*\n/\n()\n^\n&lt;-\n&lt;\n&gt;\n&lt;=\n&gt;=\n==\n!=\n\n\n\n7.3 Storing data into R\nWe’ve already seen how to assign a value to a name/symbol using &lt;-. However, we’ve only seen how to assign a single number, e.g, x &lt;- 5.\nTo store a sequence of numbers into R, we combine the values using the combine function c() and give the sequence a name. A sequence of elements all of the same type is called a vector. To view the stored content, simply type the name of the vector.\n\nmyfirstvector &lt;- c(1, 5, 3, 7)\nmyfirstvector\n\n[1] 1 5 3 7\n\n\nWe can perform arithmetic operations on each value of the vector. For example, to add five to each entry:\n\nmyfirstvector + 5\n\n[1]  6 10  8 12\n\n\nRecall that vectors are sequences of elements all of the same type. They do not have to be always numbers; they could be words such as real or fictional animals. Words need to be written inside quotations, e.g. “anything”, and instead of being of numeric type, we say they are characters.\n\nwordsvector &lt;- c(\"cat\", \"dog\", \"parrot\", \"peppapig\")\nwordsvector\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nNOTE\nYou can use either double-quote or single-quote:\n\nc(\"cat\", \"dog\", \"parrot\", \"peppapig\")\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\nc('cat', 'dog', 'parrot', 'peppapig')\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nThe function class() will tell you the type of the object. In this case, it is a character vector.\n\nclass(wordsvector)\n\n[1] \"character\"\n\n\nIt does not make sense to add a number to words, hence some operations like addition and multiplication are only defined on vectors of numeric type. If you make a mistake, R will warn you with a red error message.\n\nwordsvector + 5\n\n\nError in wordsvector + 5 : non-numeric argument to binary operator\n\nFinally, it is important to notice that if you combine together in a vector a number and a word, R will transform all elements to be of the same type. Why? Recall: vectors are sequences of elements all of the same type. Typically, R chooses the most general type between the two. In this particular case, it would make everything a character, check the ““, as it would be harder to transform a word into a number!\n\nmysecondvector &lt;- c(4, \"cat\")\nmysecondvector\n\n[1] \"4\"   \"cat\"\n\n\n\n7.4 Reading data into R\nWhile we can manually input data like we did above, more often, we will need to read in data which has been created elsewhere (like in excel, or by some software which is used to present participants with experiments).\n\n\n\n\n\n\nTask\n\n\n\nAdd a new heading by typing the following:\n# Reading and storing data\nRemember: We make headings using the # outside of a code chunk.\n\n\n\n\n\n\n\n\nTask\n\n\n\nOpen Microsoft Excel, or LibreOffice Calc, or whatever spreadsheet software you have available to you, and create some data with more than one variable.\nIt can be whatever you want, but we’ve used a very small example here for you to follow, so feel free to use it if you like.\nWe’ve got two sets of values here: the names and the birth-years of each member of the Beatles. The easiest way to think of this would be to have a row for each Beatle, and a column for each of name and birth-year.\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSave the data as a .csv file.\nAlthough R can read data when it’s saved in Microsoft/LibreOffice formats, the simplest, and most universal way to save data is as simple text, with the values separated by some character - .csv stands for comma separated values.\nIn Microsoft Excel, if you go to File &gt; Save as\nIn the Save as Type box, choose to save the file as CSV (Comma delimited).\nImportant: save your data in the project folder you created at the start of this lab.\n\n\nBack in RStudio…\nNext, we’re going to read the data into R. We can do this by using the read_csv() function, and directing it to the file you just saved.\nIf you are using RStudio on the server, you will need to upload the file you just saved to the server. The video below shows an example of this:\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nCreate a new code-chunk in your Rmarkdown and, in the chunk, type: read_csv(\"name-of-your-data.csv\"), where you replace name-of-your-data with whatever you just saved your data as in your spreadsheet software.\n\n\nHelpful tip\nIf you have your text-cursor inside the quotation marks, and press the tab key on your keyboard, it will show you the files inside your project. You can then use the arrow keys to choose between them and press Enter to add the code:\n\n\n\n\n\n\nWhen you run the line of code you just wrote, it will print out the data, but will not store it. To do that, we need to assign it as something:\n\nbeatles &lt;- read_csv(\"data_from_excel.csv\")\n\nNote that this will now turn up in the Environment pane of RStudio.\nNow that we’ve got our data in R, we can print it out by simply invoking its name:\n\nbeatles\n\n# A tibble: 4 × 2\n  name   birth_year\n  &lt;chr&gt;       &lt;dbl&gt;\n1 John         1940\n2 Paul         1942\n3 George       1943\n4 Ringo        1940\n\n\nAnd we can do things such as ask R how many rows and columns there are:\n\ndim(beatles)\n\n[1] 4 2\n\n\nThis says that there are 4 members of the Beatles, and for each we have 2 measurements.\nTo get more insight into what the data actually are, you can either use the structure str() function, or glimpse() function to get a glimpse at the data:\n\nstr(beatles)\n\nspc_tbl_ [4 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ name      : chr [1:4] \"John\" \"Paul\" \"George\" \"Ringo\"\n $ birth_year: num [1:4] 1940 1942 1943 1940\n - attr(*, \"spec\")=\n  .. cols(\n  ..   name = col_character(),\n  ..   birth_year = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(beatles)\n\nRows: 4\nColumns: 2\n$ name       &lt;chr&gt; \"John\", \"Paul\", \"George\", \"Ringo\"\n$ birth_year &lt;dbl&gt; 1940, 1942, 1943, 1940\n\n\n\n\n\n\n\n\nTask\n\n\n\nUse dim() to confirm how many rows and columns are in your data.\nUse str() or glimpse() to take a look at the structure of the data. Don’t worry about the output of str() right now, we’ll pick up with this in the next chapter.\n\n\n\n7.5 Getting help in R\ndim(), str(), read_csv() are all functions.\nFunctions perform specific operations / transformations in computer programming.\nThey can have inputs and outputs. For example, dim() takes some data you have stored in R as its input, and gives the dimensions of the data as its output.\nIn R, functions come with help pages, where you can see information about the various inputs and outputs, and examples of how to use them.\nIn the console, type ?dim (or ?dim() will work too) and press Enter.\nThe bottom-right pane (where things like plots are also shown), should switch to the Help tab, and open the documentation page for the dim() function!\n\n\n\n\n\n\nWhy did we ask you to write this bit in the console, whereas previously we’ve been writing stuff in the RMarkdown document in the editor?\nWell, when writing an RMarkdown document, the aim at the end is to have a nice document which we can read. For instance, we can write statistical reports, journal papers, coursework reports etc, in Rmarkdown. But the reader doesn’t need to see that we’re looking up how to use some function - just like they don’t need to know that we might look up a word in the dictionary before using it.\n\n\n\n\n7.6 Compiling a .Rmd file\n\n\n\n\n\n\nTask\n\n\n\nBy now, you should have an Rmardkown document (.Rmd) with your answers to the tasks we’ve been through today.\nCompile the document by clicking on the Knit button at the top (it will ask you to save your document first). The little arrow to the right of the Knit button allows you to compile to either .pdf or .html.\n\n\n\n8 Checklist for today\n\n\nEITHER:\n\nOption A: Get started with the PPLS RStudio Server   ✔\nOption B: Install R and RStudio   ✔\n\n\nStart a new project for the course   ✔\nChange a few RStudio settings (recommended)   ✔\nInstall some R packages (the “tidyverse”)   ✔\nCreate a new Rmarkdown document   ✔\nComplete today’s tasks and exercises on storing data in R   ✔\nCompile your Rmarkdown document   ✔\nCelebrate!   ✔ 🎉\n\n9 Glossary\n\nConsole: where the code gets executed\nEnvironment: R’s memory, it lists all the names of things with stuff stored into them\nEditor: where we edit code\nR script: a file with R code and comments\nRmarkdown document: an enhanced file where you can combine together R code, explanatory text, and plots.\npackages (also library): user-created bundles providing additional functionality to your local R installation\nfunctions: they take inputs, do some transformation or computation on them, and return a result (output)\n?: returns the help page of a function, e.g. ?dim.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nExample\n\n\n\n+\nAdds two numbers together\n\n2+2 - two plus two\n\n\n-\nSubtract one number from another\n\n3-1 - three minus one\n\n\n*\nMultiply two numbers together\n\n3*3 - three times three\n\n\n/\nDivide one number by another\n\n9/3 - nine divided by three\n\n\n()\ngroup operations together\n\n(2+2)/4 is different from 2+2/4\n\n\n\n^\nto the power of..\n\n4^2 - four to the power of two, or four squared\n\n\n&lt;-\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx &lt;- 10\n\n\n=\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx = 10\n\n\n&lt;\nis less than?\n2 &lt; 3\n\n\n&gt;\nis greater than?\n2 &gt; 3\n\n\n&lt;=\nis less than or equal to?\n2 &lt;= 3\n\n\n&gt;=\nis greater than or equal to?\n2 &gt;= 2\n\n\n==\nis equal to?\n(5+5) == 10\n\n\n!=\nis not equal to?\n(2+3) != 4\n\n\nc()\ncombines values into a vector (a sequence of values)\nc(1,2,3,4)"
  },
  {
    "objectID": "rd1_01.html#r-scripts-and-rmarkdown",
    "href": "rd1_01.html#r-scripts-and-rmarkdown",
    "title": "Getting started with R and RStudio",
    "section": "\n3 R scripts and Rmarkdown",
    "text": "3 R scripts and Rmarkdown\nWhat if we want to edit our code? Whatever we write in the console just disappears upwards. What if we want to change things we did earlier on?\nWell, we can write and edit our code in a separate place before sending it to the console to be executed!!\n\n3.1 R scripts\n\n\n\n\n\n\nTask\n\n\n\n\nOpen an R script\n\nFile &gt; New File &gt; R script\n\n\nCopy and paste the following into the R script\n\n\nx &lt;- 210\ny &lt;- 15\nx / y\n\n\nPosition your text-cursor (blinking vertical line) on the top line and press:\n\nCtrl + Enter on Windows\nCmd + Enter on macOS\n\n\n\n\n\nNotice what has happened - it has sent the command x &lt;- 210 to the console, where it has been executed, and x is now in your environment. Additionally, it has moved the text-cursor to the next line.\n\n\n\n\n\n\n\nTask\n\n\n\nPress Ctrl + Enter (Windows) or Cmd + Enter (macOS) again. Do it twice (this will run the next two lines).\nThen, change x to some other number in your R script, and run the lines again (starting at the top).\n\n\n\n\n\n\n\n\nTask\n\n\n\nAdd the following line to your R script and execute it (send it to the console pressing Ctrl/Cmd + Enter):\n\nplot(1,5)\n\n\n\nA very basic plot should have appeared in the bottom-right of RStudio. The bottom-right window actually does some other useful things.\n\n\n\n\n\n\nTask\n\n\n\n\nSave the R script you have been working with:\n\nFile &gt; Save\ngive it an appropriate name, and click save.\n\n\nCheck that you can now see that file in the file pane, by clicking on the “Files” tab of the bottom-right window.\n\n\n\nNOTE: When you save R script files, they terminate with a .R extension.\n\n3.2 Rmarkdown\n\n\n\n\nArtwork by @allison_horst\n\n\n\nIn addition to R scripts, there is another type of document we can create, known as an “Rmarkdown”.\nRmarkdown documents combine the analytical power of R and the utility of a text-processor. We can have one document which contains all of our analysis as well as our written text, and can be compiled into a nicely formatted report. This saves us doing analysis in R and copying results across to Microsoft Word. It ensures our report accurately reflects our analysis. Everything that you’re reading now has all been written in Rmarkdown!\nWe’re going to use Rmarkdown documents throughout this course. We’ll get into it how to write them lower down, but it basically involves writing normal text interspersed with “code-chunks” (i.e., chunks of code!). In the example below, you can see the grey boxes indicating the R code, with text in between. We can then compile the document into either a .pdf or a .html file."
  },
  {
    "objectID": "rd1_01.html#recap",
    "href": "rd1_01.html#recap",
    "title": "Getting started with R and RStudio",
    "section": "\n4 Recap",
    "text": "4 Recap\nOkay, so we’ve now seen all of the different windows in RStudio in action:\n\nThe console is where R code gets executed\nThe environment is R’s memory, you can assign something a name and store it here, and then refer to it by name in your code.\nThe editor is where you can write and edit R code and Rmarkdown documents. You can then send this to the console for it to be executed.\nThe bottom-right window shows you the plots that you create, the files in your project, and some other things (we’ll get to these later)."
  },
  {
    "objectID": "rd1_01.html#take-a-breather",
    "href": "rd1_01.html#take-a-breather",
    "title": "Getting started with R and RStudio",
    "section": "\n5 Take a breather",
    "text": "5 Take a breather\nBelow are a couple of our recommended settings for you to change as you begin your journey in R. After you’ve changed them, take a 5 minute break before moving on to learning about how we store data in R.\n\n\n\n\n\n\nUseful Settings 1: Clean environments\nAs you use R more, you will store lots of things with different names. Throughout this course alone, you’ll probably name hundreds of different things. This could quickly get messy within our project.\nWe can make it so that we have a clean environment each time you open RStudio. This will be really handy.\n\nIn the top menu, click Tools &gt; Global Options…\n\nThen, untick the box for “Restore .RData into workspace at startup”, and change “Save workspace to .RData on exit” to Never:\n\n\n\n\n\n\n\n\n\n\n\nUseful Settings 2: Wrapping code\nIn the editor, you might end up with a line of code which is really long, but you can make RStudio ‘wrap’ the line, so that you can see it all, without having to scroll:\n\nx &lt;- 1+2+3+6+3+45+8467+356+8565+34+34+657+6756+456+456+54+3+78+3+3476+8+4+67+456+567+3+34575+45+2+6+9+5+6\n\n\nIn the top menu, click Tools &gt; Global Options…\n\nIn the left menu of the box, click “Code”\n\nTick the box for “Soft-wrap R source files”"
  },
  {
    "objectID": "rd1_01.html#r-packages",
    "href": "rd1_01.html#r-packages",
    "title": "Getting started with R and RStudio",
    "section": "\n6 R Packages",
    "text": "6 R Packages\n\n6.1 Installing R packages\nAlongside the basic installation of R and RStudio, there are many add-on packages which the R community create and maintain.\nThe thousands of packages are part of what makes R such a powerful and useful tool - there is a package for almost everything you could want to do in R.\n\n\n\n\n\n\nTask\n\n\n\nIn the console, type install.packages(\"cowsay\") and hit Enter.\nLots of red text will come up, and it will take a bit of time.\nWhen it has finished, and R is ready for you to use again, you will see the blue sign &gt;.\n\n\n\n6.2 Using R packages\nIt’s not enough just to install a package - to actually use the package, we need to load it using library().\nWe install a package only once. But each time we open RStudio, we have to load the packages we need.\n\n\n\n\n\n\n\nTask\n\n\n\nIn the console again, type library(cowsay) and hit enter. This loads the package for us to use it.\nThen, type say(\"hello world\", by = \"cow\") and hit enter.\nHopefully you got a similar result to ours:\n\nlibrary(cowsay)\nsay(\"Hi Folks!\", by = \"cow\")\n\n\n ___________ \n&lt; Hi Folks! &gt;\n ----------- \n      \\\n       \\\n\n        ^__^ \n        (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||"
  },
  {
    "objectID": "rd1_01.html#your-first-.rmd-file",
    "href": "rd1_01.html#your-first-.rmd-file",
    "title": "Getting started with R and RStudio",
    "section": "\n7 Your first .Rmd file",
    "text": "7 Your first .Rmd file\nIn order to be able to write and compile Rmarkdown documents (and do a whole load of other things which we are going to need throughout the course) we are now going to install a set of packages known collectively as the “tidyverse” (this includes the “rmarkdown” package).\n\n\nIf you installed R/Rstudio on your own computer, then in the console, type install.packages(\"tidyverse\") and hit Enter. You may have to wait a while.\n\nIf you are using rstudio.ppls.ed.ac.uk, then we have already installed “tidyverse” and a few other useful packages for you, so you don’t have to do anything!\n\n\n\n\n\n\n\nTask\n\n\n\nOpen a new Rmarkdown document: File &gt; New File &gt; R Markdown…\nWhen the box pops-up, give a title of your choice (“Intro lab”, maybe?) and your name as the author.\n\n\n\n7.1 Writing code in a .Rmd file\nThe file which you have just created will have some template stuff in it. Delete everything below the first code chunk to start with a fresh document:\n\n\n\n\n\n\n\nTask\n\n\n\nInsert a new code chunk by either using the Insert button in the top right of the document and selecting R, or by typing Ctrl + Alt + i on Windows or Option + Cmd + i on MacOS.\nInside the chunk, type:\nprint(\"Hello world! My name is ?\")\nTo execute the code inside the chunk, you can either:\n\ndo as you did in the R script - put the text-cursor on the first line, and hit Ctrl/Cmd + Enter to run the lines sequentially;\nclick the little green arrow at the top right of your code-chunk to run all of the code inside the chunk;\nwhile your cursor is inside the code chunk, press Cmd + Shift + Enter to run all of the code inside the chunk.\n\nYou can see that the output gets printed below.\n\n\n\nWe’re going to use some functions which are in the tidyverse package, which we already installed above (or which we installed for you on the server).\nTo use the package, we need to load it.\nWhen writing analysis code, we want it to be reproducible - we want to be able to give somebody else our code and the data, and ensure that they can get the same results. To do this, we need to show what packages we use.\nIt is good practice to load any packages you use at the top of your code, so that users of your code will know what packages they will need to install to run your code.\nIn your first code chunk, type:\n\n# I'm going to use these packages in this document:\nlibrary(tidyverse)\n\nand run the chunk.\nNOTE: You might get various messages popping up below when you run this chunk, that is fine.\nComments in code\nNote that using # in R code makes that line a comment, which basically means that R will ignore the line. Comments are useful for you to remind yourself of what your code is doing.\n\n7.2 Writing text in a .Rmd file\nPlace your cursor outside the code chunk, and below the code chunk add a new line with the following:\n# R code examples\nNote that when the # is used in a Rmarkdown file outside of a code-chunk, it will make that line a heading when we finally get to compiling the document. Below, what you see on the left will be compiled to look like those on the right:\n\n\n7.2.1 RECALL:\n\n\nInside a code-chunk, one or more #s will create a comment\n\n\nOutside a code-chunk, one ore more #s will create headings\n\nIn your Rmarkdown document, choose a few of the symbols below, and write an explanation of what it does, giving an example in a code chunk. You can see an example of the first few below.\n\n+\n-\n*\n/\n()\n^\n&lt;-\n&lt;\n&gt;\n&lt;=\n&gt;=\n==\n!=\n\n\n\n7.3 Storing data into R\nWe’ve already seen how to assign a value to a name/symbol using &lt;-. However, we’ve only seen how to assign a single number, e.g, x &lt;- 5.\nTo store a sequence of numbers into R, we combine the values using the combine function c() and give the sequence a name. A sequence of elements all of the same type is called a vector. To view the stored content, simply type the name of the vector.\n\nmyfirstvector &lt;- c(1, 5, 3, 7)\nmyfirstvector\n\n[1] 1 5 3 7\n\n\nWe can perform arithmetic operations on each value of the vector. For example, to add five to each entry:\n\nmyfirstvector + 5\n\n[1]  6 10  8 12\n\n\nRecall that vectors are sequences of elements all of the same type. They do not have to be always numbers; they could be words such as real or fictional animals. Words need to be written inside quotations, e.g. “anything”, and instead of being of numeric type, we say they are characters.\n\nwordsvector &lt;- c(\"cat\", \"dog\", \"parrot\", \"peppapig\")\nwordsvector\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nNOTE\nYou can use either double-quote or single-quote:\n\nc(\"cat\", \"dog\", \"parrot\", \"peppapig\")\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\nc('cat', 'dog', 'parrot', 'peppapig')\n\n[1] \"cat\"      \"dog\"      \"parrot\"   \"peppapig\"\n\n\n\nThe function class() will tell you the type of the object. In this case, it is a character vector.\n\nclass(wordsvector)\n\n[1] \"character\"\n\n\nIt does not make sense to add a number to words, hence some operations like addition and multiplication are only defined on vectors of numeric type. If you make a mistake, R will warn you with a red error message.\n\nwordsvector + 5\n\n\nError in wordsvector + 5 : non-numeric argument to binary operator\n\nFinally, it is important to notice that if you combine together in a vector a number and a word, R will transform all elements to be of the same type. Why? Recall: vectors are sequences of elements all of the same type. Typically, R chooses the most general type between the two. In this particular case, it would make everything a character, check the ““, as it would be harder to transform a word into a number!\n\nmysecondvector &lt;- c(4, \"cat\")\nmysecondvector\n\n[1] \"4\"   \"cat\"\n\n\n\n7.4 Reading data into R\nWhile we can manually input data like we did above, more often, we will need to read in data which has been created elsewhere (like in excel, or by some software which is used to present participants with experiments).\n\n\n\n\n\n\nTask\n\n\n\nAdd a new heading by typing the following:\n# Reading and storing data\nRemember: We make headings using the # outside of a code chunk.\n\n\n\n\n\n\n\n\nTask\n\n\n\nOpen Microsoft Excel, or LibreOffice Calc, or whatever spreadsheet software you have available to you, and create some data with more than one variable.\nIt can be whatever you want, but we’ve used a very small example here for you to follow, so feel free to use it if you like.\nWe’ve got two sets of values here: the names and the birth-years of each member of the Beatles. The easiest way to think of this would be to have a row for each Beatle, and a column for each of name and birth-year.\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nSave the data as a .csv file.\nAlthough R can read data when it’s saved in Microsoft/LibreOffice formats, the simplest, and most universal way to save data is as simple text, with the values separated by some character - .csv stands for comma separated values.\nIn Microsoft Excel, if you go to File &gt; Save as\nIn the Save as Type box, choose to save the file as CSV (Comma delimited).\nImportant: save your data in the project folder you created at the start of this lab.\n\n\nBack in RStudio…\nNext, we’re going to read the data into R. We can do this by using the read_csv() function, and directing it to the file you just saved.\nIf you are using RStudio on the server, you will need to upload the file you just saved to the server. The video below shows an example of this:\n\n\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nCreate a new code-chunk in your Rmarkdown and, in the chunk, type: read_csv(\"name-of-your-data.csv\"), where you replace name-of-your-data with whatever you just saved your data as in your spreadsheet software.\n\n\nHelpful tip\nIf you have your text-cursor inside the quotation marks, and press the tab key on your keyboard, it will show you the files inside your project. You can then use the arrow keys to choose between them and press Enter to add the code:\n\n\n\n\n\n\nWhen you run the line of code you just wrote, it will print out the data, but will not store it. To do that, we need to assign it as something:\n\nbeatles &lt;- read_csv(\"data_from_excel.csv\")\n\nNote that this will now turn up in the Environment pane of RStudio.\nNow that we’ve got our data in R, we can print it out by simply invoking its name:\n\nbeatles\n\n# A tibble: 4 × 2\n  name   birth_year\n  &lt;chr&gt;       &lt;dbl&gt;\n1 John         1940\n2 Paul         1942\n3 George       1943\n4 Ringo        1940\n\n\nAnd we can do things such as ask R how many rows and columns there are:\n\ndim(beatles)\n\n[1] 4 2\n\n\nThis says that there are 4 members of the Beatles, and for each we have 2 measurements.\nTo get more insight into what the data actually are, you can either use the structure str() function, or glimpse() function to get a glimpse at the data:\n\nstr(beatles)\n\nspc_tbl_ [4 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ name      : chr [1:4] \"John\" \"Paul\" \"George\" \"Ringo\"\n $ birth_year: num [1:4] 1940 1942 1943 1940\n - attr(*, \"spec\")=\n  .. cols(\n  ..   name = col_character(),\n  ..   birth_year = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(beatles)\n\nRows: 4\nColumns: 2\n$ name       &lt;chr&gt; \"John\", \"Paul\", \"George\", \"Ringo\"\n$ birth_year &lt;dbl&gt; 1940, 1942, 1943, 1940\n\n\n\n\n\n\n\n\nTask\n\n\n\nUse dim() to confirm how many rows and columns are in your data.\nUse str() or glimpse() to take a look at the structure of the data. Don’t worry about the output of str() right now, we’ll pick up with this in the next chapter.\n\n\n\n7.5 Getting help in R\ndim(), str(), read_csv() are all functions.\nFunctions perform specific operations / transformations in computer programming.\nThey can have inputs and outputs. For example, dim() takes some data you have stored in R as its input, and gives the dimensions of the data as its output.\nIn R, functions come with help pages, where you can see information about the various inputs and outputs, and examples of how to use them.\nIn the console, type ?dim (or ?dim() will work too) and press Enter.\nThe bottom-right pane (where things like plots are also shown), should switch to the Help tab, and open the documentation page for the dim() function!\n\n\n\n\n\n\nWhy did we ask you to write this bit in the console, whereas previously we’ve been writing stuff in the RMarkdown document in the editor?\nWell, when writing an RMarkdown document, the aim at the end is to have a nice document which we can read. For instance, we can write statistical reports, journal papers, coursework reports etc, in Rmarkdown. But the reader doesn’t need to see that we’re looking up how to use some function - just like they don’t need to know that we might look up a word in the dictionary before using it.\n\n\n\n\n7.6 Compiling a .Rmd file\n\n\n\n\n\n\nTask\n\n\n\nBy now, you should have an Rmardkown document (.Rmd) with your answers to the tasks we’ve been through today.\nCompile the document by clicking on the Knit button at the top (it will ask you to save your document first). The little arrow to the right of the Knit button allows you to compile to either .pdf or .html."
  },
  {
    "objectID": "rd1_01.html#checklist-for-today",
    "href": "rd1_01.html#checklist-for-today",
    "title": "Getting started with R and RStudio",
    "section": "\n8 Checklist for today",
    "text": "8 Checklist for today\n\n\nEITHER:\n\nOption A: Get started with the PPLS RStudio Server   ✔\nOption B: Install R and RStudio   ✔\n\n\nStart a new project for the course   ✔\nChange a few RStudio settings (recommended)   ✔\nInstall some R packages (the “tidyverse”)   ✔\nCreate a new Rmarkdown document   ✔\nComplete today’s tasks and exercises on storing data in R   ✔\nCompile your Rmarkdown document   ✔\nCelebrate!   ✔ 🎉"
  },
  {
    "objectID": "rd1_01.html#glossary",
    "href": "rd1_01.html#glossary",
    "title": "Getting started with R and RStudio",
    "section": "\n9 Glossary",
    "text": "9 Glossary\n\nConsole: where the code gets executed\nEnvironment: R’s memory, it lists all the names of things with stuff stored into them\nEditor: where we edit code\nR script: a file with R code and comments\nRmarkdown document: an enhanced file where you can combine together R code, explanatory text, and plots.\npackages (also library): user-created bundles providing additional functionality to your local R installation\nfunctions: they take inputs, do some transformation or computation on them, and return a result (output)\n?: returns the help page of a function, e.g. ?dim.\n\n\n\n\n\n\n\n\nSymbol\nDescription\nExample\n\n\n\n+\nAdds two numbers together\n\n2+2 - two plus two\n\n\n-\nSubtract one number from another\n\n3-1 - three minus one\n\n\n*\nMultiply two numbers together\n\n3*3 - three times three\n\n\n/\nDivide one number by another\n\n9/3 - nine divided by three\n\n\n()\ngroup operations together\n\n(2+2)/4 is different from 2+2/4\n\n\n\n^\nto the power of..\n\n4^2 - four to the power of two, or four squared\n\n\n&lt;-\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx &lt;- 10\n\n\n=\nstores an object in R with the left hand side (LHS) as the name, and the RHS as the value\nx = 10\n\n\n&lt;\nis less than?\n2 &lt; 3\n\n\n&gt;\nis greater than?\n2 &gt; 3\n\n\n&lt;=\nis less than or equal to?\n2 &lt;= 3\n\n\n&gt;=\nis greater than or equal to?\n2 &gt;= 2\n\n\n==\nis equal to?\n(5+5) == 10\n\n\n!=\nis not equal to?\n(2+3) != 4\n\n\nc()\ncombines values into a vector (a sequence of values)\nc(1,2,3,4)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The lab materials were created by:\n\nDr Umberto Noe\nDr Josiah King\nDr Emma Waterston\nDepartment of Psychology, The University of Edinburgh\n\n\n\n\n Back to top"
  },
  {
    "objectID": "1_11_sampling.html",
    "href": "1_11_sampling.html",
    "title": "Formative report B",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\n\n\n\n\n\nThis week: Submission of Formative Report B\n\n\n\n\n\n\nYour group must submit one PDF file for formative report B by 12 noon on Friday 29th of November 2024.\n\nNo extensions are possible for group-based reports, see “Assessment Information” page on LEARN.\nTo submit, go to the course Learn page &gt; click “Assessment” &gt; click “Submit Formative Report B (PDF file only)”.\nOnly one person per group is required to submit on behalf of the entire group. Once submitted, let your group know on the Group Discussion Space. The other members in the group don’t have to do anything else.\nEnsure that everyone in the group has joined the group on LEARN. Otherwise, you won’t see the feedback.\nIf more than one submission is made per group, only the most recent one will be considered.\n\n\nThe submitted report must be a PDF file of max 6 sides of A4 paper.\n\nKeep the default settings in terms of Rmd knitting font and page margins.\nEnsure your report title includes the group name: Group NAME.LETTER\nIn the author section, ensure the report lists the exam numbers of all group members: B000001, B000002, …\n\n\nAt the end of the file, you will place the appendices and these will not count towards the six-page limit.\n\nYou can include an optional appendix for additional tables and figures which you can’t fit in the main part of the report;\n\nYou must include a compulsory appendix listing all of the R code used in the report. This is done automatically if you end your file with the following section, which is already included in the template Rmd file:\n# Appendix: R code\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n\n```\n\nExcluding the Appendix, the report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge.\n\n\nIn Week 12 (next week)\n\nThere will be no lectures\nThere will be no labs\nThere will be no weekly quiz\nSolutions to Formative Report B will be posted on LEARN as study material.\nAt the end of week 12, we will send an announcement when we will have finished providing feedback on your submissions.\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback.\n\n\nThis week you will only focus on task B5. Below there are sub-steps you need to consider to complete task B5.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nDid you install tinytex? If yes, go to the next bullet point. If not, check the hint. This package is required to compile an Rmd file into a PDF for submission.2\n\n\n\n\nOrganise the report to have the following structure:\n\n\nIntroduction:\n\nWhat are the data that you are analysing (i.e. give a brief intro) and where can these be found?\nWhich questions of interest are you investigating in the report?\nWhich variables will you use to answer those questions and what do those variables represent?\nWhat is the type of these variables?\nAre there any missing values in these variables?\n\n\nAnalysis: Present and interpret your results. This section should only contain text, figures, and tables. No R code or R output printout should be visible.\nDiscussion: Summarise the key findings from the analysis section, and provide take-home messages that directly answer the questions of interest. Link your answers to the questions detailed in the introduction. No new statistical results should be presented in the discussion.\nAppendix A (optional): For additional figures and tables that don’t fit in the page limit. Any figures/tables presented here should be referenced in the main part of the report and have a caption. Appendix A doesn’t count in the page limit.\nAppendix B (compulsory): Presents all the R code used. This is automatically created for you if you used the template Rmd file. If you haven’t copy and paste that section from the template into your file. Appendix B doesn’t count in the page limit.\n\n\nEdit your figures/tables formatting as required to ensure that your report meets the page limit.\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\nKnit the document to PDF: click File &gt; Knit Document.\n\n\n\n\n\n\n\nSuccessful knitting checklist\n\n\n\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist\n\n\n\n\nSubmit the PDF file on Learn by 12 noon on Friday 29th November 2024:\n\nGo to the Learn page of the course\nClick Assessments\nClick Submit Formative Report B (PDF file only)\nFollow the instructions"
  },
  {
    "objectID": "1_11_sampling.html#formative-report-b",
    "href": "1_11_sampling.html#formative-report-b",
    "title": "Formative report B",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\n\n\n\n\n\nThis week: Submission of Formative Report B\n\n\n\n\n\n\nYour group must submit one PDF file for formative report B by 12 noon on Friday 29th of November 2024.\n\nNo extensions are possible for group-based reports, see “Assessment Information” page on LEARN.\nTo submit, go to the course Learn page &gt; click “Assessment” &gt; click “Submit Formative Report B (PDF file only)”.\nOnly one person per group is required to submit on behalf of the entire group. Once submitted, let your group know on the Group Discussion Space. The other members in the group don’t have to do anything else.\nEnsure that everyone in the group has joined the group on LEARN. Otherwise, you won’t see the feedback.\nIf more than one submission is made per group, only the most recent one will be considered.\n\n\nThe submitted report must be a PDF file of max 6 sides of A4 paper.\n\nKeep the default settings in terms of Rmd knitting font and page margins.\nEnsure your report title includes the group name: Group NAME.LETTER\nIn the author section, ensure the report lists the exam numbers of all group members: B000001, B000002, …\n\n\nAt the end of the file, you will place the appendices and these will not count towards the six-page limit.\n\nYou can include an optional appendix for additional tables and figures which you can’t fit in the main part of the report;\n\nYou must include a compulsory appendix listing all of the R code used in the report. This is done automatically if you end your file with the following section, which is already included in the template Rmd file:\n# Appendix: R code\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n\n```\n\nExcluding the Appendix, the report should not include any reference to R code or functions, but be written for a generic reader who is only assumed to have a basic statistical understanding without any R knowledge.\n\n\nIn Week 12 (next week)\n\nThere will be no lectures\nThere will be no labs\nThere will be no weekly quiz\nSolutions to Formative Report B will be posted on LEARN as study material.\nAt the end of week 12, we will send an announcement when we will have finished providing feedback on your submissions.\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback.\n\n\nThis week you will only focus on task B5. Below there are sub-steps you need to consider to complete task B5.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nDid you install tinytex? If yes, go to the next bullet point. If not, check the hint. This package is required to compile an Rmd file into a PDF for submission.2\n\n\n\n\nOrganise the report to have the following structure:\n\n\nIntroduction:\n\nWhat are the data that you are analysing (i.e. give a brief intro) and where can these be found?\nWhich questions of interest are you investigating in the report?\nWhich variables will you use to answer those questions and what do those variables represent?\nWhat is the type of these variables?\nAre there any missing values in these variables?\n\n\nAnalysis: Present and interpret your results. This section should only contain text, figures, and tables. No R code or R output printout should be visible.\nDiscussion: Summarise the key findings from the analysis section, and provide take-home messages that directly answer the questions of interest. Link your answers to the questions detailed in the introduction. No new statistical results should be presented in the discussion.\nAppendix A (optional): For additional figures and tables that don’t fit in the page limit. Any figures/tables presented here should be referenced in the main part of the report and have a caption. Appendix A doesn’t count in the page limit.\nAppendix B (compulsory): Presents all the R code used. This is automatically created for you if you used the template Rmd file. If you haven’t copy and paste that section from the template into your file. Appendix B doesn’t count in the page limit.\n\n\nEdit your figures/tables formatting as required to ensure that your report meets the page limit.\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\nKnit the document to PDF: click File &gt; Knit Document.\n\n\n\n\n\n\n\nSuccessful knitting checklist\n\n\n\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist\n\n\n\n\nSubmit the PDF file on Learn by 12 noon on Friday 29th November 2024:\n\nGo to the Learn page of the course\nClick Assessments\nClick Submit Formative Report B (PDF file only)\nFollow the instructions"
  },
  {
    "objectID": "1_11_sampling.html#footnotes",
    "href": "1_11_sampling.html#footnotes",
    "title": "Formative report B",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nInstalling tinytex. Copy the line below and paste it into the console:\ninstall.packages(\"tinytex\")\npress Enter. Copy and paste the line below into the console:\ntinytex::install_tinytex()\npress Enter. Type Y and press Enter again.↩︎"
  },
  {
    "objectID": "1_09_discrete_dist.html",
    "href": "1_09_discrete_dist.html",
    "title": "Random Variables (Discrete)",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\n\n\n\n\n\nNames of operators\n\n\n&lt;- is the assignment operator\n\n\nobject_name &lt;- what_to_store means: assign what_to_store into object_name\n\n\n\n\n|&gt; or %&gt;% is the pipe operator\n\ntake what’s on the left and then do something to it\n\n\n\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.B3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback."
  },
  {
    "objectID": "1_09_discrete_dist.html#formative-report-b",
    "href": "1_09_discrete_dist.html#formative-report-b",
    "title": "Random Variables (Discrete)",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\n\n\n\n\n\nNames of operators\n\n\n&lt;- is the assignment operator\n\n\nobject_name &lt;- what_to_store means: assign what_to_store into object_name\n\n\n\n\n|&gt; or %&gt;% is the pipe operator\n\ntake what’s on the left and then do something to it\n\n\n\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.B3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback."
  },
  {
    "objectID": "1_09_discrete_dist.html#b3-sub-tasks",
    "href": "1_09_discrete_dist.html#b3-sub-tasks",
    "title": "Random Variables (Discrete)",
    "section": "\n2 B3 sub-tasks",
    "text": "2 B3 sub-tasks\nThis week you will only focus on task B3. Below there are sub-steps you need to consider to complete task B3.\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nContext\n\n\n\nA new movie theatre is opening in Texas, and the management team are reviewing the requirements for snack stalls and their car parking capacity. They are interested in determining whether to have only one or multiple snack stalls selling popular food and drink items; and are considering whether to expand their car park size or introduce a bus stop on cinema grounds.\nIn this lab, you will need to consider both the Snacks and PrivateTransport variables from the Hollywood movies dataset when answering the questions below.\n\n\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nConsider the Snacks and PrivateTransport variables, are they discrete or continuous? What are the lowest and highest possible values that the two random variables could take?2\n\n\n\nPlot separately the sample frequency distribution of the Snacks and PrivateTransportvariables with barplots.3\n\n\n\nWhat distribution do the Snacks and PrivateTransport variables follow? Estimate the parameters of the two distributions from the sample data.4\n\n\n\nPlot the fitted Binomial distribution on top of the sample frequency distribution for each variable. Is the Binomial distribution a good model for each variable?5\n\n\n\n\n\n\n\n\n\n\nIn R, 0:50 gives you the sequence of whole numbers from 0 to 50, i.e. 0, 1, 2, …, 49, 50.\nThe code start:stop in R is short syntax to create a sequence of values going from start to stop in steps of 1.\n\nWhat is the probability that more than half of the audience for each movie (i.e., &gt;25 viewers) bought snacks? What is the probability that less than half of the audience for each movie (i.e., &lt;25 viewers) traveled via private transportation?6\n\n\n\nBased on the probabilities you have reported above, do you think that the new movie theatre should (1) invest in multiple snack stations, or just one; and (2) increase car parking capacity or add a bus stop? Justify your answer.\nA recent survey suggested that 49% of movie viewers always buy some form of snack item (i.e., popcorn, drinks, sweets) when watching a movie, and another suggested that 70% of movie viewers travel to the cinema via private transport. Are your two probability estimates consistent with the survey-reported values?7\n\n\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_09_discrete_dist.html#worked-example",
    "href": "1_09_discrete_dist.html#worked-example",
    "title": "Random Variables (Discrete)",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe dataset available at https://uoepsy.github.io/data/RestaurantTips2.csv was collected by the owner of a US bistro, and contains 99 observations on 10 variables. It is a subset of the RestaurantTips.csv data presented in the past weeks, focusing only on parties of 2 people.8\nThe bistro owner is interested in coffee sales, and whether they should consider introducing a 2 for 1 coffee deal to entice customers to purchase one of their Christmas coffees. Another option they are considering is starting a loyalty scheme for customers to be rewarded for every coffee purchase. Your job is to advise them on which scheme they should run to benefit most customers.\nFor context, Americans drink a lot of coffee, but slightly less than Norwegians (89.4% of Norwegians drink at least one coffee per day!9). We are interested in estimating the probability that an adult American will drink coffee.\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\nHadCoffee\nNumber of guests in the group who had coffee\n\n\nIQ1\nScore on IQ test for guest 1\n\n\nIQ2\nScore on IQ test for guest 2\n\n\n\n\n\nFirst, let’s read the new dataset into R:\n\nlibrary(tidyverse)\ntips2 &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips2.csv\")\nhead(tips2)\n\n# A tibble: 6 × 10\n   Bill   Tip Credit Guests Day   Server PctTip HadCoffee   IQ1   IQ2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2         2    93   100\n2  32.0  5.01 y           2 f     A        15.7         2    96    98\n3  17.4  3.61 y           2 f     B        20.8         2    94    99\n4  15.4  3    n           2 f     B        19.5         2    99   108\n5  18.6  2.5  n           2 f     A        13.4         2   129   106\n6  21.6  3.44 n           2 f     B        16           2    82   118\n\n\nAs mentioned in the data description, the variable HadCoffee records how many, out of the 2 people in each party, had coffee. Clearly this can only take the values 0, 1, or 2 if none, only one, or both of the people in the party ordered coffee.\n\nIf we were asked to describe what kind of variable HadCoffee is, and to comment on the kind of probability distribution it may follow, we could say:\n\nThe number of coffee-consuming guests out of parties of size 2 is a discrete random variable that could be modeled by a Binomial probability distribution.\n\nWe can plot the frequency distribution of the HadCoffee variable in the sample as following:\n\nCompute the sample frequency distribution, showing the absolute frequencies (counts) and relative frequencies (proportions):\n\n# freq_distr data contains the sample frequencies\nfreq_distr &lt;- tips2 |&gt;\n    count(HadCoffee) |&gt;\n    mutate(rel_freq = n / sum(n))\nfreq_distr\n\n# A tibble: 3 × 3\n  HadCoffee     n rel_freq\n      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1         0     9   0.0909\n2         1    29   0.293 \n3         2    61   0.616 \n\n\nPlot the frequency distribution of the discrete variable HadCoffee using a barplot. As we already have the x values (HadCoffee) and the y values (the height of the bars, rel_freq), we use geom_col().\n\n\ngeom_col() is used to create a barplot when you have already computed the bar heights, i.e. the frequency table. This draws columns at x of height given by y as specified inside aes(x = ?, y = ?)geom_bar() takes the original data, and does the counting for you. Because of this, it only requires you to specify x into aes(x = ?). The height is computed for you internally by counting the unique values in x.\n\nggplot(freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_col() +\n    labs(x = \"Coffee-drinking customers per party\",\n         y = \"Relative frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a distribution\n\n\n\n\n\nFitting a Binomial distribution to data involves estimating the parameters of the distribution from the data. In other words, we want to find values for \\(n\\) and \\(p\\) from the variable HadCoffee in the our data.\n\n\n\n\n\nWe can now fit a Binomial distribution to the variable. To do so, we need to start by estimating the parameters of the Binomial distribution:\n\n\n\\(n\\), the number of trials (or size)\n\n\\(p\\), the probability of success\n\n\n\nFor this dataset, \\(n\\) represents the size of each party, i.e. \\(n = 2\\). The discrete variable HadCoffee represents how many guests had coffee, out of the 2 possible guests per party.\n\n\nIn a Binomial distribution, the number of trials (\\(n\\)) should not be confused with the sample size. The sample size would be the total number of parties in the dataset, i.e. 99. It’s just unfortunate that both use the same symbol \\(n\\), but which one is the correct one should be clear from the context.\nThe event “had coffee” represents our “success”, and \\(p\\) denotes the probability of success. In other words, \\(p\\) represents the probability of an individual having coffee.\n\n\n\n\n\n\nEstimating the probability of success\n\n\n\n\n\nThe expected value \\(E(X)\\) of a Binomial distribution (i.e., the mean) is \\(E(X) = n * p\\) where \\(n\\) = size = number of trials, and \\(p\\) is the probability of success.\nFrom this, we have that \\(p = E(X) / n\\).\nWe typically denote the estimated probability of success from the sample data with a hat on top, \\(\\hat{p}\\), written in text as $\\hat{p}$.\n\n\n\n\n# The mean of the discrete random variable\nEX &lt;- mean(tips2$HadCoffee)  \n\n# Each party size (n in the formula)\nsize &lt;- 2\n\n# Estimated probability of having coffee: E(X) / n\np_hat &lt;- EX / size\n\n#check value\np_hat\n\n[1] 0.7626263\n\n\n\nWe can then compare the sample frequency distribution to the Binomial distribution, and comment on whether the Binomial fit is good:\n\nWe can create a new tibble having two columns. The first has the possible values of the Binomial distribution (0, 1, or 2 guests ordering coffee out of the 2). The second column has the theoretical probabilities, for each of the possible values, predicted by the Binomial distribution with parameters \\(n = 2\\) and \\(p = 0.76\\).\n\n# binom_distr data contains the binomial probabilities\nbinom_distr &lt;- tibble(\n    HadCoffee = 0:2,\n    binom_prob = dbinom(x = HadCoffee, size = 2, prob = p_hat)\n)\nbinom_distr\n\n# A tibble: 3 × 2\n  HadCoffee binom_prob\n      &lt;int&gt;      &lt;dbl&gt;\n1         0     0.0563\n2         1     0.362 \n3         2     0.582 \n\n\nWe can plot the sample frequency distribution as a bar plot and put on top the fitted Binomial probability distribution as dots (see Option 1), and even add segments to show the Binomial distribution (see Option 2).\nIn both cases, you need to specify different data for each geom_. The bars will use the sample frequency distribution computed using count() before. The points will use the Binomial probabilities computed using dbinom().\n\n\nOption 1\nOption 2\n\n\n\n\nggplot() +\n    geom_col(data = freq_distr, aes(x = HadCoffee, y = rel_freq)) + # uses freq_distr data\n    geom_point(data = binom_distr,                                  # uses binom_distr data\n               aes(x = HadCoffee, y = binom_prob),  \n               colour = 'red', size = 3)\n\n\n\n\n\n\n\n\n\n\nggplot() +\n    geom_col(data = freq_distr, aes(x = HadCoffee, y = rel_freq)) +\n    geom_point(data = binom_distr, \n               aes(x = HadCoffee, y = binom_prob),  # adds points\n               colour = 'red', size = 3) +\n    geom_segment(data = binom_distr, \n                 aes(x = HadCoffee, y = binom_prob, # adds line (optional) \n                     xend = HadCoffee, yend = 0),   # from (x,y) to (xend,yend)\n                 colour = 'red')\n\n\n\n\n\n\nFigure 1: Probability of guests drinking coffee.\n\n\n\n\n\n\n\nThe Binomial distribution seems to be a good fit for the sample distribution, as the probabilities tend to agree. Among all parties of two guests, the highest probability is that both guests had coffee, and no one having coffee has the lowest probability.\n\n\n\n\n\n\nProbability Mass Function\n\n\n\n\n\n\n\n\\(P(X = x)\\) = dbinom(x, size, prob)\n\n\nThe probability mass function computes \\(P(X = x)\\) for a Binomial distribution where number of trials is size and probability of success is prob.\n\n\n\n\nTo calculate the probability that 1 person in the party orders coffee, we can compute the following:\n\nThe probability P(X = 1) for a Binomial with \\(x = 1\\), \\(size = 2\\), and \\(p = 0.76\\) is:\n\ndbinom(1, size = 2, prob = p_hat)\n\n[1] 0.3620549\n\n\n\nTo calculate the probability that 2 people in the party order coffee, we can compute the following:\n\nThe probability P(X = 2) for a Binomial with \\(x = 2\\), \\(size = 2\\), and \\(p = 0.76\\) is:\n\ndbinom(2, size = 2, prob = p_hat)\n\n[1] 0.5815988\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\n\n\n\n\n\\(P(X \\leq q)\\) = pbinom(q, size, prob)\n\n\nThe cumulative distribution function \\(P(X \\leq q)\\) gives the probability of having less than or equal to \\(q\\) successes.\nFrom the total probability rule, the probability of a value being greater than \\(q\\) is computed as \\(P(X &gt; q) = 1 - P(X \\leq q)\\):\n# option 1: P(X &gt; q) = 1 - P(X &lt;= q)\n1 - pbinom(q, size, prob)\n# option 2: P(X &gt; q) directly\npbinom(q, size, prob, lower.tail = FALSE)\n\n\n\n\nTo calculate the probability that 1 person or less orders coffee, we can compute the following:\n\nWe do this with the pbinom(q, size, prob) function, with q = 1 to have \\(P(X \\leq 1) = P(X = 0) + P(X = 1)\\):\n\npbinom(1, size = 2, prob = p_hat)\n\n[1] 0.4184012\n\n\nYou can also see this by creating a new column in the binomial distribution which has the cumulative sums of the probabilities, i.e. the values P(X = 0), followed by P(X = 0) + P(X = 1), and finally by P(X = 0) + P(X = 1) + P(X = 2):\n\nbinom_distr |&gt; \n    mutate(\n        cumul_prob = cumsum(binom_prob)\n    )\n\n# A tibble: 3 × 3\n  HadCoffee binom_prob cumul_prob\n      &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1         0     0.0563     0.0563\n2         1     0.362      0.418 \n3         2     0.582      1     \n\n\nAs you can see, P(X = 0) + P(X = 1) is 0.42 in the second row, which agrees with the result computed using pbinom above.\n\nTo calculate the probability that at least one person from the group orders coffee, we can compute one of the below four equivalent calculations:\n\n\n\n1 - P(X &lt;= 0)\nP(X &gt; 0)\nP(X = 1) + P(X = 2)\n1 - P(X = 0)\n\n\n\n\n1 - pbinom(0, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\npbinom(0, size = 2, prob = p_hat, lower.tail = FALSE)\n\n[1] 0.9436537\n\n\n\n\n\n# sum up the probability of that 1 person orders coffee and the probability that both persons order coffee\n# P(1 guest has coffee) + P(2 guests has coffee)\ndbinom(1, size = 2, prob = p_hat) + dbinom(2, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\n# 1 - P(X = 0)\n# As the total probability is 1, we can get it as 1 - P(0 guests have coffee). \n# This follows from the total probability rule\n1 - dbinom(0, size = 2, prob = p_hat)\n\n[1] 0.9436537\n\n\n\n\n\n\nTo check whether our estimated probability of drinking coffee consistent with the one reported by a recent YouGov survey, which reported that three quarters of adult Americans drink coffee, we can take a look at our estimated \\(\\hat{p}\\). We can also then comment on whether adult Americans are more or less likely to drink coffee than Norwegians?\n\n\n# YouGov reported probability\np_survey &lt;- 3/4\np_survey\n\n[1] 0.75\n\n\n\n# Our estimate\np_hat\n\n[1] 0.7626263\n\n\nThe estimated probability of drinking coffee, based on the data from a US bistro, is 0.76. This is relatively close to the YouGov reported result of 0.75. The small deviation may be due to sampling variability, due to having chosen a different sample of people from the one that were considered in this bistro.\nAccording to a recent survey, 89.4% of Norwegians drink at least one coffee per day10 As such, while adult Americans drink lots of coffee, an adult American is less likely to drink coffee than a Norwegian.\n\n\n\n\n\n\nExample writeup\n\n\n\nFigure 1 displays the distribution of coffee-drinking customers in two-party tables, with a Binomial fit superimposed as red dots. Both party guests are more likely to both drink coffee (0.58) than not (0.06). The probability of only one guest drinking coffee is 0.36. As such, the probability of at least one guest drinking coffee is 0.94. The owner of the bistro should not consider running a 2 for 1 offer on Christmas coffees, as they would lose out on income. It is most likely that groups of two customers will already buy two coffees when they visit the bistro (0.58). Instead, the owner might want to offer a loyalty scheme to reward customers for purchasing coffees, regardless of the quantity, as this will also reward parties of two where only one person purchases a coffee."
  },
  {
    "objectID": "1_09_discrete_dist.html#student-glossary",
    "href": "1_09_discrete_dist.html#student-glossary",
    "title": "Random Variables (Discrete)",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nrename()\n?\n\n\ndbinom()\n?\n\n\npbinom()\n?\n\n\ngeom_col()\n?\n\n\ngeom_point()\n?\n\n\ngeom_segment()\n?\n\n\ncumsum()\n?"
  },
  {
    "objectID": "1_09_discrete_dist.html#footnotes",
    "href": "1_09_discrete_dist.html#footnotes",
    "title": "Random Variables (Discrete)",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nThink about the definition of the two variables from the data description in week 7:\n\n\nSnacks: How many of the 50 audience raters bought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation\n\nThe lowest possible value for Snack will be 0 if none of the 50 audience raters bought snacks, and 50 if all of them bought snacks. Similarly, for PrivateTransport, 0 if none of them travelled to the cinema via private transportation, and 50 if all of them did use private transportation to reach the cinema.↩︎\n\n\nHint:\nFirst, compute the frequency distribution of each variable, using count() and mutate().\nNext, plot each frequency distribution as a barplot. Since you already have the bar heights, use geom_col() to create the bars.\nWarning: The function geom_bar(), instead, does the counting for you and requires a data set instead of a frequency table.↩︎\n\n\nHint: A Binomial distribution has two parameters, the number of trials or size (\\(n\\)) and the probability of success (\\(p\\)).\n\nTo estimate the number of trials, \\(n\\), use the study design information that you can find in the data description.\nTo estimate the probability of success, \\(p\\), use the following fact. The expected value \\(E(X)\\) of a Binomial distribution (i.e., the mean) is \\(E(X) = n * p\\) where \\(n\\) = number of trials, and \\(p\\) is the probability of success. From this, we have that \\(p = E(X) / n\\).\n\nIn the movies data, the number of trials (size) is \\(n = 50\\). This is because the variables Snacks and PrivateTransport record how many, out of the 50 audience movie raters, bought snacks or travelled to the cinema by private transportation, respectively.↩︎\n\n\nFirst, create a new tibble with two columns: (1) the possible values of the Binomial distribution, and (2) the probabilities of each outcome.\nNext, take the barplots previously created and use geom_point(data = ?, aes(x = ?, y = ?)) to add the Binomial probabilities on top of each barplot with points.\nIf the plot looks weird, are your bar heights probabilities or counts? The Binomial probabilities are, as the words says, probabilities so make sure you plot two relatable measures.↩︎\n\n\nHint: Here we need to use the function pbinom(), as we are interested in the cumulative probability of getting a specific result:\npbinom(q = ?, size = ?, prob = ?, lower.tail = ?)\nThe function pbinom() will require you to state four arguments:\n\n\nq: the number of successful outcomes\n\nsize: the number of trials (i.e., the Binomial parameter \\(n\\))\n\nprob: the probability of success (i.e., the Binomial parameter \\(p\\))\n\nlower.tail: if TRUE probabilities are \\(P(X \\leq q)\\), if FALSE probabilities are \\(P(X &gt; q)\\). This is TRUE by default, change to FALSE if you require \\(P(X &gt; q)\\).\n\n↩︎\n\nHint: Compare your estimated probability of buying snacks to that reported in the above survey (which suggested that 49% of movie viewers always buy some form of snack item (i.e., popcorn, drinks, sweets) when watching a movie); and compare your estimated probability of travelling via private transportation to that reported in the above survey (which suggested that 70% of movie viewers viewers travel to the cinema via private transport).↩︎\nData adapted from Lock et al. (2020).↩︎\nTverdal, A., Hjellvik, V. & Selmer, R. Coffee intake and oral–oesophageal cancer: follow-up of 389,624 Norwegian men and women 40–45 years. Br J Cancer 105, 157–161 (2011).↩︎\nTverdal, A., Hjellvik, V. & Selmer, R. Coffee intake and oral–oesophageal cancer: follow-up of 389,624 Norwegian men and women 40–45 years. Br J Cancer 105, 157–161 (2011).↩︎"
  },
  {
    "objectID": "1_07_prob_theory.html",
    "href": "1_07_prob_theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Download the template Rmd file and start your work there. Save your work regularly by clicking File &gt; Save.\n\n\n\n\n\n\nInstructions - Don’t Skip!\n\n\n\n\n\nFormative Report B covers the labs from weeks 7-11 of the DAPR1 course. You’ll need to create a PDF report using RMarkdown, which will be submitted by 12 noon on Friday, 29th November 2024. Remember, these submissions are group-based, so there are no extensions. Expect written formative feedback in week 12.\nYour report should be tailored for a reader with basic statistical knowledge and should not include any references to R code or functions in the main report write-up. Instead, keep the main report focused on text, figures, and tables. All R code should be included in the compulsory Appendix B for reproducibility, which is automatically created for you in the template Rmd file. If you need to add extra tables or figures that don’t fit in the main part of the report, you can use an optional Appendix A. Remember, the main report should be a PDF file and should not exceed six sides of A4 paper, though appendices at the end don’t count towards this limit.\nEnsure to use the default settings for font and page margins in your RMarkdown file. Also, make sure your report title includes your group name: Group NAME.LETTER, and list the exam numbers of all group members in the author section.\nDon’t forget to register for your lab table group on LEARN by navigating to the course LEARN page, clicking on Groups, selecting Labs_1_2_3, finding your group, and clicking Join.\n\n\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. Among all variables, the following will be used for this report:\n\n\n\nLeadStudio: Primary U.S. distributor of the movie \n\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western          \n\n\nIQ1-IQ50: IQ score of each of 50 audience raters (every movie had different raters)\n\nSnacks: How many of the 50 audience raters bought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback.\n\n\nThis week you will only focus on task B1. Below there are sub-steps you need to consider to complete task B1.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the Hollywood movies data into R and give it a helpful name. Data link: https://uoepsy.github.io/data/hollywood_movies_subset.csv\nFind the 3 most frequent movie genres. Next, filter the data to only keep movies from the 3 most frequent genres. This is the dataset you will be working with for the rest of the report.1\n\n\nCreate a variable called Rating where the AudienceScore variable is recoded so that the movies scoring less than or equal to 50 are coded as “Bad” and those scoring over 50 (i.e., &gt;50) are “Good”.2\n\n\n\nEnsure Rating and Genre are coded as factors.3\n\n\n\nCreate a contingency table of Rating by Genre.4\n\n\n\nTransform the table of counts to a relative frequency table.5\n\n\n\nDo the numbers in the table satisfy the requirements of probabilities?6\n\n\n\nInstead of checking the probability requirements manually, add row and column totals to the relative frequency table.7\n\n\n\nVisualise the relative frequency table as a mosaic plot, making sure to add a main title and clear axis titles.8\n\n\n\nDoes your plot have any NAs? If yes, you can drop the NAs before re-plotting, see hint. If there are no NAs, no action is needed but read over the hint anyway.9\n\n\n\n\nIn the introduction section of your report, write up a small introduction to the data and variables.\n\nWhat are the data that you are analysing and where can these be found?\nWhich variables are you using and what do they represent?\nWhat is the type of these variables?\nAre there any missing values in these variables?\nIn the next weeks, update the introduction based on the new variables that you will consider.\n\n\nIn the analysis section of your report, present and write up your results from above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_07_prob_theory.html#formative-report-b",
    "href": "1_07_prob_theory.html#formative-report-b",
    "title": "Probability Theory",
    "section": "",
    "text": "Download the template Rmd file and start your work there. Save your work regularly by clicking File &gt; Save.\n\n\n\n\n\n\nInstructions - Don’t Skip!\n\n\n\n\n\nFormative Report B covers the labs from weeks 7-11 of the DAPR1 course. You’ll need to create a PDF report using RMarkdown, which will be submitted by 12 noon on Friday, 29th November 2024. Remember, these submissions are group-based, so there are no extensions. Expect written formative feedback in week 12.\nYour report should be tailored for a reader with basic statistical knowledge and should not include any references to R code or functions in the main report write-up. Instead, keep the main report focused on text, figures, and tables. All R code should be included in the compulsory Appendix B for reproducibility, which is automatically created for you in the template Rmd file. If you need to add extra tables or figures that don’t fit in the main part of the report, you can use an optional Appendix A. Remember, the main report should be a PDF file and should not exceed six sides of A4 paper, though appendices at the end don’t count towards this limit.\nEnsure to use the default settings for font and page margins in your RMarkdown file. Also, make sure your report title includes your group name: Group NAME.LETTER, and list the exam numbers of all group members in the author section.\nDon’t forget to register for your lab table group on LEARN by navigating to the course LEARN page, clicking on Groups, selecting Labs_1_2_3, finding your group, and clicking Join.\n\n\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres. Among all variables, the following will be used for this report:\n\n\n\nLeadStudio: Primary U.S. distributor of the movie \n\n\nAudienceScore: Audience rating (via Rotten Tomatoes)\n\nGenre: One of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western          \n\n\nIQ1-IQ50: IQ score of each of 50 audience raters (every movie had different raters)\n\nSnacks: How many of the 50 audience raters bought snacks\n\nPrivateTransport: How many of the 50 audience raters reached the cinema via private transportation\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback.\n\n\nThis week you will only focus on task B1. Below there are sub-steps you need to consider to complete task B1.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the Hollywood movies data into R and give it a helpful name. Data link: https://uoepsy.github.io/data/hollywood_movies_subset.csv\nFind the 3 most frequent movie genres. Next, filter the data to only keep movies from the 3 most frequent genres. This is the dataset you will be working with for the rest of the report.1\n\n\nCreate a variable called Rating where the AudienceScore variable is recoded so that the movies scoring less than or equal to 50 are coded as “Bad” and those scoring over 50 (i.e., &gt;50) are “Good”.2\n\n\n\nEnsure Rating and Genre are coded as factors.3\n\n\n\nCreate a contingency table of Rating by Genre.4\n\n\n\nTransform the table of counts to a relative frequency table.5\n\n\n\nDo the numbers in the table satisfy the requirements of probabilities?6\n\n\n\nInstead of checking the probability requirements manually, add row and column totals to the relative frequency table.7\n\n\n\nVisualise the relative frequency table as a mosaic plot, making sure to add a main title and clear axis titles.8\n\n\n\nDoes your plot have any NAs? If yes, you can drop the NAs before re-plotting, see hint. If there are no NAs, no action is needed but read over the hint anyway.9\n\n\n\n\nIn the introduction section of your report, write up a small introduction to the data and variables.\n\nWhat are the data that you are analysing and where can these be found?\nWhich variables are you using and what do they represent?\nWhat is the type of these variables?\nAre there any missing values in these variables?\nIn the next weeks, update the introduction based on the new variables that you will consider.\n\n\nIn the analysis section of your report, present and write up your results from above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_07_prob_theory.html#worked-example",
    "href": "1_07_prob_theory.html#worked-example",
    "title": "Probability Theory",
    "section": "\n2 Worked Example",
    "text": "2 Worked Example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\nWe can filter our data to only include rows of data from Servers A and B, and save this filtered data to a new dataset called “tips2”.\nBecause we want to include servers A and B, but not C, we can use the != (or does not equal) operator.\n\n\nSome common operators include:\n\n\nOperator\nDescription\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\nLEFT %in% RIGHT\nis LEFT a member of RIGHT?\n\n\n\n\ntips2 &lt;- tips |&gt; \n    filter(Server != \"C\")\n\nConsider if, for example, we wanted to recode the Tip variable so that those tipping less than or equal to 15% were coded as “Below” average, and those tipping over 15% were coded as “Above” average (15% being used as below/above average tips cut-off in relation to usual US tipping rates). To do so, we could create a new variable called “Tip_Avg”.\n\ntips2 &lt;- tips2 |&gt;\n    mutate(Tip_Avg = ifelse(PctTip &lt;= 15, 'Below', 'Above'))\n\ntable(tips2$Tip_Avg)\n\n\nAbove Below \n   80    45 \n\n\nNow that we have the variables we want, it would be a good point to make these both factors:\n\ntips2$Server &lt;- factor(tips2$Server)\ntips2$Tip_Avg &lt;- factor(tips2$Tip_Avg)\n\nTo visually represent the distribution of how many customers were served by each server and if they left a below or above average tip, we could Create a contingency table:\n\nfreq_tbl &lt;- table(tips2$Server, tips2$Tip_Avg)\nfreq_tbl\n\n   \n    Above Below\n  A    40    20\n  B    40    25\n\n\nWe could then transform the table of counts above to instead represent a relative frequency table:\n\nrel_freq_tbl &lt;- freq_tbl |&gt;\n    prop.table()\nrel_freq_tbl\n\n   \n    Above Below\n  A  0.32  0.16\n  B  0.32  0.20\n\n\nBefore we interpret our results, we must ensure that the numbers above satisfy the requirements of probabilities. We can do this two ways:\n\n\nManually\nUsing addmargins()\n\n\n\nCheck that all values in the proportions table are greater than or equal 0, all values are less than or equal to 1, and they all sum to 1:\n\nall(rel_freq_tbl &gt;= 0)\n\n[1] TRUE\n\n\n\nall(rel_freq_tbl &lt;= 1)\n\n[1] TRUE\n\n\n\nsum(rel_freq_tbl)\n\n[1] 1\n\n\n\n\nInstead of checking manually, we can use the function addmargins() to check that the probabilities sum to 1:\n\nrel_freq_tbl &lt;- freq_tbl |&gt;\n    prop.table() |&gt;\n    addmargins()\nrel_freq_tbl\n\n     \n      Above Below  Sum\n  A    0.32  0.16 0.48\n  B    0.32  0.20 0.52\n  Sum  0.64  0.36 1.00\n\n\n\n\n\nIn order to visualise our results in a figure, we could use a mosaic plot:\n\nlibrary(ggmosaic)\nmos_plot &lt;- ggplot(tips2) +\n    geom_mosaic(aes(x = product(Server, Tip_Avg), fill = Server)) +  \n    labs(title = \"Association between Servers and Tips\", x = \"Tip\", y = \"Server\")\nmos_plot\n\n\n\n\n\n\nFigure 3: Association between Servers and Tips\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nMore customers tipped above (64%) than below (36%) average. Both Server A and Server B received an equal distribution of tips above average (32%), but server B had a higher proportion of tips below average (20%) in comparison to server A (16%). These associations are visually represented in Figure 3."
  },
  {
    "objectID": "1_07_prob_theory.html#student-glossary",
    "href": "1_07_prob_theory.html#student-glossary",
    "title": "Probability Theory",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\nfilter\n?\n\n\nmutate\n?\n\n\ndrop_na\n?\n\n\nfactor\n?\n\n\ntable\n?\n\n\nifelse\n?\n\n\nprop.table\n?\n\n\naddmargins\n?\n\n\nall\n?\n\n\n|\n?\n\n\ngeom_mosaic\n?"
  },
  {
    "objectID": "1_07_prob_theory.html#footnotes",
    "href": "1_07_prob_theory.html#footnotes",
    "title": "Probability Theory",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHint: use sort(table(DATA$COLUMN)) to find top 3 movie genres, and filter() to select the specific 3 genres.\nExample: For the starwars dataset, we can filter to include only the two most frequent species, Humans and Droids, via the following code:\n\n# sort the frequency table\nsort(table(starwars$species))\n\n# or, for decreasing order, you can use:\nsort(table(starwars$species), decreasing = TRUE)\n\n# keep only the rows where species is Human or Droid\n# in R, or is the vertical bar |\nstarwars2 &lt;- starwars |&gt; \n    filter(species == \"Human\" | species ==  \"Droid\")\n\n↩︎\n\n\nHint: use ifelse() and mutate() functions to create a new ‘Rating’ variable capturing whether movies are good or bad.\nExample: For the starwars dataset, we can create a variable to capture whether characters were short (&lt;180cm) or tall (&gt;180cm) based on their recorded height using the following code:\n\nstarwars2 &lt;- starwars2 |&gt;\n    mutate(size = ifelse(height &lt; 180, 's', 't'))\n\nIn the code above, mutate() changes to data to include a column named size, computed with the computation that follows the equal sign.\nThe ifelse function checks a condition (is height &lt; 180?) and uses the value 's' if the condition is TRUE, and 't' if the condition is FALSE.↩︎\n\n\nHint: use the factor() function.\nExample: For the starwars data, if I wanted to give better labels for my size variable, I could use the following code. Since the names are fine as is for species, I will just specify factor().\n\nstarwars2$size &lt;- factor(starwars2$size,\n                         levels = c(\"s\", \"t\"),\n                         labels = c(\"short\", \"tall\"))\n\nstarwars2$species &lt;- factor(starwars2$species)\n\n↩︎\n\n\nHint: use the table() function, passing to it the two columns that you want to be included in your contingency table.\nExample: For the starwars dataset, if I wanted a contingency table displaying how many humans and droids were short and tall, I could use the following code:\n\nswars_freq_tbl &lt;- table(starwars2$species, starwars2$size)\nswars_freq_tbl\n\n\n        short tall\n  Droid     4    1\n  Human    13   17\n\n\n↩︎\n\n\nHint: use the prop.table() function. You will want to pass your contingency table created above to this.\nExample: For the starwars dataset, if I wanted a relative frequency table, I could use the following code:\n\nswars_rel_freq_tbl &lt;- swars_freq_tbl |&gt;\n    prop.table()\nswars_rel_freq_tbl\n\n\n             short       tall\n  Droid 0.11428571 0.02857143\n  Human 0.37142857 0.48571429\n\n\n↩︎\n\n\nHint: Recall the requirements of probabilities - are the proportions &gt;= 0 or &lt;= 1?; and that the values in the relative frequency table should sum to 1. The all() and sum() functions could be useful here.\nExample: For the starwars dataset, I could check these requirements using the following code:\n\nall(swars_rel_freq_tbl &gt;= 0)\n\n[1] TRUE\n\nall(swars_rel_freq_tbl &lt;= 1)\n\n[1] TRUE\n\nsum(swars_rel_freq_tbl)\n\n[1] 1\n\n\n↩︎\n\n\nHint: Use the addmargins() function.\nExample: For the starwars dataset, I could add this using the following code:\n\nswars_rel_freq_sum &lt;- swars_freq_tbl |&gt;\n    prop.table() |&gt;\n    addmargins()\nswars_rel_freq_sum\n\n\n             short       tall        Sum\n  Droid 0.11428571 0.02857143 0.14285714\n  Human 0.37142857 0.48571429 0.85714286\n  Sum   0.48571429 0.51428571 1.00000000\n\n\n↩︎\n\n\nHint: Make sure to load the ggmosaic() package so that you can specify geom_mosaic() when building your plot. To add a title, as well as x- and y-axis titles, specify labs(title = , x = , y = ).\nExample: For the starwars dataset, I create a mosaic plot using the following code:\n\nlibrary(ggmosaic)\nm_plot &lt;- ggplot(starwars2) +\n    geom_mosaic(aes(x = product(species, size), fill = species)) +\n    labs(title = \"Starwars Mosaic Plot Example Title\", \n         x = \"Size\", \n         y = \"Species\",\n         fill = \"Species\")\nm_plot\n\n\n\n\n\n\nFigure 1: Starwars Mosaic Plot Example Title\n\n\n\n\n↩︎\n\n\nHint: Here you could make sense of the drop_na() function.\nExample: For the starwars dataset, I do have NAs in my ‘size’ variable, so I need to remove these before re-plotting:\n\nstarwars2 &lt;- starwars2 |&gt;\n    drop_na(size) |&gt;\n    mutate(size = factor(size))\n\n\nlibrary(ggmosaic)\nm_plot &lt;- ggplot(starwars2) +\n    geom_mosaic(aes(x = product(species, size), fill = species)) +\n    labs(title = \"Starwars Mosaic Plot Example Title\", \n         x = \"Size\", \n         y = \"Species\",\n         fill = \"Species\")\nm_plot\n\n\n\n\n\n\nFigure 2: Starwars Mosaic Plot Example Title - No NAs\n\n\n\n\n↩︎"
  },
  {
    "objectID": "1_04_relationships.html",
    "href": "1_04_relationships.html",
    "title": "Relationships",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\n\n\n\n\n\nNext week: Submission of Formative report A\nYour group must submit one PDF file for formative report A by 12 noon on Friday the 18th of October 2024 (next week). One person must submit on behalf of the entire group and let the group know when they have submitted by leaving a note on the Group Discussion Space.\nTo submit go to the course Learn page, click “Assessment”, then click “Submit Formative Report A (PDF file only)”.\nNo extensions. As mentioned in the Assessment Information page, no extensions are possible for group-based reports.\nName your submission: Group NUMBER.LETTER Formative A.pdf\n\n\n\n\n\n\n\n\n\nTip: On the kbl() and kable() functions\nThe two functions are equivalent.\nHowever, if you provide a list of tables, we recommend using the kable() function as it allows you to also provide a caption without errors when knitting. We have been made aware that this doesn’t work with kbl() unfortunately.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choiceA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A4. Below there are some guided sub-steps you may want to consider to complete task A4.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nChoose two variables (either both numeric, or one categorical and one numeric). Create a plot displaying the relationship between the two variables.2\n\n\nSummarise the relationship with descriptive statistic(s), depending on the type of the variables.3\n\n\n\nChoose a third variable of categorical type, and visualise how the relationship above varies across this third variable.4\n\n\n\nSummarise with a table of descriptive statistics how the relationship above varies across the third variable.5\n\nOrganise your report to have three sections:\n\n\nIntroduction: where you write a concise introduction to the data for a generic reader\n\nAnalysis: where you present your results, tables, and plots\n\nDiscussion: where you write take-home messages about the data and the insights you discovered\n\n\nKnit the report to PDF, making sure that only text, tables, and plots are visible. Hide the R code chunks so that no R code is visible.\n\n\n\n\n\n\n\nHiding R code and/or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_04_relationships.html#formative-report-a",
    "href": "1_04_relationships.html#formative-report-a",
    "title": "Relationships",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\n\n\n\n\n\nNext week: Submission of Formative report A\nYour group must submit one PDF file for formative report A by 12 noon on Friday the 18th of October 2024 (next week). One person must submit on behalf of the entire group and let the group know when they have submitted by leaving a note on the Group Discussion Space.\nTo submit go to the course Learn page, click “Assessment”, then click “Submit Formative Report A (PDF file only)”.\nNo extensions. As mentioned in the Assessment Information page, no extensions are possible for group-based reports.\nName your submission: Group NUMBER.LETTER Formative A.pdf\n\n\n\n\n\n\n\n\n\nTip: On the kbl() and kable() functions\nThe two functions are equivalent.\nHowever, if you provide a list of tables, we recommend using the kable() function as it allows you to also provide a caption without errors when knitting. We have been made aware that this doesn’t work with kbl() unfortunately.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choiceA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A4. Below there are some guided sub-steps you may want to consider to complete task A4.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nChoose two variables (either both numeric, or one categorical and one numeric). Create a plot displaying the relationship between the two variables.2\n\n\nSummarise the relationship with descriptive statistic(s), depending on the type of the variables.3\n\n\n\nChoose a third variable of categorical type, and visualise how the relationship above varies across this third variable.4\n\n\n\nSummarise with a table of descriptive statistics how the relationship above varies across the third variable.5\n\nOrganise your report to have three sections:\n\n\nIntroduction: where you write a concise introduction to the data for a generic reader\n\nAnalysis: where you present your results, tables, and plots\n\nDiscussion: where you write take-home messages about the data and the insights you discovered\n\n\nKnit the report to PDF, making sure that only text, tables, and plots are visible. Hide the R code chunks so that no R code is visible.\n\n\n\n\n\n\n\nHiding R code and/or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```"
  },
  {
    "objectID": "1_04_relationships.html#worked-example",
    "href": "1_04_relationships.html#worked-example",
    "title": "Relationships",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWe can replace each factor level with a clearer label:\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip      \n A:60   Min.   :  6.70  \n B:65   1st Qu.: 14.30  \n C:32   Median : 16.20  \n        Mean   : 17.89  \n        3rd Qu.: 18.20  \n        Max.   :221.00  \n                        \n\n\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips |&gt; \n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\nWith a bill of 49.6, the tip would be 109.62 dollars:\n\n49.6 * 221 / 100\n\n[1] 109.616\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip &gt; 100] &lt;- NA\n\nConsider, for example, the relationship between bill and tip size. As these are two numerical variables, we visualise the relationship with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\")\n\n\n\n\n\n\nFigure 1: Scatterplot displaying the relationship between bill and tip size\n\n\n\n\n\n\nThe code starts by setting up a blank canvas for plotting the dataset tip, and placing on the x axis the variable Bill and on the y axis the variable Tip:\nggplot(tips, aes(x = Bill, y = Tip))\nThe following line adds a geometric shape to the plot, in this case points:\ngeom_point()\nThe final line uses more informative labels for the reader, setting a label for the x and y axis respectively:\nlabs(x = \"Bill size (in US dollars)\", \n     y = \"Tip size (in US dollars)\")\nThe layers of the plot need to be added to each other with a + symbol at the end of each line, excluding the last one.\nWe can numerically summarise this relationship with the covariance between the two variables:\n\ncov(tips$Bill, tips$Tip)\n\n[1] NA\n\n\nThere are missing values, so the covariance cannot be computed if one or both of the values \\(X, Y\\) is missing.\nTo fix this, we use the option use = \"pairwise.complete.obs\" to tell R to only keep the complete pairs to compute the covariance, i.e. ignoring pairs where at least one number is NA:\n\nround(cov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\"), digits = 2)\n\n[1] 25.96\n\n\nTo investigate the relationship between bill and tip size for those who paid by credit card and those who didn’t we can create faceted scatterplots:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit)\n\n\n\n\n\n\n\n\n\nInstead of facet_wrap() you can use the facet_grid() function. This allows you to facet the rows by a variable, and the columns by another variable, or both:\n\n\nfacet_grid(rows ~ .)\n\n\nfacet_grid(. ~ cols)\n\nfacet_grid(rows ~ cols)\n\nTry replacing the last line of code with:\nfacet_grid(Server ~ Credit)\nWe can improve the labelling by using labeller = \"label_both\", which displays not only the group value as label, but both the variable and value:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\")\n\n\n\n\n\n\nFigure 2: Relationship between bill and tip size by paying method\n\n\n\n\nSimilarly, you can compute grouped covariances via:\n\nlibrary(kableExtra)\ntips |&gt;\n    group_by(Credit) |&gt;\n    summarise(Cov = cov(Bill, Tip, use = \"pairwise.complete.obs\")) |&gt; \n    kable(digits = 2, booktabs = TRUE,\n        caption = \"Relationship between bill and tip size by credit card usage\")\n\n\n\n\nTable 1: Relationship between bill and tip size by credit card usage\n\n\n\n\nCredit\nCov\n\n\n\nNo\n15.38\n\n\nYes\n37.68\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample write-up\n\n\n\nFigure 1 highlights a strong positive relationship between bill and tip size (in US dollars). The covariance between the two variables is 25.96 squared dollars. The relationship between bill and tip size is stronger for those who paid by credit card than those who did not, as highlighted by Figure 2 and Table 1, where the covariance between the two variables is 37.68 for those that used a credit card and 15.38 for those that did not."
  },
  {
    "objectID": "1_04_relationships.html#helpful-references-on-relationships-between-variables",
    "href": "1_04_relationships.html#helpful-references-on-relationships-between-variables",
    "title": "Relationships",
    "section": "\n3 Helpful references on relationships between variables",
    "text": "3 Helpful references on relationships between variables\nIn the following, Cat = Categorical and Num = Numerical.\n\n\n\n\n\n\nRelationships between two variables\n\n\n\n\n\n\n\nCat-Cat\nNum-Cat\nNum-Num\n\n\n\nVisualise with a mosaic plot:\n\nlibrary(ggmosaic)\nggplot(tips)+\n    geom_mosaic(aes(x = product(Credit, Server), fill=Credit))\n\n\n\n\n\n\n\nSummarise with a contingency table:\n\ntips |&gt;\n    select(Credit, Server) |&gt;\n    table() |&gt;\n    kable(booktabs = TRUE)\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n39\n50\n17\n\n\nYes\n21\n15\n15\n\n\n\n\n\n\n\nBoxplot\n\nggplot(tips, aes(x = Credit, y = Tip)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nor grouped histogram\n\nggplot(tips, aes(x = Tip)) +\n    geom_histogram(color='white') +\n    facet_wrap(~Credit)\n\n\n\n\n\n\n\nor coloured density plot\n\nggplot(tips, aes(x = Tip, colour = Credit)) +\n    geom_density()\n\n\n\n\n\n\n\nSummarise via a grouped table of descriptive statistics:\n\ntips |&gt;\n    group_by(Credit) |&gt;\n    summarise(N = n(),\n              M = mean(Tip),\n              SD = sd(Tip),\n              Med = median(Tip),\n              IQR = IQR(Tip)) |&gt;\n    kable(digits = 2, booktabs = TRUE)\n\n\n\n\n\nCredit\nN\nM\nSD\nMed\nIQR\n\n\n\nNo\n106\n3.25\n1.93\n3.0\n2.00\n\n\nYes\n50\n4.99\n2.77\n4.1\n3.04\n\n\n\n\n\n\n\nVisualise with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point()\n\n\n\n\n\n\n\nSummarise with the covariance:\n\ncov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\") |&gt;\n    round(digits = 2)\n\n[1] 25.96\n\n\nThere’s no need to put just one number into a table, write it up in a sentence.\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships between three variables\n\n\n\n\n\n\n\nCat-Cat-Cat\nCat-Num-Cat\nNum-Num-Cat\n\n\n\nVisalise with a faceted mosaic plot:\n\nggplot(tips)+\n    geom_mosaic(aes(x = product(Credit, Server), fill=Credit)) +\n    facet_wrap(~Day, scales = \"free\") # scales = \"free\" shows the x-axis on each plot\n\n\n\n\n\n\n\nSummarise with grouped frequency tables:\n\nmon &lt;- tips |&gt;\n    filter(Day == \"Monday\") |&gt;\n    select(Credit, Server) |&gt;\n    table()\n\ntue &lt;- tips |&gt;\n    filter(Day == \"Tuesday\") |&gt;\n    select(Credit, Server) |&gt;\n    table()\n\nwed &lt;- tips |&gt;\n    filter(Day == \"Wednesday\") |&gt;\n    select(Credit, Server) |&gt;\n    table()\n\nthu &lt;- tips |&gt;\n    filter(Day == \"Thursday\") |&gt;\n    select(Credit, Server) |&gt;\n    table()\n\nfri &lt;- tips |&gt;\n    filter(Day == \"Friday\") |&gt;\n    select(Credit, Server) |&gt;\n    table()\n\n\nkable(list(mon, tue, wed, thu, fri), booktabs = TRUE,\n      caption = \"Frequency tables of Credit by Server for each day of the week (Mon to Fri)\")\n\n\n\n\nFrequency tables of Credit by Server for each day of the week (Mon to Fri)\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n0\n14\n0\n\n\nYes\n0\n6\n0\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n3\n0\n2\n\n\nYes\n6\n0\n2\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n18\n16\n7\n\n\nYes\n10\n5\n6\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n10\n11\n3\n\n\nYes\n3\n3\n6\n\n\n\n\n\n\nA\nB\nC\n\n\n\nNo\n8\n9\n5\n\n\nYes\n2\n1\n1\n\n\n\n\n\n\n\n\n\nVisualise using a faceted boxplot:\n\nggplot(tips)+\n    geom_boxplot(aes(x = Credit, y = Tip)) + \n    facet_wrap(~Day)\n\n\n\n\n\n\n\nor faceted and coloured density plots:\n\nggplot(tips)+\n    geom_density(aes(x = Tip, color = Credit)) + \n    facet_wrap(~Day)\n\n\n\n\n\n\n\nSummarise with a table of descriptive statistics grouped by the categorical variables:\n\ntips |&gt;\n    group_by(Credit, Day) |&gt;\n    summarise(M = mean(Tip, na.rm=TRUE),\n              SD = sd(Tip, na.rm=TRUE)) |&gt;\n    kable(digits = 2, booktabs = TRUE)\n\n\n\n\n\nCredit\nDay\nM\nSD\n\n\n\nNo\nMonday\n2.97\n1.36\n\n\nNo\nTuesday\n4.97\n3.86\n\n\nNo\nWednesday\n2.92\n1.64\n\n\nNo\nThursday\n3.75\n1.92\n\n\nNo\nFriday\n3.10\n2.07\n\n\nYes\nMonday\n5.02\n2.95\n\n\nYes\nTuesday\n3.90\n2.23\n\n\nYes\nWednesday\n5.43\n3.51\n\n\nYes\nThursday\n4.99\n1.76\n\n\nYes\nFriday\n4.84\n1.37\n\n\n\n\n\n\n\nScatterplot\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    facet_wrap(~Day)\n\n\n\n\n\n\n\nSummarise via a grouped table of descriptive statistics:\n\ntips |&gt;\n    group_by(Day) |&gt;\n    summarise(\n        Cov = cov(Bill, Tip, use = \"pairwise.complete.obs\")\n    ) |&gt;\n    kable(digits = 2, booktabs = TRUE)\n\n\n\n\n\nDay\nCov\n\n\n\nMonday\n23.46\n\n\nTuesday\n37.05\n\n\nWednesday\n34.76\n\n\nThursday\n18.92\n\n\nFriday\n12.26"
  },
  {
    "objectID": "1_04_relationships.html#student-glossary",
    "href": "1_04_relationships.html#student-glossary",
    "title": "Relationships",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_density\n?\n\n\ngeom_boxplot\n?\n\n\ngeom_point\n?\n\n\ngeom_mosaic\n?\n\n\nfacet_wrap\n?\n\n\nfacet_grid\n?\n\n\ngroup_by\n?\n\n\nsummarise\n?\n\n\ncor\n?\n\n\nround\n?"
  },
  {
    "objectID": "1_04_relationships.html#footnotes",
    "href": "1_04_relationships.html#footnotes",
    "title": "Relationships",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: Some possibilities are, among many others:\n\nCategorical-Numeric: faceted geom_histogram(), coloured geom_density(), or geom_boxplot()\n\nNumeric-Numeric: geom_point()\n\n\n↩︎\n\n\nHint:\nDepending on the type of the variables some of these functions may be useful: group_by(), n(), mean(), sd(), cov()\nStop and think. If the result is NA, what could have caused that?\n\nFor some functions, this is solved by adding the argument na.rm = TRUE.\nFor cov() you need the argument use = \"pairwise.complete.obs\". This is because the covariance between a pair of variables \\(X, Y\\) cannot be computed if one or both the values in a product is NA. The argument above tells R to only use pairs with complete observations, i.e. no missing values.\n\n↩︎\n\nHint: the function facet_wrap() may be useful with a categorical variable.↩︎\n Hint: you may want to use functions such as group_by, summarise, n, mean, sd, cov↩︎"
  },
  {
    "objectID": "1_02_categorical_data.html",
    "href": "1_02_categorical_data.html",
    "title": "Categorical data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structureA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A2. Below there are some guided sub-steps you may want to consider to complete task A2.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nSelecting a subset of columns\n\n\n\n\n\nConsider a table of toy data comprising a participant identifier (id: 1 to 5), the participant age, the course (A or B) they are enrolled into, and their height:\n\n# This code creates some toy data. \n#   tibble() creates a dataset\n#   each column is specied as column_name = values\n#   the function c() is used to concatenate the values going into a column\ntoy_data &lt;- tibble(\n    id = 1:5,\n    age = c(18, 20, 25, 22, 19),\n    course = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n    height = c(171, 180, 168, 193, 174)\n)\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo select columns to keep you can either (1) specify the range from:to, if the columns are sequential, or (2) list the columns one by one.\n\n\nRange from:to\nListing all columns\n\n\n\nIf the columns you want to keep are sequential, you can just specify the first and last by using numbers:\n\ntoy_data |&gt;\n    select(1:3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nor using their names:\n\ntoy_data |&gt;\n    select(id:course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nEither option keeps columns id up to course.\n\n\nIf the columns you want to keep are not in sequential order, you have to list all of the columns you want to keep. This can be tedious if you have many.\nYou can do so using numbers:\n\ntoy_data |&gt;\n    select(1, 2, 3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nOr column names:\n\ntoy_data |&gt;\n    select(id, age, course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\n\n\n\nHowever, if you check the data in toy_data, those didn’t change. The result of the above computation was only printed to the screen but not stored.\n\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo store it, we need to assign the result to an object. By using the same name toy_data we overwrite the data:\n\ntoy_data &lt;- toy_data |&gt;\n    select(id:course)\n\n\ntoy_data\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nBy doing the above, we have overwritten the data stored in toy_data with the selected columns.\n\n\n\n\nIn Formative Report A you will only work with the variables (i.e., columns) Movie up to, and including, Year. Overwrite the data to only include the first 15 variables.2\nCreate a plot displaying the frequency distribution of movie genres.3\n\n\nCreate a plot displaying the frequency distribution of the lead studios.4\nThinking question: Would it make sense to plot the frequency distribution of movie titles (Movie)?5\n\n\n\n\n\n\n\nTip\n\n\n\nBefore applying a function to your data, you should always ask yourself if what you are about to do is going to convey insights about the data, as opposed to directly looking at the data.\nThe goal of data analysis is to to go from a multitude of values to insights that provide actionable information.\n\n\n\nDescribe the distribution of movie genres. You may want to include both the frequency and the percentage frequency.6\n\n\n\n\n\n\n\n\nAn alternative to count()\n\n\n\n\n\nConsider the code below, which creates a table of absolute frequencies (or counts):\n\ntoy_data |&gt;\n    count(course)\n\n# A tibble: 2 × 2\n  course     n\n  &lt;chr&gt;  &lt;int&gt;\n1 A          3\n2 B          2\n\n\nAn alternative to the above involves using the group_by(), summarise(), and n() functions from tidyverse:\n\ntoy_data |&gt;\n    group_by(course) |&gt;\n    summarise(\n        n = n(),\n    )\n\n# A tibble: 2 × 2\n  course     n\n  &lt;chr&gt;  &lt;int&gt;\n1 A          3\n2 B          2\n\n\nLine 1 takes toy_data and then does something to it (|&gt;).\nLine 2 specifies to do any computations which follow separately for each course (the groups).\nLines 3-5 summarise the data by creating a column named n (the name goes before the = sign) that stores the sizes of each group. The group size is returned by the tidyverse function n().\n\n\n\n\nDescribe the distribution of lead studios. You may want to include both the frequency and the percentage frequency.7\nWhat is the most common genre and the most common lead studio?8\nFormat your frequency tables properly using the kbl() function from the kableExtra package.9\nSummarise your findings in the Analysis section. For each categorical variable, show either the frequency table or frequency plot in the Analysis section, not both. This avoids duplication of information."
  },
  {
    "objectID": "1_02_categorical_data.html#formative-report-a",
    "href": "1_02_categorical_data.html#formative-report-a",
    "title": "Categorical data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structureA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A2. Below there are some guided sub-steps you may want to consider to complete task A2.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nSelecting a subset of columns\n\n\n\n\n\nConsider a table of toy data comprising a participant identifier (id: 1 to 5), the participant age, the course (A or B) they are enrolled into, and their height:\n\n# This code creates some toy data. \n#   tibble() creates a dataset\n#   each column is specied as column_name = values\n#   the function c() is used to concatenate the values going into a column\ntoy_data &lt;- tibble(\n    id = 1:5,\n    age = c(18, 20, 25, 22, 19),\n    course = c(\"A\", \"B\", \"A\", \"B\", \"A\"),\n    height = c(171, 180, 168, 193, 174)\n)\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo select columns to keep you can either (1) specify the range from:to, if the columns are sequential, or (2) list the columns one by one.\n\n\nRange from:to\nListing all columns\n\n\n\nIf the columns you want to keep are sequential, you can just specify the first and last by using numbers:\n\ntoy_data |&gt;\n    select(1:3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nor using their names:\n\ntoy_data |&gt;\n    select(id:course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nEither option keeps columns id up to course.\n\n\nIf the columns you want to keep are not in sequential order, you have to list all of the columns you want to keep. This can be tedious if you have many.\nYou can do so using numbers:\n\ntoy_data |&gt;\n    select(1, 2, 3)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nOr column names:\n\ntoy_data |&gt;\n    select(id, age, course)\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\n\n\n\nHowever, if you check the data in toy_data, those didn’t change. The result of the above computation was only printed to the screen but not stored.\n\ntoy_data\n\n# A tibble: 5 × 4\n     id   age course height\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1    18 A         171\n2     2    20 B         180\n3     3    25 A         168\n4     4    22 B         193\n5     5    19 A         174\n\n\nTo store it, we need to assign the result to an object. By using the same name toy_data we overwrite the data:\n\ntoy_data &lt;- toy_data |&gt;\n    select(id:course)\n\n\ntoy_data\n\n# A tibble: 5 × 3\n     id   age course\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; \n1     1    18 A     \n2     2    20 B     \n3     3    25 A     \n4     4    22 B     \n5     5    19 A     \n\n\nBy doing the above, we have overwritten the data stored in toy_data with the selected columns.\n\n\n\n\nIn Formative Report A you will only work with the variables (i.e., columns) Movie up to, and including, Year. Overwrite the data to only include the first 15 variables.2\nCreate a plot displaying the frequency distribution of movie genres.3\n\n\nCreate a plot displaying the frequency distribution of the lead studios.4\nThinking question: Would it make sense to plot the frequency distribution of movie titles (Movie)?5\n\n\n\n\n\n\n\nTip\n\n\n\nBefore applying a function to your data, you should always ask yourself if what you are about to do is going to convey insights about the data, as opposed to directly looking at the data.\nThe goal of data analysis is to to go from a multitude of values to insights that provide actionable information.\n\n\n\nDescribe the distribution of movie genres. You may want to include both the frequency and the percentage frequency.6\n\n\n\n\n\n\n\n\nAn alternative to count()\n\n\n\n\n\nConsider the code below, which creates a table of absolute frequencies (or counts):\n\ntoy_data |&gt;\n    count(course)\n\n# A tibble: 2 × 2\n  course     n\n  &lt;chr&gt;  &lt;int&gt;\n1 A          3\n2 B          2\n\n\nAn alternative to the above involves using the group_by(), summarise(), and n() functions from tidyverse:\n\ntoy_data |&gt;\n    group_by(course) |&gt;\n    summarise(\n        n = n(),\n    )\n\n# A tibble: 2 × 2\n  course     n\n  &lt;chr&gt;  &lt;int&gt;\n1 A          3\n2 B          2\n\n\nLine 1 takes toy_data and then does something to it (|&gt;).\nLine 2 specifies to do any computations which follow separately for each course (the groups).\nLines 3-5 summarise the data by creating a column named n (the name goes before the = sign) that stores the sizes of each group. The group size is returned by the tidyverse function n().\n\n\n\n\nDescribe the distribution of lead studios. You may want to include both the frequency and the percentage frequency.7\nWhat is the most common genre and the most common lead studio?8\nFormat your frequency tables properly using the kbl() function from the kableExtra package.9\nSummarise your findings in the Analysis section. For each categorical variable, show either the frequency table or frequency plot in the Analysis section, not both. This avoids duplication of information."
  },
  {
    "objectID": "1_02_categorical_data.html#worked-example",
    "href": "1_02_categorical_data.html#worked-example",
    "title": "Categorical data",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\n\n\n\nWe load the tidyverse package as we will use the functions read_csv and glimpse from this package.\n\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\n\n\n\nread_csv is the function to read CSV (comma separated values) files. Once we have read the file, it is stored into an object called tips using the arrow (&lt;-).\n\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows the top 6 rows of data. Use the n = ... option to change the default behaviour, e.g. head(&lt;data&gt;, n = 10).\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package and is used to check the type of each variable.\nWe can use better and more descriptive labels for the categorical variables:\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\n\n\ntips$Day, i.e. the column Day within the data tips, is converted to a factor in R (the appropriate storage mode for categorical variables). Furthermore, it replaces the level “m” with the new label “Monday”, “t” with the new label “Tuesday”, and so on.\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\nWe don’t have better labels for Server (current values A , B, or C), so we will just convert it to a factor by keeping the current levels:\n\ntips$Server &lt;- factor(tips$Server)\n\nCheck the relabelled columns:\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;fct&gt; No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;fct&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server &lt;fct&gt; A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\n\n\n\n\n\n\n\n\nLast week, we also saw that if someone tipped more than 100% of the bill size, it was likely a data input error and we decided to replace that value with NA (not available):\n\n\nThe mutate function takes as arguments:\n\ncolumn name\n\n=\n\nhow to compute that column\n\n\nThe syntax for ifelse is:\nifelse(test_condition, \n       value_if_true, \n       avalue_if_false)\n\n\ntips &lt;- tips |&gt;\n    mutate(\n        PctTip = ifelse(PctTip &gt; 100, NA, PctTip)\n    )\n\nThis displays the frequency distribution of credit card payers:\n\nplt_credit &lt;- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\", y = \"Count\")\nplt_credit\n\n\n\n\n\n\n\nYou can even flip the coordinates, if you wish to, using the coord_flip() function:\n\nggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paid by credit card?\") +\n    coord_flip()\n\n\n\n\n\n\n\nYou can use the patchwork package to place graphs side by side. Simply create an object for each graph, and concatenate the objects with | for horizontal concatenation and / for vertical concatenation of graphs. You can even combine this by using parentheses, e.g. (plot1 | plot2) / (plot3 | plot4) for 2 rows and 2 columns.\n\n\nRun install.packages(\"patchwork\") first in your R console\nWe can display the frequency distribution of all the categorical variables: Credit, Day, and Server:\n\n\n\n\n\n\nRotate x-axis labels\n\n\n\n\n\nTo rotate x-axis labels by 90 degrees, you can use this code:theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\nTo rotate the labels by 45 degrees, you can use: theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\nDon’t worry, no one remembers it. People always google “rotate x-axis labels ggplot” to find it.\n\n\n\n\nlibrary(patchwork)\n\nplt1 &lt;- ggplot(tips, aes(x = Credit)) +\n    geom_bar() +\n    labs(x = \"Paird by credit card?\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt2 &lt;- ggplot(tips, aes(x = Day)) +\n    geom_bar() +\n    labs(x = \"Day of week\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt3 &lt;- ggplot(tips, aes(x = Server)) +\n    geom_bar() +\n    labs(y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt1 | plt2 | plt3\n\n\n\n\n\n\n\n\n\n\n\n\n\nSorting the barplot\n\n\n\n\n\nIf wanted, you can sort the bars in order of frequency by using the fct_infreq() function.\nIn the last plot, plt3, this involves changing the first row from ggplot(tips, aes(x = Server)) to ggplot(tips, aes(x = fct_infreq(Server))).\nIn these plot I have preferred not to do so, as changing the order of levels may confuse the reader when the factors have easily understood ordering: credit (No/Yes), day (Mon,Tue,Wed,Thu,Fri), server (A,B,C)\n\nlibrary(patchwork)\n\nplt1 &lt;- ggplot(tips, aes(x = fct_infreq(Credit))) +\n    geom_bar() +\n    labs(x = \"Paird by credit card?\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt2 &lt;- ggplot(tips, aes(x = fct_infreq(Day))) +\n    geom_bar() +\n    labs(x = \"Day of week\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt3 &lt;- ggplot(tips, aes(x = fct_infreq(Server))) +\n    geom_bar() +\n    labs(x = \"Server\", y = \"Count\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\nplt1 | plt2 | plt3\n\n\n\n\n\n\n\n\n\n\nA frequency table can be obtained using:\n\ntbl_credit &lt;- tips |&gt;\n    count(Credit) |&gt;\n    mutate(\n        Percent = round((n / sum(n)) * 100, digits = 2)\n    )\ntbl_credit\n\n# A tibble: 2 × 3\n  Credit     n Percent\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;\n1 No       106    67.5\n2 Yes       51    32.5\n\n\n\ntbl_day &lt;- tips |&gt;\n    count(Day) |&gt;\n    mutate(\n        Percent = round((n / sum(n)) * 100, digits = 2)\n    )\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n Percent\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Monday       20   12.7 \n2 Tuesday      13    8.28\n3 Wednesday    62   39.5 \n4 Thursday     36   22.9 \n5 Friday       26   16.6 \n\n\n\ntbl_server &lt;- tips |&gt;\n    count(Server) |&gt;\n    mutate(\n        Percent = round((n / sum(n)) * 100, digits = 2)\n    )\ntbl_server\n\n# A tibble: 3 × 3\n  Server     n Percent\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;\n1 A         60    38.2\n2 B         65    41.4\n3 C         32    20.4\n\n\nYou can create nice tables with the kbl command from the kableExtra package.\n\n\nRun install.packages(\"kableExtra\") first in your R console\n\nlibrary(kableExtra)\n\nkbl(list(tbl_credit, tbl_day, tbl_server), booktabs = TRUE)\n\n\n\n\n\n\n\n\nPaid with a credit card {#tbl-anonymous-5579015-1}\n\nCredit\nn\nPercent\n\n\n\nNo\n106\n67.52\n\n\nYes\n51\n32.48\n\n\n\n\nDay of the week {#tbl-anonymous-5579015-2}\n\nDay\nn\nPercent\n\n\n\nMonday\n20\n12.74\n\n\nTuesday\n13\n8.28\n\n\nWednesday\n62\n39.49\n\n\nThursday\n36\n22.93\n\n\nFriday\n26\n16.56\n\n\n\n\nServer {#tbl-anonymous-5579015-3}\n\nServer\nn\nPercent\n\n\n\nA\n60\n38.22\n\n\nB\n65\n41.40\n\n\nC\n32\n20.38\n\n\n\n\n\n\nFrequency tables of categorical variables\n\n\n\n\n\n\n\n\n\n\nHow do I arrange by descending frequency order?\n\n\n\n\n\nAdd arrange(desc(&lt;column_of_freq&gt;)). For example:\n\ntbl_day &lt;- tips |&gt;\n    count(Day) |&gt;\n    mutate(\n        Percent = round((n / sum(n)) * 100, digits = 2)\n    ) |&gt;\n    arrange(desc(n))\ntbl_day\n\n# A tibble: 5 × 3\n  Day           n Percent\n  &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Wednesday    62   39.5 \n2 Thursday     36   22.9 \n3 Friday       26   16.6 \n4 Monday       20   12.7 \n5 Tuesday      13    8.28\n\n\nIf you just did arrange(n), it would be in ascending order.\n\n\n\n\n\n\n\n\n\nHow do I rename the frequency and percent columns?\n\n\n\n\n\nYou can specify a different name for the column of counts by using name = \"new name\". If you don’t specify it, the default is n.\nYou can specify any valid name for the percentages inside of mutate.\nFor example:\n\ntbl_day &lt;- tips |&gt;\n    count(Day, name = \"Freq\") |&gt;\n    mutate(\n        Perc = round((Freq / sum(Freq)) * 100, digits = 2)\n    ) |&gt;\n    arrange(desc(Freq))\ntbl_day\n\n# A tibble: 5 × 3\n  Day        Freq  Perc\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Wednesday    62 39.5 \n2 Thursday     36 22.9 \n3 Friday       26 16.6 \n4 Monday       20 12.7 \n5 Tuesday      13  8.28\n\n\n\n\n\nFrom the univariate distribution (or marginal distribution) of each categorical variable we see that the most common payment method was not a credit card, and the most common day of the week to dine at that restaurant was Wednesday, followed by Thursday and Friday. Finally, most parties were waited on by server B.\n\n\nThe mode of a variable is the value that appears most often.\nThe term comes from the French expression “à la mode”, i.e. in fashion. If you think about it, something is considered to be in fashion if it’s worn very often.\n\n\n\n\n\n\nReferencing tables\n\n\n\n\n\nTo reference a table in text you first give the code chunk a unique label, e.g. tableLabel, and a caption to the table, e.g. “My table caption is this”\n```{r tableLabel}\ntbl_credit |&gt;\n    kbl(digits = 2, booktabs = TRUE, caption = \"My table caption is this\")\n```\nThis creates\n\n\n\nTable 1: My table caption is this\n\n\n\nMy table caption is this\n\nCredit\nn\nPercent\n\n\n\nNo\n106\n67.52\n\n\nYes\n51\n32.48\n\n\n\n\n\n\n\n\nThen you reference it in text using \\@ref(tab:tableLabel). For example:\n\nTable \\@ref(tab:tableLabel) displays etc.\n\nWhich renders as:\nTable 1 displays etc."
  },
  {
    "objectID": "1_02_categorical_data.html#student-glossary",
    "href": "1_02_categorical_data.html#student-glossary",
    "title": "Categorical data",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\nfactor\n?\n\n\n|&gt;\n?\n\n\ngeom_bar\n?\n\n\nlabs\n?\n\n\ncount\n?\n\n\nmutate\n?\n\n\nsum\n?\n\n\nround\n?\n\n\ncoord_flip\n?\n\n\nkbl\n?\n\n\narrange\n?\n\n\ndesc\n?"
  },
  {
    "objectID": "1_02_categorical_data.html#footnotes",
    "href": "1_02_categorical_data.html#footnotes",
    "title": "Categorical data",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams.↩︎\nHint: the select() function from tidyverse.For an explanation of the function, did you read the drop down “Selecting a subset of columns”?↩︎\n\nHint: we display categorical variables with barplots. Consider the geom_bar() function.\nExample: For the toy_data from above, the frequency distribution of course enrollment is:\n\nggplot(toy_data, aes(x = course)) +\n    geom_bar() +\n    labs(x = \"Enrollment per course\", y = \"Frequency\")\n\nLine 1 sets the plotting canvas: it tells R to make a plot of the data toy_data and to put the column course on the x-axis of the plot.\nLine 2 tells R to plot the data as a frequency barplot.\nLine 3 provides user-friendly labels for the x-axis and y-axis.\n\n\n\n\n\n\n\n\n↩︎\n\n Hint: similar to above, change the column to LeadStudio.↩︎\n Hint: what would be the height of each bar? Would adding such plot to a report give any useful insights to decision makers? In the data, Movie stores the movie titles. This variable is what is known in statistics as an “identifier” or “ID” variable as it uniquely identifies each unit in the study. If your study involved several participants, your ID would be the unique participant identifier. Plotting the frequency distribution of an identifier variable doesn’t convey insights or summarise the data as all vertical bars in the frequency plot will have height equal 1.↩︎\n\nHint: We describe categorical variables with frequency distributions.\nConsider using the count() function from tidyverse and mutate() for adding percentages.\nExample:\n\ntoy_data |&gt;\n    count(course) |&gt;\n    mutate(\n        Percent = round(n / sum(n) * 100, digits = 2)\n    )\n\n# A tibble: 2 × 3\n  course     n Percent\n  &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt;\n1 A          3      60\n2 B          2      40\n\n\nAdvanced: count(course) is equivalent to group_by(course) |&gt; summarise(n = n()). See the box below for more details.↩︎\n\n Hint: similar to above, but replacing Genre with LeadStudio↩︎\n Hint: What is the mode of Genre and LeadStudio? In other words, which category in each of those frequency distributions has the highest frequency? Tip: You may want to order the barplots and/or frequency tables in descending order to help you identify the mode.\nFor barplots, use aes(x = fct_infreq(VARIABLE)) instead of aes(x = VARIABLE). The function fct_infreq() orders a categorical variable according to the frequencies.\nFor tables, add |&gt; arrange(desc(n)) at the end of the table code.↩︎\nHint: See the worked example below.↩︎"
  },
  {
    "objectID": "1_01_design_and_data.html",
    "href": "1_01_design_and_data.html",
    "title": "Research design & data",
    "section": "",
    "text": "Practicalities:\n\nSit in tables of at most 5 students per table.\nCheck your table name and register for that group on LEARN. To do so, navigate to the course LEARN page &gt; click Groups &gt; select Labs_1_2_3 &gt; find your group &gt; click Join.\nEach week, one person within a table will be the “driver”, responsible for typing on the PC, while the others are the “navigators”, responsible for spotting typos and suggesting the strategy/code the driver should use to answer tasks. This is based on novel pedagogical practice called pair-programming.\nThe person who is the “driver” will rotate every week, so that everyone experiences being a driver at least once.\n\nDriver:\n\nLog into the RStudio Server Online: go to the course LEARN page &gt; click Quick Links &gt; click RStudio Server Online (by Noteable) &gt; select RStudio from the dropdown &gt; click Start.\nClick here to download the template Rmd file\nUpload the template Rmd file to the RStudio server and add your work to that file.\nSave your work regularly by clicking File &gt; Save.\nAt the end of the lab, download the Rmd file and then share it on your Group Discussion Space.\n\n\nNavigators:\n\nHave the lab tasks open and be ready to tell the driver what to do.\n\n\nIf you have questions, raise your hand and a tutor will come over to help!\n\nOverview:\n\nAt the start of each teaching block (recall that each block is five weeks long), you will be given a dataset that you will use throughout the labs of that block. By the end of each block your group should have produced a report that analyses the given dataset.\nThe reports are due by:\n\nFormative Report A (Block 1): 12 noon, Friday the 17th of October 2025.\nFormative Report B (Block 2): 12 noon, Friday the 28th of November 2025.\nFormative Report C (Block 3): 12 noon, Friday the 13th of February 2026.\nAssessed Report (Block 4): 12 noon, Friday the 27th of March 2026.\n\n\nYou will be required to submit a PDF file, not the Rmd file used to create the PDF.\nNo extensions will be available for the formative reports.\nYou will receive written formative feedback on each of the formative reports the week after the report due date. This will be signposted via announcements.\n\nLab help and support:\n\nThe lab is structured to provide various levels of support.\nWhen attending a lab, you should put away distractions and prioritise completing that week’s tasks to make the most of the help available.\nIf you are unsure or stuck at any point, you should make use of all the available help:\n\nRaise your hand to get help from a tutor;\nHover your mouse on the superscript number to get a quick hint. The hints may sometimes show multiple equivalent ways of getting an answer - you just need one way;\nScroll down to the Worked Example section, where you can read through a worked example.\nEven if you don’t use the Worked Example to complete the tasks, ensure you review and study its content during your independent study time."
  },
  {
    "objectID": "1_01_design_and_data.html#setup",
    "href": "1_01_design_and_data.html#setup",
    "title": "Research design & data",
    "section": "",
    "text": "Practicalities:\n\nSit in tables of at most 5 students per table.\nCheck your table name and register for that group on LEARN. To do so, navigate to the course LEARN page &gt; click Groups &gt; select Labs_1_2_3 &gt; find your group &gt; click Join.\nEach week, one person within a table will be the “driver”, responsible for typing on the PC, while the others are the “navigators”, responsible for spotting typos and suggesting the strategy/code the driver should use to answer tasks. This is based on novel pedagogical practice called pair-programming.\nThe person who is the “driver” will rotate every week, so that everyone experiences being a driver at least once.\n\nDriver:\n\nLog into the RStudio Server Online: go to the course LEARN page &gt; click Quick Links &gt; click RStudio Server Online (by Noteable) &gt; select RStudio from the dropdown &gt; click Start.\nClick here to download the template Rmd file\nUpload the template Rmd file to the RStudio server and add your work to that file.\nSave your work regularly by clicking File &gt; Save.\nAt the end of the lab, download the Rmd file and then share it on your Group Discussion Space.\n\n\nNavigators:\n\nHave the lab tasks open and be ready to tell the driver what to do.\n\n\nIf you have questions, raise your hand and a tutor will come over to help!\n\nOverview:\n\nAt the start of each teaching block (recall that each block is five weeks long), you will be given a dataset that you will use throughout the labs of that block. By the end of each block your group should have produced a report that analyses the given dataset.\nThe reports are due by:\n\nFormative Report A (Block 1): 12 noon, Friday the 17th of October 2025.\nFormative Report B (Block 2): 12 noon, Friday the 28th of November 2025.\nFormative Report C (Block 3): 12 noon, Friday the 13th of February 2026.\nAssessed Report (Block 4): 12 noon, Friday the 27th of March 2026.\n\n\nYou will be required to submit a PDF file, not the Rmd file used to create the PDF.\nNo extensions will be available for the formative reports.\nYou will receive written formative feedback on each of the formative reports the week after the report due date. This will be signposted via announcements.\n\nLab help and support:\n\nThe lab is structured to provide various levels of support.\nWhen attending a lab, you should put away distractions and prioritise completing that week’s tasks to make the most of the help available.\nIf you are unsure or stuck at any point, you should make use of all the available help:\n\nRaise your hand to get help from a tutor;\nHover your mouse on the superscript number to get a quick hint. The hints may sometimes show multiple equivalent ways of getting an answer - you just need one way;\nScroll down to the Worked Example section, where you can read through a worked example.\nEven if you don’t use the Worked Example to complete the tasks, ensure you review and study its content during your independent study time."
  },
  {
    "objectID": "1_01_design_and_data.html#formative-report-a",
    "href": "1_01_design_and_data.html#formative-report-a",
    "title": "Research design & data",
    "section": "\n1 Formative Report A",
    "text": "1 Formative Report A\n\n\n\n\n\n\nInstructions - Don’t Skip!\n\n\n\n\n\nFormative Report A covers the labs from weeks 1-5 of the DAPR1 course in semester 1. You’ll need to create a PDF report using RMarkdown, which will be submitted by 12 noon on Friday, 17th October 2025. No extensions are available as this is a formative report. Expect written formative feedback in week 6 of semester 1.\nYour report should be tailored for a reader with basic statistical knowledge and should not include any references to R code or functions in the main report write-up. Instead, keep the main report focused on text, figures, and tables. All R code should be included in the compulsory Appendix B for reproducibility, which is automatically created for you in the template Rmd file (do not edit that section). If you need to add extra tables or figures that don’t fit in the main part of the report, you can use an optional Appendix A. Remember, the main report should be a PDF file and should not exceed six sides of A4 paper, though appendices at the end don’t count towards this limit.\nEnsure to use the default settings for font and page margins in your RMarkdown file. Also, make sure your report title includes your group name: Group NAME.LETTER, and list the exam numbers of all group members in the author section.\n\n\n\n\n\n\n\n\n\nFormatting resources\n\n\n\n\n\nAt this page you can find resources to help you with your report formatting.\n\n\n\n\n1.1 Data\nHollywood Movies. At the link https://uoepsy.github.io/data/hollywood_movies_subset.csv you will find data on Hollywood movies released between 2012 and 2018 from the top 5 lead studios and top 10 genres.\nThe following variables were recorded. For formative report A, please only focus on the variables Movie to Year, ignoring anything beyond that. In other words, do not analyse the variables IQ1 to PrivateTransport in the next five weeks of the course. We will use those later in the course.\n\n\n\n\nVariable\nDescription\n\n\n\nMovie\nTitle of the movie\n\n\nLeadStudio\nPrimary U.S. distributor of the movie\n\n\nRottenTomatoes\nRotten Tomatoes rating (critics)\n\n\nAudienceScore\nAudience rating (via Rotten Tomatoes)\n\n\nGenre\nOne of Action Adventure, Black Comedy, Comedy, Concert, Documentary, Drama, Horror, Musical, Romantic Comedy, Thriller, or Western\n\n\nTheatersOpenWeek\nNumber of screens for opening weekend\n\n\nOpeningWeekend\nOpening weekend gross (in millions)\n\n\nBOAvgOpenWeekend\nAverage box office income per theater, opening weekend\n\n\nBudget\nProduction budget (in millions)\n\n\nDomesticGross\nGross income for domestic (U.S.) viewers (in millions)\n\n\nWorldGross\nGross income for all viewers (in millions)\n\n\nForeignGross\nGross income for foreign viewers (in millions)\n\n\nProfitability\nWorldGross as a percentage of Budget\n\n\nOpenProfit\nPercentage of budget recovered on opening weekend\n\n\nYear\nYear the movie was released\n\n\nIQ1-IQ50 (ignore for Formative report A)\nIQ score of each of 50 audience raters\n\n\nSnacks (ignore for Formative report A)\nHow many of the 50 audience raters bought snacks\n\n\nPrivateTransport (ignore for Formative report A)\nHow many of the 50 audience raters reached the cinema via private transportation\n\n\n\n\n\n\n1.2 This week’s task\nTask A1\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\n\nSub-steps\nBelow there are sub-steps you need to consider to complete this week’s task.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nRead the movie data into R, and give it a useful name. Inspect the data by looking at the data in RStudio. By viewing, we actually mean looking at the data either on the viewer or the console.1\nHow many observations are there?2\nHow many variables are there?3\n\n\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\n\nWhat does dim(DATA) return?\nWhat is the function of appending a [1] or [2]?\n\n\n\n\n\nWhat is the type of each variable?4\nWhat’s the minimum and maximum budget in the sample? What about the minimum and maximum Rotten Tomatoes rating?5\nDo you notice any issues when computing the minimum/maximum Budget and the minimum/maximum RottenTomatoes rating?6\nWhich variables have missing values in the dataset, and how many missing values does each have?7\nWrite-up a description of the dataset for the reader. You don’t need to show the actual data in the report, but a description in words is sufficient for the reader."
  },
  {
    "objectID": "1_01_design_and_data.html#worked-example",
    "href": "1_01_design_and_data.html#worked-example",
    "title": "Research design & data",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\n\n\n\nWe load the tidyverse package as we will use the functions read_csv and glimpse from this package.\n\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\n\n\n\nread_csv is the function to read CSV (comma separated values) files. Once we have read the file, it is stored into an object called tips using the arrow (&lt;-).\n\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\n\n\nhead() shows by default the top 6 rows of the data. Use the n = ... option to change the default behaviour, e.g. head(&lt;data&gt;, n = 10).\n\ndim(tips)\n\n[1] 157   7\n\n\n\n\nThis returns the number of rows and columns in the data\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\n\nglimpse is part of the tidyverse package\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatives to glimpse are the data “structure” function:\n\nstr(tips)\n\nspc_tbl_ [157 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Bill  : num [1:157] 23.7 36.1 32 17.4 15.4 ...\n $ Tip   : num [1:157] 10 7 5.01 3.61 3 2.5 3.44 2.42 3 2 ...\n $ Credit: chr [1:157] \"n\" \"n\" \"y\" \"y\" ...\n $ Guests: num [1:157] 2 3 2 2 2 2 2 2 2 2 ...\n $ Day   : chr [1:157] \"f\" \"f\" \"f\" \"f\" ...\n $ Server: chr [1:157] \"A\" \"B\" \"A\" \"B\" ...\n $ PctTip: num [1:157] 42.2 19.4 15.7 20.8 19.5 13.4 16 12.4 12.7 10.7 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Bill = col_double(),\n  ..   Tip = col_double(),\n  ..   Credit = col_character(),\n  ..   Guests = col_double(),\n  ..   Day = col_character(),\n  ..   Server = col_character(),\n  ..   PctTip = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nor:\n\nsapply(tips, data.class)\n\n       Bill         Tip      Credit      Guests         Day      Server \n  \"numeric\"   \"numeric\" \"character\"   \"numeric\" \"character\" \"character\" \n     PctTip \n  \"numeric\" \n\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nA dataset containing records on 7 variables related to tipping was obtained from https://uoepsy.github.io/data/RestaurantTips.csv, and was provided by the owner of a bistro in the US interested in studying which factors affected the tipping behaviour of the bistro’s customers. The data contains measurements for a total of 157 parties on four numeric variables: size of the bill (in dollars), size of the tip, number of guests in the group, and tip as a percentage of the bill total. The data also includes three categorical variables indicating whether or not the party paid with a credit card, the day of the week, as well as a server-specific identifier.\n\n\n\nsummary(tips)\n\n      Bill            Tip            Credit              Guests     \n Min.   : 1.66   Min.   : 0.250   Length:157         Min.   :1.000  \n 1st Qu.:15.19   1st Qu.: 2.075   Class :character   1st Qu.:2.000  \n Median :20.22   Median : 3.340   Mode  :character   Median :2.000  \n Mean   :22.73   Mean   : 3.807                      Mean   :2.096  \n 3rd Qu.:28.84   3rd Qu.: 5.000                      3rd Qu.:2.000  \n Max.   :70.51   Max.   :15.000                      Max.   :7.000  \n                 NA's   :1                                          \n     Day               Server              PctTip      \n Length:157         Length:157         Min.   :  6.70  \n Class :character   Class :character   1st Qu.: 14.30  \n Mode  :character   Mode  :character   Median : 16.20  \n                                       Mean   : 17.89  \n                                       3rd Qu.: 18.20  \n                                       Max.   :221.00  \n                                                       \n\n\n\n\nsummary returns a quick summary of the data, i.e. a list of numerical summaries.\nYou probably won’t understand some parts of the output above, but we will learn more in the coming weeks, so don’t worry too much about it. For the moment, you should be able to understand the minimum, maximum, and the mean.\nCurrently, it is not showing very informative output for the categorical variables, also known as factors.\nWe can replace each factor level with a clearer label. The following code takes the column Day from the tips data and assigns a new label “Monday” to the level “m”, etc.\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nYou can change/update a variable (column) in the data using the function mutate from the tidyverse package. It works as follows:\n\ntips &lt;- tips |&gt;\n    mutate(\n        Day = factor(Day,\n                     levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                     labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")),\n        Credit = factor(Credit,\n                        levels = c(\"n\", \"y\"),\n                        labels = c(\"No\", \"Yes\")),\n        Server = factor(Server)\n    )\n\nThe function |&gt; is called pipe and works by taking what’s on the left and passing it to the operation on the right. For example, you can take the logarithm of the whole numbers from 1 to 10 and round them to 2 digits using this code:\n\nround(log(1:10), digits = 2)\n\n [1] 0.00 0.69 1.10 1.39 1.61 1.79 1.95 2.08 2.20 2.30\n\n\nor this equivalent code that uses the pipe |&gt;:\n\n1:10 |&gt;\n    log() |&gt;\n    round(digits = 2)\n\n [1] 0.00 0.69 1.10 1.39 1.61 1.79 1.95 2.08 2.20 2.30\n\n\nYou can loosely think of the pipe as “then”. In fact, the pipe takes what’s to its left, and then passes it on to what’s on its right.\nCuriosity: Sometimes you may also find an older version of the pipe, which is %&gt;% and works in the same style. However, it requires the package tidyverse to be loaded before you can use the older pipe %&gt;%.\n\n\n\nLet’s check the result of the changes to the variable types:\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;fct&gt; No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;fct&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server &lt;fct&gt; A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip      \n A:60   Min.   :  6.70  \n B:65   1st Qu.: 14.30  \n C:32   Median : 16.20  \n        Mean   : 17.89  \n        3rd Qu.: 18.20  \n        Max.   :221.00  \n                        \n\n\n\n\nAfter making categorical variables factors, summary shows the count of each category for the categorical variables.\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips[tips$PctTip &gt; 100, ]\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively, using tidyverse, the function filter is used to only filter the rows that satisfy a condition:\n\ntips |&gt; \n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\n\nWith a bill of 49.6, the tip would be 109.62 dollars:\n\n49.6 * 221 / 100\n\n[1] 109.616\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip &gt; 100] &lt;- NA\n\n\n\na &gt; b tests whether a is greater than b. a &lt; b tests whether a is smaller than b. a == b tests whether a is equal to b; notice the double equal sign! You can also use &gt;= or &lt;=\n\n\n\n\n\n\nAlternative\n\n\n\n\n\nAlternatively you can use tidyverse:\n\ntips &lt;- tips |&gt;\n    mutate(\n        PctTip = ifelse(PctTip &gt; 100, NA, PctTip)\n    )\n\nWhere the function ifelse selects a value depending on a condition to test: ifelse(test, value_if_true, value_if_false). In the case above, each value in the column PctTip is replaced by NA if Pct &gt; 100, and it is kept the same otherwise.\n\n\n\n\nsummary(tips)\n\n      Bill            Tip         Credit        Guests             Day    \n Min.   : 1.66   Min.   : 0.250   No :106   Min.   :1.000   Monday   :20  \n 1st Qu.:15.19   1st Qu.: 2.075   Yes: 51   1st Qu.:2.000   Tuesday  :13  \n Median :20.22   Median : 3.340             Median :2.000   Wednesday:62  \n Mean   :22.73   Mean   : 3.807             Mean   :2.096   Thursday :36  \n 3rd Qu.:28.84   3rd Qu.: 5.000             3rd Qu.:2.000   Friday   :26  \n Max.   :70.51   Max.   :15.000             Max.   :7.000                 \n                 NA's   :1                                                \n Server     PctTip     \n A:60   Min.   : 6.70  \n B:65   1st Qu.:14.30  \n C:32   Median :16.15  \n        Mean   :16.59  \n        3rd Qu.:18.05  \n        Max.   :42.20  \n        NA's   :1      \n\n\nThe function summary() return a numeric answer for the min/max/mean, see above, even in the presence of missing values (NAs).\nHowever, if you use functions such as min(), max(), mean(), which compute the minimum, maximum, and mean (i.e., average) of a variable respectively, they will return NA when applied to a variable that contains a missing value:\n\nmin(tips$Tip)\n\n[1] NA\n\n\nTo get a numeric result, you need to include the argument, i.e. the option, na.rm = TRUE:\n\nmin(tips$Tip, na.rm = TRUE)\n\n[1] 0.25\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe average bill size was $22.73, and the average tip was $3.85, corresponding to roughly 17% of the total bill. Out of 157 parties, only 51 paid with a credit card. Most parties tended to be of around 2 people each, and people tended to go to that restaurant more often on Wednesday. Among the three servers, server C was the one that served the least number of parties. The data also included a missing tipping value, corresponding to a bill $49.59, and a data inputting error for the corresponding measure of the tip as a percentage of the total bill."
  },
  {
    "objectID": "1_01_design_and_data.html#student-glossary",
    "href": "1_01_design_and_data.html#student-glossary",
    "title": "Research design & data",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nA good understanding of R functions is essential for your success in the DAPR curriculum and the degree more generally. We strongly recommend you get into the habit of keeping track of any new R function that you encounter and write a short description of what the function does and which package it comes from. You could save this into a Word, Excel, or Notebook file. You should get into the habit of updating that document as you encounter new R functions. For this week, we have provided below a completed table to help you get started!\n\n\n\n\n\n\nFunction\nUse and package\n\n\n\nread_csv\nFor reading comma separated value files. Part of tidyverse package\n\n\nView\nOpens a spreadsheet-like data viewer in RStudio. Base R function\n\n\nhead\nShows the first 6 rows of a dataset (by default). Base R function\n\n\nnrow\nReturns the number of rows. Base R function\n\n\nncol\nReturns the number of columns. Base R function\n\n\ndim\nReturns the dimensions (rows and columns). Base R function\n\n\nglimpse\nSimilar to str. Displays an overview of the data variables with their types. Part of tidyverse package\n\n\nstr\nSimilar to glimpse. Displays an overview of the data variables with their types. Base R function\n\n\nsummary\nProduces summary statistics. Base R function\n\n\nfactor\nCreates or modifies a variable into a categorical one (factor) with levels. Base R function"
  },
  {
    "objectID": "1_01_design_and_data.html#footnotes",
    "href": "1_01_design_and_data.html#footnotes",
    "title": "Research design & data",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: To read the data use read_csv() from the tidyverse package.  To preview the data, use View(DATA) or head(DATA)↩︎\n Hint: nrow(DATA)  or dim(DATA)[1]↩︎\n Hint: ncol(DATA)  or dim(DATA)[2]↩︎\n Hint: glimpse(DATA) from tidyverse or str(DATA) or sapply(DATA, data.class)↩︎\n Hint: summary(DATA$VARIABLE) or min(DATA$VARIABLE) and max(DATA$VARIABLE)↩︎\n For some movies, data on the budget or rotten tomatoes rating are not available (NA). These are also called missing values.\nIf you used the functions min(), max(), you will get NA as a result. This is because if a value is missing, you cannot compute the mean of something you don’t know. For example, what is the mean of 5, 10, and NA? How would I compute (5 + 10 + NA) / 3? I don’t know, so it remains NA.\nYou can tell R to ignore the missing values by saying min(DATA$VARIABLE, na.rm = TRUE) and similarly for max and other functions like mean, which computes the average instead.\nThe summary() function, instead, does this for you automatically and immediately tells you if a variable had any NAs and how many.↩︎\n Hint: summary(DATA) shows the variables and the number of missing values in each variable.↩︎"
  },
  {
    "objectID": "1_03_numeric_data.html",
    "href": "1_03_numeric_data.html",
    "title": "Numeric data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variablesA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A3. Below there are some guided sub-steps you may want to consider to complete task A3.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nVisualising the distribution of numerical data\n\n\n\n\n\nWe display numeric variables with histograms, density plots, or boxplots. Respectively, these use the function geom_histogram(), geom_density(), or geom_boxplot() from ggplot2, which is a package automatically loaded when you load tidyverse via library(tidverse). For illustration purposes, we will use the starwars dataset from tidyverse, containing information on Starwars characters.\n\nlibrary(tidyverse)\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\n\nHistogram\nDensity plot\nBoxplot\n\n\n\nThe distribution of the character heights (cm) can be displayed with a histogram:\n\nggplot(starwars, aes(x = height)) +\n    geom_histogram(color = 'gray', fill = 'lightblue') +\n    labs(x = \"Character height (cm)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a density plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_density(color = 'dodgerblue') +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a box plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_boxplot() +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate six plots, each displaying the distribution of:2\n\nProduction budgets\nAudience scores\nRotten Tomatoes ratings\nWorld gross income\nForeign gross income\nYear of movie release\n\n\n\n\nArrange the above plots as a single figure comprising 2 by 3 panels3\n\n\n\n\n\n\n\n\nCompute the mean and standard deviation of a variable\n\n\n\n\n\nConsider again the starwars dataset. The mean and SD of the height variable can be computed as:\n\nstarwars |&gt;\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) |&gt;\n    round(digits = 2)\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  175.  34.8\n\n\nTo make a nice table for the PDF document, you can use the kbl() function from the kableExtra package:\n\nlibrary(kableExtra)\n\nstarwars |&gt;\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) |&gt;\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\nM\nSD\n\n\n174.6\n34.77\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises the production budgets using the mean and standard deviation.\nIn the next step you will learn how to create a table for all variables at once, so this step can be excluded from your report, but it’s important to know how to manually compute it too.4\n\n\n\n\n\n\n\n\nTable of descriptive statistics\n\n\n\n\n\nUsing summarise for more than a couple of variables would make the job very tedious and long. There is a shortcut, which uses the describe function from the psych package.\nThe following code creates a table of descriptive statistics (via the describe function from the psych package) and ensures the table is in proper format by using the kbl function from the kableExtra package.\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars |&gt;\n    select(height, mass) |&gt;\n    describe() |&gt;\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nheight\n1\n81\n174.60\n34.77\n180\n178.48\n17.79\n66\n264\n198\n-1.05\n1.80\n3.86\n\n\nmass\n2\n59\n97.31\n169.46\n79\n75.44\n16.31\n15\n1358\n1343\n6.97\n48.93\n22.06\n\n\n\n\n\nTo only show the columns n, mean, sd, median you can use:\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars |&gt;\n    select(height, mass) |&gt;\n    describe() |&gt;\n    select(n, mean, sd, median) |&gt;\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nn\nmean\nsd\nmedian\n\n\n\nheight\n81\n174.60\n34.77\n180\n\n\nmass\n59\n97.31\n169.46\n79\n\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises (using the mean and standard deviation) the six numeric variables which you plotted above. 5\n\n\n\nWrite up a summary of what you have reported in the plots and/or tables, using proper rounding to 2 decimal places and avoiding any reference to R code or functions.\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\nMake sure that all R code and output is not visible in the PDF report. The PDF report should only include text, tables, and plots."
  },
  {
    "objectID": "1_03_numeric_data.html#formative-report-a",
    "href": "1_03_numeric_data.html#formative-report-a",
    "title": "Numeric data",
    "section": "",
    "text": "Instructions and data were released in week 1.\n\n\n\n\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variablesA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choice\nA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback\n\n\nThis week you will only focus on task A3. Below there are some guided sub-steps you may want to consider to complete task A3.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\n\n\n\n\n\n\n\n\nVisualising the distribution of numerical data\n\n\n\n\n\nWe display numeric variables with histograms, density plots, or boxplots. Respectively, these use the function geom_histogram(), geom_density(), or geom_boxplot() from ggplot2, which is a package automatically loaded when you load tidyverse via library(tidverse). For illustration purposes, we will use the starwars dataset from tidyverse, containing information on Starwars characters.\n\nlibrary(tidyverse)\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\n\nHistogram\nDensity plot\nBoxplot\n\n\n\nThe distribution of the character heights (cm) can be displayed with a histogram:\n\nggplot(starwars, aes(x = height)) +\n    geom_histogram(color = 'gray', fill = 'lightblue') +\n    labs(x = \"Character height (cm)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a density plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_density(color = 'dodgerblue') +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\nThe distribution of the character heights (cm) can be displayed with a box plot:\n\nggplot(starwars, aes(x = height)) +\n    geom_boxplot() +\n    labs(x = \"Character height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate six plots, each displaying the distribution of:2\n\nProduction budgets\nAudience scores\nRotten Tomatoes ratings\nWorld gross income\nForeign gross income\nYear of movie release\n\n\n\n\nArrange the above plots as a single figure comprising 2 by 3 panels3\n\n\n\n\n\n\n\n\nCompute the mean and standard deviation of a variable\n\n\n\n\n\nConsider again the starwars dataset. The mean and SD of the height variable can be computed as:\n\nstarwars |&gt;\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) |&gt;\n    round(digits = 2)\n\n# A tibble: 1 × 2\n      M    SD\n  &lt;dbl&gt; &lt;dbl&gt;\n1  175.  34.8\n\n\nTo make a nice table for the PDF document, you can use the kbl() function from the kableExtra package:\n\nlibrary(kableExtra)\n\nstarwars |&gt;\n    summarise(M = mean(height, na.rm = TRUE),\n              SD = sd(height, na.rm = TRUE)) |&gt;\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\nM\nSD\n\n\n174.6\n34.77\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises the production budgets using the mean and standard deviation.\nIn the next step you will learn how to create a table for all variables at once, so this step can be excluded from your report, but it’s important to know how to manually compute it too.4\n\n\n\n\n\n\n\n\nTable of descriptive statistics\n\n\n\n\n\nUsing summarise for more than a couple of variables would make the job very tedious and long. There is a shortcut, which uses the describe function from the psych package.\nThe following code creates a table of descriptive statistics (via the describe function from the psych package) and ensures the table is in proper format by using the kbl function from the kableExtra package.\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars |&gt;\n    select(height, mass) |&gt;\n    describe() |&gt;\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nheight\n1\n81\n174.60\n34.77\n180\n178.48\n17.79\n66\n264\n198\n-1.05\n1.80\n3.86\n\n\nmass\n2\n59\n97.31\n169.46\n79\n75.44\n16.31\n15\n1358\n1343\n6.97\n48.93\n22.06\n\n\n\n\n\nTo only show the columns n, mean, sd, median you can use:\n\nlibrary(kableExtra) # for the kbl function\nlibrary(psych)      # for the describe function\n\nstarwars |&gt;\n    select(height, mass) |&gt;\n    describe() |&gt;\n    select(n, mean, sd, median) |&gt;\n    kbl(digits = 2, booktabs = TRUE)   # kbl converts to a nice PDF table\n\n\n\n\n\n\nn\nmean\nsd\nmedian\n\n\n\nheight\n81\n174.60\n34.77\n180\n\n\nmass\n59\n97.31\n169.46\n79\n\n\n\n\n\n\n\n\n\nCreate a table of descriptive statistics that summarises (using the mean and standard deviation) the six numeric variables which you plotted above. 5\n\n\n\nWrite up a summary of what you have reported in the plots and/or tables, using proper rounding to 2 decimal places and avoiding any reference to R code or functions.\n\n\n\n\n\n\n\nHiding R code or ouput\n\n\n\n\n\n\n\nHide code but show output\nShow code but hide output\nHide both code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\nMake sure that all R code and output is not visible in the PDF report. The PDF report should only include text, tables, and plots."
  },
  {
    "objectID": "1_03_numeric_data.html#worked-example",
    "href": "1_03_numeric_data.html#worked-example",
    "title": "Numeric data",
    "section": "\n2 Worked example",
    "text": "2 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;chr&gt; \"n\", \"n\", \"y\", \"y\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\"…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\"…\n$ Server &lt;chr&gt; \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\", \"B\"…\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\nglimpse(tips)\n\nRows: 157\nColumns: 7\n$ Bill   &lt;dbl&gt; 23.70, 36.11, 31.99, 17.39, 15.41, 18.62, 21.56, 19.58, 23.59, …\n$ Tip    &lt;dbl&gt; 10.00, 7.00, 5.01, 3.61, 3.00, 2.50, 3.44, 2.42, 3.00, 2.00, 1.…\n$ Credit &lt;fct&gt; No, No, Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, N…\n$ Guests &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 3, 2, 2, 1, 5, 5, …\n$ Day    &lt;fct&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Friday,…\n$ Server &lt;fct&gt; A, B, A, B, B, A, B, A, A, B, B, A, B, B, B, B, C, C, C, C, C, …\n$ PctTip &lt;dbl&gt; 42.2, 19.4, 15.7, 20.8, 19.5, 13.4, 16.0, 12.4, 12.7, 10.7, 11.…\n\n\n\ntips &lt;- tips |&gt;\n    mutate(\n        PctTip = ifelse(PctTip &gt; 100, NA, PctTip)\n    )\n\nWe can create a histogram of tips via:\n\nggplot(tips, aes(x = Tip)) + \n    geom_histogram(color = 'white') + \n    labs(x = \"Size of the tip (US dollars)\")\n\n\n\n\n\n\n\nWe can create a single figure with the distribution of all numeric variables by using the patchwork package:\n\n\nIf you save each plot into an object with a name, e.g. p1, p2, p3, p4, you can arrange the four plots into 2 by 2 panels as follows:\nlibrary(patchwork)\n(p1 | p2) / (p3 | p4)\nTo do 1 row of 4 plots:\np1 | p2 | p3 | p4\n\nlibrary(patchwork)\n\npltBill &lt;- ggplot(tips, aes(x = Bill)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Size of the bill (US dollars)\")\n\npltTip &lt;- ggplot(tips, aes(x = Tip)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Size of the tip (US dollars)\")\n\npltGuests &lt;- ggplot(tips, aes(x = Guests)) +\n    geom_bar(fill = 'lightblue') +\n    labs(x = \"Number of people in the group\")\n\npltPctTip &lt;- ggplot(tips, aes(x = PctTip)) +\n    geom_histogram(color = 'white', fill = 'lightblue') +\n    labs(x = \"Tip as a percentage of the bill\")\n\n(pltBill | pltTip) / (pltGuests | pltPctTip)\n\n\n\n\n\n\n\nTo summarise one numeric variable, you can use the summarise function from tidyverse, which takes the data and computes a numeric summary. The syntax is:\ndata |&gt;\n    summarise(\n        write_the_column_name = computation\n    )\nThis computes the mean and SD of tip size (in US dollars), and calls the column storing the mean M, and the column storing the standard deviation SD:\n\ntips |&gt;\n    summarise(\n        M = mean(Tip, na.rm = TRUE),\n        SD = sd(Tip, na.rm = TRUE)\n    ) |&gt;\n    kbl(digits = 2, booktabs = TRUE)\n\n\n\n\n\nM\nSD\n\n\n3.81\n2.37\n\n\n\n\nTo summarise all of the numeric variables into a single table of descriptive statistics you can use the describe function from the psych package:\n\nlibrary(kableExtra)\nlibrary(psych)\n\ntips |&gt;\n    select(Bill, Tip, Guests, PctTip) |&gt;\n    describe() |&gt;\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nBill\n1\n157\n22.73\n12.16\n20.22\n21.37\n11.03\n1.66\n70.51\n68.85\n1.22\n1.89\n0.97\n\n\nTip\n2\n156\n3.81\n2.37\n3.34\n3.50\n1.99\n0.25\n15.00\n14.75\n1.50\n3.13\n0.19\n\n\nGuests\n3\n157\n2.10\n0.93\n2.00\n1.98\n0.00\n1.00\n7.00\n6.00\n2.22\n7.81\n0.07\n\n\nPctTip\n4\n156\n16.59\n4.39\n16.15\n16.25\n2.74\n6.70\n42.20\n35.50\n2.50\n12.39\n0.35\n\n\n\n\n\nTo only keep the sample size, mean, SD, median, min, max, we use the select function from tidyverse:\n\nlibrary(kableExtra)\nlibrary(psych)\n\ntips |&gt;\n    select(Bill, Tip, Guests, PctTip) |&gt;\n    describe() |&gt;\n    select(n, mean, sd, median, min, max) |&gt;\n    kbl(booktabs = TRUE, digits = 2)\n\n\n\n\n\n\nn\nmean\nsd\nmedian\nmin\nmax\n\n\n\nBill\n157\n22.73\n12.16\n20.22\n1.66\n70.51\n\n\nTip\n156\n3.81\n2.37\n3.34\n0.25\n15.00\n\n\nGuests\n157\n2.10\n0.93\n2.00\n1.00\n7.00\n\n\nPctTip\n156\n16.59\n4.39\n16.15\n6.70\n42.20\n\n\n\n\n\n\n\n\n\n\n\nUse the appropriate summary for each variable type\n\n\n\nEnsure that you summarise variables correctly:\n\nFor categorical variables use frequency tables\nFor continuous variables use a table of descriptive statistics (mean, SD, Median, etc.)\n\nYou should not summarise categorical variables with the mean, SD, and this is why it’s important to use select() before describe() to only keep the variables that are continuous.\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nThe distributions of bill size, tip size, and group size are skewed to the right. The distribution of tips, as a percentage of the total bill, appears to be approximately bell shaped, with three outliers in the right tail of the distribution.\nThe average bill was about $22.73, with a SD of $12.16. The average tip was $3.81, with a SD of $2.37, corresponding to an average tip as a percentage of the total bill of $16.59, with a SD of $4.39. The average party size comprised 2 guests, with a SD of roughly 1 person."
  },
  {
    "objectID": "1_03_numeric_data.html#student-glossary",
    "href": "1_03_numeric_data.html#student-glossary",
    "title": "Numeric data",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_histogram\n?\n\n\ngeom_density\n?\n\n\ngeom_boxplot\n?\n\n\npatchwork: | and /\n\n?\n\n\nsummarise\n?\n\n\nselect\n?\n\n\nkbl\n?\n\n\ndescribe\n?"
  },
  {
    "objectID": "1_03_numeric_data.html#footnotes",
    "href": "1_03_numeric_data.html#footnotes",
    "title": "Numeric data",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: you could use geom_histogram(), geom_density(), or geom_boxplot().\nStop and think. Do you notice anything strange in the distribution of one of the variables? Among the six variables, one is different from the other 5. Can you think of which one, and why?\nAnswer. The variable that is slightly different from the other numeric variables is Year. While Year is stored as a numeric variable, is it perhaps better visualised by geom_bar(). You could think of it as an ordinal variable rather than a continuous one.↩︎\n\n\nHint: use the | and / functions from the patchwork package. Before you can use these functions, remember to load the package via library(patchwork).\nFor example, if you stored four plots into plt1, plt2, plt3, and plt4, the code below creates a single figure of 2 by 2 panels:\n(plt1 | plt2) / (plt3 | plt4)↩︎\n\n Hint: the summarise() function from tidyverse, mean() and sd().  Hint: if you remember, there are NAs in the variable. What do you need to add to the mean() and sd() function to ensure they do not affect the computation?↩︎\n\nHint: the describe function from the psych package.\nStop and think. Think about the variable Year again. Does it make sense to compute the average year? If not, you may wish to exclude it from your table of descriptive statistics.↩︎"
  },
  {
    "objectID": "1_05_formative_report_a.html",
    "href": "1_05_formative_report_a.html",
    "title": "Formative report A",
    "section": "",
    "text": "Instructions and data were released in week 1."
  },
  {
    "objectID": "1_05_formative_report_a.html#tasks",
    "href": "1_05_formative_report_a.html#tasks",
    "title": "Formative report A",
    "section": "\n1 Tasks",
    "text": "1 Tasks\nFor formative report A, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find guided sub-steps you may want to consider to complete this week’s task.\n\nA1) Read the data into R, inspect it, and write a concise introduction to the data and its structure\nA2) Display and describe the categorical variables\nA3) Display and describe six numerical variables of your choice\nA4) Display and describe a relationship of interest between two or three variables of your choiceA5) Finish the report write-up, knit to PDF, and submit the PDF for formative feedback"
  },
  {
    "objectID": "1_05_formative_report_a.html#a5-sub-tasks",
    "href": "1_05_formative_report_a.html#a5-sub-tasks",
    "title": "Formative report A",
    "section": "\n2 A5 sub-tasks",
    "text": "2 A5 sub-tasks\nThis week you will only focus on task A5. Below there are some guided sub-steps you may want to consider to complete task A5.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\nEnsure all group members have joined the group on LEARN. If you have not done so yet, go to the course LEARN page, click “Groups” from the top menu bar, find the group having the same name as your table label, and click Join.\nReopen last week’s Rmd file, as you will continue last week’s work and build on it.1\nDid you install tinytex? If yes, go to the next bullet point. If not, check the hint. This package is required to compile an Rmd file into a PDF for submission.2\n\n\n\n\n\n\n\nFormatting tip: Hiding R code or ouput\n\n\n\n\n\n\n\nHiding R code\nHiding R output\nHiding R code and output\n\n\n\nTo not show the code of an R code chunk, and only show the output, write:\n```{r, echo=FALSE}\n# code goes here\n```\n\n\nTo show the code of an R code chunk, but hide the output, write:\n```{r, results='hide'}\n# code goes here\n```\nTo hide both text output and figures, use:\n```{r, results='hide', fig.show='hide'}\n# code goes here\n```\n\n\nTo hide both code and output of an R code chunk, write:\n```{r, include=FALSE}\n# code goes here\n```\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting tip: Reducing figure size\n\n\n\n\n\nYou could place multiple panels into a single figure using the functions | and / from the patchwork package.\nYou could adjust the figure height and width by playing with a few options for the numbers fig.height = ? and fig.width = ?, for example 5 and 4, or 12 and 8, and so on. Please note this is typically found by trial and error. Keep in mind, however, that the figure labels should still be legible in the plot you show.\n```{r, fig.height = 5, fig.width = 4}\n# your code to display the figure here\n```\n\n\n\n\nOrganise the Rmd file to have the following structure:\n\n---\ntitle: \"Formative Report A (Group &lt;NUMBER&gt;.&lt;LETTER&gt;)\"\nauthor: \"&lt;insert exam numbers here, e.g. B001, B002, B003, B004, B005&gt;\"\ndate: \"&lt;insert date here&gt;\"\noutput: bookdown::pdf_document2\ntoc: false\n---\n\n\nThis is the metadata block. It includes the:\n\ndocument title\nauthor name\ndate (to leave empty, use an empty string \"\")\nthe output type\nwhether or not to display the Table of Contents (TOC)\n\nThe output type could be html_document, pdf_document, etc. We use bookdown::pdf_document2 so that we can reference figures, which pdf_document doesn’t let you do. The code bookdown::pdf_document2 simply means to use the pdf_document2 type from the bookdown package.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)\n```\n\n\nThis is the setup chunk and should always be included in your Rmd document. It sets the global options for all code chunks that will follow.\nIn your knitted file:\n\nIf echo=FALSE, the R code in chunks is not displayed. If TRUE, it is.\nIf message=FALSE, information messages are not displayed. If TRUE, they are.\nIf warning=FALSE, warning messages are not printed. If TRUE, they are.\n\nIf you want to change the setting in a specific code chunk, you can do so via:\n```{r, echo=TRUE}\n# A code chunk\n```\n\n```{r, include=FALSE}\n# Week 1 code below\nlibrary(tidyverse)\n\n# Week 2 code below\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar()\n\n# Week 3 code below\n\n# week 4 code below\n```\n\n\nThis code chunks contains your rough work from each week. Give names to plots and tables, so that you can reference those later on. The option include=FALSE hides both code and output.\nTo run each line of code while you are working, put your cursor on the line and press Control + Enter on Windows or Command + Enter on a macOS.\n# Introduction\n\nWrite here a concise introduction to the data, the variables, and anything worth \nof notice in the dataset.\n\n\n# Analysis\n\nPresent here your tables, plots, and results. In the code chunk below, you do \nnot need to put the chunk option `echo=FALSE` as you set this option globally \nin the setup chunk. \n\n```{r}\npltEye\n```\n\nIf you didn't set it globally, you would need to put it in the chunk options:\n\n```{r, echo=FALSE}\npltEye\n```\n\nMore text...\n\n\n# Discussion\n\nWrite up your take home messages here...\n\n\nThis contains your actual textual reporting, as well as tables and figures. To show in place a plot previously created, just include the plot name in a code chunk with the option echo=FALSE to hide the code but display the output.\n# Appendix A: Additional figures and tables\n\nInclude here additional tables and figures, with captions, and properly \nreferenced. These should be used somewhere in the text, do not include tables or \nfigures which are not referenced anywhere in your writing.\n\n\nIf you don’t need Appendix A, because all your figures and tables fit in the page limit, you can delete it.\n# Appendix B: R code\n\nDo not edit the code chunk below, but remove this paragraph of text before \nsubmitting.\n\n```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}\n\n```\n\n\nThis special code chunk (do not edit it) takes all code above and places it here into Appendix B for you. As such, it allows the marker to see the code you used to obtain your results. Please note that only the code should be visible in the appendix, no output. Hence why the options are echo=TRUE (show the code), but eval=FALSE (do not evaluate/run the code).\nThe appendices do not count towards the 6-page limit.\n\n\n\n\n\n\nSuccessful knitting checklist\n\n\n\nIf you encounter errors when knitting the Rmd file, go through the following checklist to try finding the source of the errors.\n\nSuccessful knitting checklist\n\n\n\n\nKnit the document to PDF: click File &gt; Knit Document.\n\nSubmit the PDF file on Learn:\n\nGo to the Learn page of the course\nClick Assessments\nClick Submit Formative Report A (PDF file only)\nFollow the instructions\n\n\n\n\n\n\n\n\n\nFormatting tips: Referencing figures\n\n\n\n\n\nFirst, you need to pick a unique label for the code chunk that displays the figure, in this case shortLabel but you should use a more descriptive name.\n```{r shortLabel, fig.cap = \"Figure caption\"}\npltEye &lt;- ggplot(starwars, aes(x = eye_color)) + \n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\npltEye\n```\n\n\n\n\nFigure caption\n\n\n\nTo reference a figure, for example the one above, you would \nwrite see Figure \\@ref(fig:shortLabel).\nwhich, when you Knit to PDF, becomes:\n\nTo reference a figure, for example the one above, you would write see Figure 1.\n\n\n\n\n\n\n\n\n\n\nFormatting tips: Referencing tables\n\n\n\n\n\nSay you created a tibble and stored it into tblEye. Please make sure you haven’t made it into a kbl() yet. Create a code chunk to display tblEye, and then pipe it into kbl() to make a table for a PDF file. You can also specify the table caption (caption = ““), whether to include tab lines (booktabs = TRUE), and whether to round to 2 digits (digits = 2). To reference a table you need to pick a unique label for the code chunk that displays the table, in this case anotherLabel but you should use a more descriptive name.\n```{r anotherLabel, echo=FALSE}\ntblEye |&gt;\n    kbl(booktabs = TRUE, caption = \"Short table caption\")\n```\n\n\n\nShort table caption\n\neye_color\nn\n\n\n\nblack\n10\n\n\nblue\n19\n\n\nblue-gray\n1\n\n\nbrown\n21\n\n\ndark\n1\n\n\ngold\n1\n\n\ngreen, yellow\n1\n\n\nhazel\n3\n\n\norange\n8\n\n\npink\n1\n\n\nred\n5\n\n\nred, blue\n1\n\n\nunknown\n3\n\n\nwhite\n1\n\n\nyellow\n11\n\n\n\n\n\nThe table is referenced as, see Table \\@ref(tab:anotherLabel).\nWhich, when you knit to PDF, is displayed as:\n\nThe table is referenced as, see Table 1.\n\nFor details on styling PDF tables, see this link."
  },
  {
    "objectID": "1_05_formative_report_a.html#worked-example",
    "href": "1_05_formative_report_a.html#worked-example",
    "title": "Formative report A",
    "section": "\n3 Worked example",
    "text": "3 Worked example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)  # we use read_csv and glimpse from tidyverse\n\n\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWe can replace each factor level with a clearer label:\n\ntips$Day &lt;- factor(tips$Day, \n                   levels = c(\"m\", \"t\", \"w\", \"th\", \"f\"),\n                   labels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"))\n\ntips$Credit &lt;- factor(tips$Credit, \n                      levels = c(\"n\", \"y\"),\n                      labels = c(\"No\", \"Yes\"))\n\ntips$Server &lt;- factor(tips$Server)\n\nThe percentage of total bill has a maximum value of 221, which seems very strange. Someone is very unlikely to tip more than their bill total. In this case 221% of their bill value seems unlikely.\nLet’s inspect the row where PctTip is greater than 100:\n\ntips |&gt; \n    filter(PctTip &gt; 100)\n\n# A tibble: 1 × 7\n   Bill   Tip Credit Guests Day      Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;dbl&gt;\n1  49.6    NA Yes         4 Thursday C         221\n\n\n\n\nThis code means: take the data tips and then filter it to only keep the rows where PctTip is larger than 100. You can also provide a condition using other comparison operators such as ==, &gt;=, &lt;=, &gt;, &lt;, …\nWith a bill of 49.59, the tip would be 109.59 dollars:\n\n49.59 * 221 / 100\n\n[1] 109.5939\n\n\nFurthermore, we also notice that the tipping amount is not available (NA). The corresponding value in the percentage of total tip seems likely an inputting error, perhaps due to double typing the leading 2 when recording the data. We will set that value to not available (NA) with the following code:\n\ntips$PctTip[tips$PctTip &gt; 100] &lt;- NA\n\nConsider, for example, the relationship between bill and tip size. As these are two numerical variables, we visualise the relationship with a scatterplot:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\")\n\n\n\n\n\n\n\nWe can numerically summarise this relationship with the covariance between the two variables:\n\nround(cov(tips$Bill, tips$Tip, use = \"pairwise.complete.obs\"), digits = 2)\n\n[1] 25.96\n\n\nThe relationship looks roughly like a line. You can superimpose a “best-fit” line with the function geom_smooth(method = lm, se = FALSE). The argument method = lm tells to fit a line (in R this is called a linar model, lm), and se = FALSE tells R to not plot the uncertainty bands.\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\n\n\nYou will only learn how to find the functional relationship between two variables in the second-year course DAPR2, so for now I will give it to you:\n\\[\ny = -0.26 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]\nWhat is the predicted tip for a bill of 50 US dollars? Let’s do the computation:\n\n-0.26 + 0.18 * 50\n\n[1] 8.74\n\n\nFrom the plot above, a tip of 8.74 US dollars seems roughly right!\nLet’s find the tips for bills of size 20, 40, 60.\n\ntips_line_func &lt;- tibble(\n    bills = c(20, 40, 60),\n    tips  = -0.26 + 0.18 * bills\n)\ntips_line_func\n\n# A tibble: 3 × 2\n  bills  tips\n  &lt;dbl&gt; &lt;dbl&gt;\n1    20  3.34\n2    40  6.94\n3    60 10.5 \n\n\nTo investigate the relationship between bill and tip size for those who paid by credit card and those who didn’t we can create faceted scatterplots:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\")\n\n\n\n\n\n\n\nYou can also fit a best-fit line by payment method:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\") +\n    geom_smooth(method = lm, se = FALSE)\n\n\n\n\n\n\n\nTo extend the lins for the full range of the x-axis, you can use the option fullrange = TRUE:\n\nggplot(tips, aes(x = Bill, y = Tip)) +\n    geom_point() +\n    labs(x = \"Bill size (in US dollars)\",\n         y = \"Tip size (in US dollars)\") +\n    facet_wrap(~Credit, labeller = \"label_both\") +\n    geom_smooth(method = lm, se = FALSE, fullrange = TRUE)\n\n\n\n\n\n\n\nAgain, you will not know how to find out the functional relationship between the variables within each group until the course DAPR2 in 2nd year, so I will give it to you.\nFor those who did not pay by credit card:\n\\[\ny = -0.17 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]\nFor those who paid by credit card:\n\\[\ny = -0.34 + 0.18 * x \\qquad \\text{where} \\qquad \\begin{cases}\nx = \\text{Bill} \\\\\ny = \\text{Tip}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "1_05_formative_report_a.html#student-glossary",
    "href": "1_05_formative_report_a.html#student-glossary",
    "title": "Formative report A",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions that you started last week.\n\n\nFunction\nUse and package\n\n\n\ngeom_smooth\n?\n\n\ntibble\n?\n\n\nknitr::opts_chunk$set()\n?"
  },
  {
    "objectID": "1_05_formative_report_a.html#footnotes",
    "href": "1_05_formative_report_a.html#footnotes",
    "title": "Formative report A",
    "section": "Footnotes",
    "text": "Footnotes\n\n Hint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nInstalling tinytex. Copy the line below and paste it into the console:\ninstall.packages(\"tinytex\")\npress Enter. Copy and paste the line below into the console:\ntinytex::install_tinytex()\npress Enter. Type Y and press Enter again.↩︎"
  },
  {
    "objectID": "1_08_prob_rules.html",
    "href": "1_08_prob_rules.html",
    "title": "Probability Rules",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.B2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback.\n\n\nThis week you will only focus on task B2. Below there are sub-steps you need to consider to complete task B2.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nReporting probabilities\n\n\n\nIn the report write-up do not include the intermediate working that was necessary to compute the probabilities required, it’s sufficient to define the probability and give the final result. The intermediate working should be part of the code, which appears in Appendix B.\nFollowing APA style, do not use a zero before the decimal point when reporting probabilities.\n\n\n\nReopen last week’s Rmd file, as you will continue working on last week’s data about the top 3 most frequent movie genres, and their ratings.1\n\n\n\nWhat’s the probability of a movie being rated as Good, i.e. \\(P(Good)\\)?2\n\n\n\nGiven that a viewer watched a Drama movie, what’s the probability of them giving a good rating, i.e. \\(P(Good \\mid Drama)\\)?3\n\n\n\nGiven that a viewer watched an Action movie, what’s the probability of them giving a good rating, i.e. \\(P(Good \\mid Action)\\)?\nGiven that a viewer watched a Comedy movie, what’s the probability of them giving a good rating, i.e. \\(P(Good \\mid Comedy)\\)?\nDo you think that a movie receiving a Good rating is independent of the Genre?4\n\n\nNow let’s move on from independence. Given that a viewer gave a bad rating, what’s the probability of them having watched a non-drama movie? 5\n\n\n\n\n\n\n\n\nAdvanced material\n\n\n\n\n\nNote the following rules of set theory:\n\n\\((A_1 \\cup A_2) \\cap B = (A_1 \\cap B) \\cup (A_2 \\cap B)\\)\n\\((A_1 \\cap A_2) \\cup B = (A_1 \\cup B) \\cap (A_2 \\cup B)\\)\n\nConsider three mutually exclusive events \\(A_1, A_2, A_3\\) and another event \\(B\\). We have that:\n\\[\n\\begin{aligned}\nP\\big(A_3^c \\mid B\\big)\n&= \\frac{P(A_3^c \\cap B)}{P(B)} \\\\\n&= \\frac{P\\big((A_1 \\cup A_2) \\cap B\\big)}{P(B)} \\\\\n&= \\frac{P\\big((A_1 \\cap B) \\cup (A_2 \\cap B)\\big)}{P(B)} \\qquad \\text{[rule (1) above]}\\\\\n&= \\frac{P(A_1 \\cap B) + P(A_2 \\cap B)}{P(B)}\n\\end{aligned}\n\\]\nSuppose \\(A_1 = Action\\), \\(A_2 = Comedy\\), \\(A_3 = Drama\\), and \\(B = Bad\\).\n\\[\n\\begin{aligned}\nP\\big(Drama^c \\mid Bad\\big)\n&= \\frac{P\\big(Drama^c \\cap Bad\\big)}{P(Bad)} \\\\\n&= \\frac{P\\big((Action \\cup Comedy) \\cap Bad\\big)}{P(Bad)} \\\\\n&= \\frac{P\\big((Action \\cap Bad) \\cup (Comedy \\cap Bad)\\big)}{P(Bad)} \\\\\n&= \\frac{P(Action \\cap Bad) + P(Comedy \\cap Bad)}{P(Bad)}\n\\end{aligned}\n\\]\n\n\n\n\nBased on your analysis above, which movie Genre do you think lead studios should invest in for their next movie?6\n\n\n\nUsing a conditional mosaic plot, display the conditional distribution of movie genres being rated as either good or bad, making sure to add a main title and clear axis titles.7\n\n\n\nIn the analysis section of your report, write up a summary of your findings from above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions. In particular, focus on whether events were independent, and which genre of movie lead studios should consider investing in based on audience ratings.8"
  },
  {
    "objectID": "1_08_prob_rules.html#formative-report-b",
    "href": "1_08_prob_rules.html#formative-report-b",
    "title": "Probability Rules",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.B2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.\nB4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback.\n\n\nThis week you will only focus on task B2. Below there are sub-steps you need to consider to complete task B2.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\n\n\n\n\n\n\n\n\nReporting probabilities\n\n\n\nIn the report write-up do not include the intermediate working that was necessary to compute the probabilities required, it’s sufficient to define the probability and give the final result. The intermediate working should be part of the code, which appears in Appendix B.\nFollowing APA style, do not use a zero before the decimal point when reporting probabilities.\n\n\n\nReopen last week’s Rmd file, as you will continue working on last week’s data about the top 3 most frequent movie genres, and their ratings.1\n\n\n\nWhat’s the probability of a movie being rated as Good, i.e. \\(P(Good)\\)?2\n\n\n\nGiven that a viewer watched a Drama movie, what’s the probability of them giving a good rating, i.e. \\(P(Good \\mid Drama)\\)?3\n\n\n\nGiven that a viewer watched an Action movie, what’s the probability of them giving a good rating, i.e. \\(P(Good \\mid Action)\\)?\nGiven that a viewer watched a Comedy movie, what’s the probability of them giving a good rating, i.e. \\(P(Good \\mid Comedy)\\)?\nDo you think that a movie receiving a Good rating is independent of the Genre?4\n\n\nNow let’s move on from independence. Given that a viewer gave a bad rating, what’s the probability of them having watched a non-drama movie? 5\n\n\n\n\n\n\n\n\nAdvanced material\n\n\n\n\n\nNote the following rules of set theory:\n\n\\((A_1 \\cup A_2) \\cap B = (A_1 \\cap B) \\cup (A_2 \\cap B)\\)\n\\((A_1 \\cap A_2) \\cup B = (A_1 \\cup B) \\cap (A_2 \\cup B)\\)\n\nConsider three mutually exclusive events \\(A_1, A_2, A_3\\) and another event \\(B\\). We have that:\n\\[\n\\begin{aligned}\nP\\big(A_3^c \\mid B\\big)\n&= \\frac{P(A_3^c \\cap B)}{P(B)} \\\\\n&= \\frac{P\\big((A_1 \\cup A_2) \\cap B\\big)}{P(B)} \\\\\n&= \\frac{P\\big((A_1 \\cap B) \\cup (A_2 \\cap B)\\big)}{P(B)} \\qquad \\text{[rule (1) above]}\\\\\n&= \\frac{P(A_1 \\cap B) + P(A_2 \\cap B)}{P(B)}\n\\end{aligned}\n\\]\nSuppose \\(A_1 = Action\\), \\(A_2 = Comedy\\), \\(A_3 = Drama\\), and \\(B = Bad\\).\n\\[\n\\begin{aligned}\nP\\big(Drama^c \\mid Bad\\big)\n&= \\frac{P\\big(Drama^c \\cap Bad\\big)}{P(Bad)} \\\\\n&= \\frac{P\\big((Action \\cup Comedy) \\cap Bad\\big)}{P(Bad)} \\\\\n&= \\frac{P\\big((Action \\cap Bad) \\cup (Comedy \\cap Bad)\\big)}{P(Bad)} \\\\\n&= \\frac{P(Action \\cap Bad) + P(Comedy \\cap Bad)}{P(Bad)}\n\\end{aligned}\n\\]\n\n\n\n\nBased on your analysis above, which movie Genre do you think lead studios should invest in for their next movie?6\n\n\n\nUsing a conditional mosaic plot, display the conditional distribution of movie genres being rated as either good or bad, making sure to add a main title and clear axis titles.7\n\n\n\nIn the analysis section of your report, write up a summary of your findings from above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions. In particular, focus on whether events were independent, and which genre of movie lead studios should consider investing in based on audience ratings.8"
  },
  {
    "objectID": "1_08_prob_rules.html#worked-example",
    "href": "1_08_prob_rules.html#worked-example",
    "title": "Probability Rules",
    "section": "\n2 Worked Example",
    "text": "2 Worked Example\nConsider the dataset available at https://uoepsy.github.io/data/RestaurantTips.csv, containing 157 observations on the following 7 variables:\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\n\n\n\nThese data were collected by the owner of a bistro in the US, who was interested in understanding the tipping patterns of their customers. The data are adapted from Lock et al. (2020).\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips.csv\")\nhead(tips)\n\n# A tibble: 6 × 7\n   Bill   Tip Credit Guests Day   Server PctTip\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2\n2  36.1  7    n           3 f     B        19.4\n3  32.0  5.01 y           2 f     A        15.7\n4  17.4  3.61 y           2 f     B        20.8\n5  15.4  3    n           2 f     B        19.5\n6  18.6  2.5  n           2 f     A        13.4\n\n\nWorking with the “Tip_Avg” variable created last week, we can see our relative frequency table for all of our servers (A, B, and C) who were tipped either Above or Below the standard tipping rate in the US (i.e., 15%).\n\ntips2 &lt;- tips |&gt;\n    mutate(Tip_Avg = ifelse(PctTip &lt;= 15, 'Below', 'Above'))\n\nrel_freq_tbl &lt;- table(tips2$Server, tips2$Tip_Avg) |&gt;\n    prop.table() |&gt;\n    addmargins()\nrel_freq_tbl\n\n     \n           Above      Below        Sum\n  A   0.25477707 0.12738854 0.38216561\n  B   0.25477707 0.15923567 0.41401274\n  C   0.12738854 0.07643312 0.20382166\n  Sum 0.63694268 0.36305732 1.00000000\n\n\n\nWhat’s the probability of a customer tipping above average?\n\nEither sum the three individual probabilities from the table above, i.e. \n\\[\nP(Above) = P(Server~A \\cap Above) + P(Server~B \\cap Above) + P(Server~C \\cap Above)\n\\]\n\n# P(Above) = \n#   P(Server A AND Above) + P(Server B AND Above) + P(Server C AND Above)\n0.25477707 + 0.25477707 + 0.12738854\n\n[1] 0.6369427\n\n\nOr obtain it from the row of sums. Alternatively, use indexing to extract the value from the table:\n\n# indexing: table[row number, col number]\nrel_freq_tbl[4, 1]\n\n[1] 0.6369427\n\n# or indexing: table[row name, col name]\nrel_freq_tbl['Sum', 'Above']\n\n[1] 0.6369427\n\n\n\nWe can write this up as: The probability of a customer tipping above average was \\(P(Above) = .64\\)\n\n\nGiven that the server was A, what’s the probability of a customer tipping above average?\n\n\n# P(Above | Server A)\n0.25477707 / 0.38216561\n\n[1] 0.6666667\n\n\n\nWe can write this up as: Given that the server was A, the probability of a customer tipping above average was \\(P(Above \\mid Server~A) = .67\\)\n\n\nGiven that the server was B, what’s the probability of a customer tipping above average?\n\n\n# P(Above | Server B)\n0.25477707 / 0.41401274\n\n[1] 0.6153846\n\n\n\nWe can write this up as: Given that the server was B, the probability of a customer tipping above average was \\(P(Above \\mid Server~B) = .62\\)\n\n\nGiven that the server was C, what’s the probability of a customer tipping above average?\n\n\n# P(Above | Server C)\n0.12738854 / 0.20382166\n\n[1] 0.625\n\n\n\nWe can write this up as: Given that the server was C, the probability of a customer tipping above average was \\(P(Above \\mid Server~C) = .63\\)\n\n\nIs tipping above average independent of the server?\n\nNo, the events seem to be dependent, but very weakly. The conditional probabilities of tipping above average for each server are different from the marginal one, \\(P(Above)\\), even though to a small extent. In particular, the probability of tipping above average after service from Server A is higher than the others.\n\nGiven that a customer tipped below average, what’s the probability of them being not being served by server A?\n\n\n# P(Server A^c | Below) = \n#   P( (Server B OR Server C) | Below ) = \n#   ( P( Server B AND Below ) + P( Server C AND Below ) ) / P(Below) = \n(0.15923567 + 0.07643312) / 0.36305732\n\n[1] 0.6491228\n\n\n\nGiven that a customer tipped below average, the probability of the customer not being served by server A was \\(P((Server \\ A)^c \\mid Below) = .65\\)\n\n\nBased on your analysis above, which server do you think offers the best customer service based on their tips?\n\nServer A appears to offer the best service to their customers, based solely on their personal tips - they had a much higher probability of receiving an above average tip (.67) than a below average tip (.33). Furthermore, the probability that a customer was served by either B or C, given that they tipped below average, was .65. This indicates that server A had the minority of the customers tipping below average.\n\nTo visualise our findings, we could use a conditional mosaic plot:\n\n\nlibrary(ggmosaic)\nmos_cond_plot &lt;- ggplot(tips2) +\n    geom_mosaic(aes(x = product(Tip_Avg), fill = Tip_Avg, conds = product(Server))) +  \n    labs(title = \"Conditional Association between Servers and Tips\", \n         x = \"Server\", \n         y = \"Tip Average\", \n         fill = \"Tip Average\")\nmos_cond_plot\n\n\n\n\n\n\nFigure 2: Conditional Association between Servers and Tips\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nIt was more likely for customers to tip above (64%) than below (36%) average. Though it was likely that all servers would receive an above average tip, tipping did not appear to be independent of server, based on conditional probabilities. Based on their personal tips, Server A appeared to offer the best service, where they were more likely to receive an above average tip (67%). Servers B and C were almost equally likely to receive above average tips (62% and 63% respectively). These associations are visually represented in Figure 2. Furthermore, among all customers that tipped below average, the majority was served by B or C (65%).\n\n\n\n\n\n\n\n\nAdvanced material: Three definitions of independence\n\n\n\n\n\nRecall the frequency table\n\nrel_freq_tbl\n\n     \n           Above      Below        Sum\n  A   0.25477707 0.12738854 0.38216561\n  B   0.25477707 0.15923567 0.41401274\n  C   0.12738854 0.07643312 0.20382166\n  Sum 0.63694268 0.36305732 1.00000000\n\n\nThe following three definitions of independence are equivalent. Two events \\(A\\) and \\(B\\) are independent if one of these holds:\n\n\\(P(A | B) = P(A)\\)\nor \\(P(B | A) = P(B)\\)\n\nor \\(P(A \\cap B) = P(A) P(B)\\)\n\n\nFor now, let’s focus on the third definition. To see if tipping above average is independent of the specific server, we can checks that condition separately for each server:\nA. Is \\(P(Server A \\cap Above)\\) equal to \\(P(Server A) P(Above)\\)?\n\n# 0.25477707 is P(Server A ∩ Above)\n0.38216561 * 0.63694268 # P(Server A) * P(Above)\n\n[1] 0.2434176\n\n\nB. Is \\(P(Server B \\cap Above)\\) equal to \\(P(Server B) P(Above)\\)?\n\n# 0.25477707 is P(Server B ∩ Above)\n0.41401274 * 0.63694268 # P(Server B) * P(Above)\n\n[1] 0.2637024\n\n\nC. Is \\(P(Server C \\cap Above)\\) equal to \\(P(Server C) P(Above)\\)?\n\n# 0.12738854 is P(Server C ∩ Above)\n0.20382166 * 0.63694268 # P(Server C) * P(Above)\n\n[1] 0.1298227\n\n\nFor server A and B, the values are close enough but not exactly equal. However, for server C, the values are identical up to the 2nd decimal place. This suggests the events are dependent, but to a small extent."
  },
  {
    "objectID": "1_08_prob_rules.html#student-glossary",
    "href": "1_08_prob_rules.html#student-glossary",
    "title": "Probability Rules",
    "section": "\n3 Student Glossary",
    "text": "3 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\nconds\n?"
  },
  {
    "objectID": "1_08_prob_rules.html#footnotes",
    "href": "1_08_prob_rules.html#footnotes",
    "title": "Probability Rules",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\n\nHint: For the starwars example data, if we were to ask what is the probability of a species being short, i.e. P(short), we would look at the relative frequencies table:\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10526316 0.02631579 0.13157895\n  Ewok    0.02631579 0.00000000 0.02631579\n  Human   0.34210526 0.44736842 0.78947368\n  Wookiee 0.00000000 0.05263158 0.05263158\n  Sum     0.47368421 0.52631579 1.00000000\n\n\nAnd calculate the following:\n\n# P(short) = P(Droid AND short) + P(Ewok AND short) \n#            + P(Human AND short) + P(Wookiee AND short)\n0.10526316 + 0.02631579 + 0.34210526 + 0.00000000\n\n[1] 0.4736842\n\n\nAlternatively, we would use look in the “Sum” value under the “short” column of our table.\n\nWe can write this up as: the probability of a Starwars character being short was .47.\n\n↩︎\n\n\nHint: For the starwars example data, if we were to ask what is the probability of being short for the Human species, i.e. P(short | human), we would use the relative frequencies table\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10526316 0.02631579 0.13157895\n  Ewok    0.02631579 0.00000000 0.02631579\n  Human   0.34210526 0.44736842 0.78947368\n  Wookiee 0.00000000 0.05263158 0.05263158\n  Sum     0.47368421 0.52631579 1.00000000\n\n\nAnd we would calculate the following:\n\n# P(short | human) = P(short AND human) / P(human) = \n0.34210526 / 0.78947368\n\n[1] 0.4333333\n\n\n\nWe can write this up as: given that a viewer watched a drama movie, the probability that they rated it as good was .43.\n\n↩︎\n\n\nHint: Recall that we say that two events \\(A\\) and \\(B\\) are independent if knowing that one occurred doesn’t change the probability of the other occurring, i.e. \\(P(A | B) = P(A)\\).\nIn the starwars example, we would say that being short seemed to be dependent on the species, given that the conditional probabilities are different from P(short). For example, the probability of being short for Ewoks and Droids is much higher than P(short):\n\n# P(short) = .47\n# P(short | droid) = .80\n# P(short | ewok) = 1\n# P(short | human) = .43\n# P(short | wookiee) = 0\n\n↩︎\n\n\nHint: For the Starwars example data, if we were to ask what is the probability of a character being not a human given they are short, we would use the relative frequencies table\n\nswars_rel_freq_sum\n\n\n               short       tall        Sum\n  Droid   0.10526316 0.02631579 0.13157895\n  Ewok    0.02631579 0.00000000 0.02631579\n  Human   0.34210526 0.44736842 0.78947368\n  Wookiee 0.00000000 0.05263158 0.05263158\n  Sum     0.47368421 0.52631579 1.00000000\n\n\nAnd then calculate the following:\n\n# P(human^c | short) = \n#   P(human^c AND short) / P(short)\n#   ( P(droid AND short) + P(ewok AND short) + P(wookiee AND short) ) / P(short)\n(0.10526316 + 0.02631579 + 0.00000000) / 0.47368421\n\n[1] 0.2777778\n\n\n\nWe can write this up as: Given that a Starwars character is short, the probability that they are a non-human is .28.\n\n↩︎\n\nHint: Here it would be useful to think about which movie genre offers the best audience experience - lead studios will likely want to invest in making movies in Genres that people enjoy watching!↩︎\n\nHint: Make sure to load the ggmosaic package so that you can specify geom_mosaic() when building your plot. To add a title, as well as x- and y-axis titles, specify labs(title = , x = , y = ). This week we will also need to specify the conds() argument.\nExample: For the starwars dataset, I create a mosaic plot using the following code, and specify conds() within my aes() argument:\n\nlibrary(ggmosaic)\nm_plot &lt;- ggplot(starwars2) +\n    geom_mosaic(aes(x = product(size), fill = size, conds = product(species))) +\n    labs(title = \"Starwars Conditional Mosaic Plot Example\", \n         x = \"Species\", \n         y = \"Size\",\n         fill = \"Size\")\nm_plot\n\n\n\n\n\n\nFigure 1: Starwars Mosaic Plot Example Title\n\n\n\n\n↩︎\n\n\nHint: You may want to consider using proper notation in your write-up (as you have seen in lectures). To do so, you can use $ equation $. For example, if I wanted to specify union, in the text section of the Rmd file (i.e., not a code chunk) I could write $P(A \\cup B)$. This would render in my main file as:\n\\(P(A \\cup B)\\)\nTo get the \\(\\cup\\) symbol, write $P(A \\cup B)$.\nTo get the \\(\\cap\\) symbol, write $P(A \\cap B)$.↩︎"
  },
  {
    "objectID": "1_10_cont_dist.html",
    "href": "1_10_cont_dist.html",
    "title": "Random Variables (Continuous)",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\n\n\n\n\n\nNext week: Submission of Formative report B\nNext week, your group must submit one PDF file for formative report B by 12 noon on Friday the 29th of November 2024. Name your submission: Group NUMBER.LETTER Formative B.pdf\nOne person must submit on behalf of the entire group and let the group know when they have submitted by leaving a note on the Group Discussion Space.\nTo submit go to the course Learn page, click “Assessment”, then click “Submit Formative Report B (PDF file only)”.\nNo extensions. As mentioned in the Assessment Information page, no extensions are possible for group-based reports.\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.B4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback."
  },
  {
    "objectID": "1_10_cont_dist.html#formative-report-b",
    "href": "1_10_cont_dist.html#formative-report-b",
    "title": "Random Variables (Continuous)",
    "section": "",
    "text": "Instructions and data were released in week 7.\n\n\n\n\n\n\n\n\n\nNext week: Submission of Formative report B\nNext week, your group must submit one PDF file for formative report B by 12 noon on Friday the 29th of November 2024. Name your submission: Group NUMBER.LETTER Formative B.pdf\nOne person must submit on behalf of the entire group and let the group know when they have submitted by leaving a note on the Group Discussion Space.\nTo submit go to the course Learn page, click “Assessment”, then click “Submit Formative Report B (PDF file only)”.\nNo extensions. As mentioned in the Assessment Information page, no extensions are possible for group-based reports.\n\n\n\n\nFor formative report B, you will be asked to perform the following tasks, each related to a week of teaching in this course.\nThis week’s task is highlighted in bold below. Please only focus on completing that task this week. In the next section, you will also find the guided sub-steps that you need to consider to complete this week’s task.\n\nB1) Create a new categorical variable, Rating, taking the value ‘Good’ if the audience score is &gt; 50, and ‘Bad’ otherwise. Inspect and describe the joint probability distribution of movie genre and rating.\nB2) Investigate if a movie receiving a good rating is independent of the genre.\nB3) Computing and plotting probabilities with a binomial distribution.B4) Computing and plotting probabilities with a normal distribution.\nB5) Finish the report write-up and formatting, knit to PDF, and submit the PDF for formative feedback."
  },
  {
    "objectID": "1_10_cont_dist.html#b4-sub-tasks",
    "href": "1_10_cont_dist.html#b4-sub-tasks",
    "title": "Random Variables (Continuous)",
    "section": "\n2 B4 sub-tasks",
    "text": "2 B4 sub-tasks\nThis week you will only focus on task B4. Below there are sub-steps you need to consider to complete task B4.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo see the hints, hover your cursor on the superscript numbers.\nFocus on completing all of the lab tasks, and leave non-essential things like changing colours for later.\n\n\n\n\n\n\n\n\nContext\n\n\n\nAs detailed last week, a new movie theatre is opening in Texas soon. The management team are thinking of innovative launch events, and they’d like to host a movie trivia night where viewers can compete to win free cinema tickets for the year! They want to make sure however that their questions aren’t too easy, or too difficult, so gave viewers an IQ test so that they could see where to pitch their questions.\nAccording to a recent survey, the average IQ in Texas is 97.4. The management team decided they are satisfied with their questions’ level of difficulty if the probability of movie viewers’ having an IQ score above 97.4 is between 50% and 60%. Otherwise, they will adjust the difficulty of the questions by making them easier (if the probability is &lt;50%) or harder (&gt;60%).\nIn this lab, you will need to consider the variables IQ1 to IQ50 (i.e. the IQ score of each of the 50 audience raters for each movie) from the Hollywood movies dataset when answering the questions below.\n\n\n\nReopen last week’s Rmd file, and continue building on last week’s work. Make sure you are still using the movies dataset filtered to only include the top 3 genres.1\n\n\n\nConsider the IQ1-IQ50 variables, are they discrete or continuous?2\n\n\n\n\n\n\n\n\nREADME: Wide vs long data\n\n\n\n\n\nCurrently the IQ data are in wide format, i.e. each movie has multiple columns providing the IQ scores for each of the 50 audience raters. However, later on we will want to plot the IQ scores of all raters across all movies. To do so, ggplot wants a single column of IQ scores into aes().\nConsider this fictitious dataset in wide form:\n\ndf &lt;- tibble(\n    movie = c('mov1', 'mov2', 'mov3'),\n    IQ1   = c(91, 92, 93),\n    IQ2   = c(101, 102, 103)\n)\ndf\n\n# A tibble: 3 × 3\n  movie   IQ1   IQ2\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 mov1     91   101\n2 mov2     92   102\n3 mov3     93   103\n\n\nTo go from two columns of IQ scores to a single one, we can get the data in long form as follows:\n\ndf_long &lt;- df |&gt;\n    pivot_longer(IQ1:IQ2, names_to = \"IQ_ID\", values_to = \"IQ_Scores\")\ndf_long\n\n# A tibble: 6 × 3\n  movie IQ_ID IQ_Scores\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 mov1  IQ1          91\n2 mov1  IQ2         101\n3 mov2  IQ1          92\n4 mov2  IQ2         102\n5 mov3  IQ1          93\n6 mov3  IQ2         103\n\n\nAs you see, the above is just a different rearrangement of the same data. The function pivot_longer takes the range of columns you want to compress (IQ1 to IQ2 in this case), and the names of 2 new columns to create:\n\n\nnames_to: where the names of the columns to compress will go into - in this case, “IQ_ID”\n\nvalues_to: where the values of the columns to compress will go into - in this case, “IQ_Scores”\n\nYou can also go from long data back to wide data. To do so, use the pivot_wider function:\n\ndf_long\n\n# A tibble: 6 × 3\n  movie IQ_ID IQ_Scores\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 mov1  IQ1          91\n2 mov1  IQ2         101\n3 mov2  IQ1          92\n4 mov2  IQ2         102\n5 mov3  IQ1          93\n6 mov3  IQ2         103\n\ndf_long |&gt; \n    pivot_wider(names_from = \"IQ_ID\", values_from = \"IQ_Scores\")\n\n# A tibble: 3 × 3\n  movie   IQ1   IQ2\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 mov1     91   101\n2 mov2     92   102\n3 mov3     93   103\n\n\n\n\n\n\nUsing the pivot_longer() function, pivot the data so that it is in long format. Ensure the column names go into a new column called “IQ_ID”, and the values go into a column called “IQ_Scores”. Store the data in long format in a new object.3\n\n\n\n\n\n\n\n\nREADME: Independence\n\n\n\n\n\nAs each movie was rated by a different pool of 50 audience raters, the variable IQ_Scores can be considered an independent sample of IQ scores.\nThis is because a single audience rater didn’t appear in more than one movie. Even if the column IQ_ID says “IQ1” for multiple movies, the rater with id “IQ1” for movie “American Reunion”, say, won’t be the same rater with id “IQ1” for movie “Battleship”.\n\n\n\n\nPlot the sample distribution of the IQ_Scores variable either using a histogram or density plot.4\n\n\n\nWhat kind of distribution does your plot follow? Estimate the parameters of your distribution from the sample data.5\n\n\n\nPlot the fitted normal distribution on top of the sample distribution. Is the normal distribution a good fit?6\n\n\n\nUsing the normal distribution, calculate the following:7\n\nThe value of IQ that 25% of movie watchers have scores equal to or less than\nThe value of IQ that 50% of movie watchers have scores equal to or less than\nThe value of IQ that 75% of movie watchers have scores equal to or less than \n\n\n\n\n\nHow do the normal quartiles computed above compare to those obtained from the summary() function? If they are similar, this will provide further evidence that the normal distribution is a good model for IQ scores.\nUsing the normal distribution, calculate the probability of a movie viewer having an IQ score &gt;97.4, i.e. more than the Texas average.8\n\n\nBased on the probabilities you have reported above, do you think that the new movie theatre should simplify the questions for their trivia night, make them harder, or make no changes? Justify your answer.\nIn the analysis section of your report, write up a summary of what you have reported above, using proper rounding to 2 decimal places and avoiding any reference to R code or functions."
  },
  {
    "objectID": "1_10_cont_dist.html#worked-example",
    "href": "1_10_cont_dist.html#worked-example",
    "title": "Random Variables (Continuous)",
    "section": "\n3 Worked Example",
    "text": "3 Worked Example\nThe dataset available at https://uoepsy.github.io/data/RestaurantTips2.csv was collected by the owner of a US bistro, and contains 99 observations on 10 variables. It is a subset of the RestaurantTips.csv data presented in the past weeks, focusing only on parties of 2 people.9\nFollowing from the bistro owner’s interest in whether they should consider introducing a 2 for 1 coffee deal or a loyalty scheme, they are also considering running a weekly quiz night in the bistro. The quiz is only open to pairs of individuals, and given that the bistro is located in a student area, the owner wants to make the quiz very demanding, and therefore wants the questions to be at a level where someone with a ‘superior’ or ‘well above average’ IQ (i.e., scores 120+) would be challenged. We need to advise the bistro owner whether the questions that they have generated are at their desired level of difficulty.\n\n\n\n\nVariable Name\nDescription\n\n\n\nBill\nSize of the bill (in dollars)\n\n\nTip\nSize of the tip (in dollars)\n\n\nCredit\nPaid with a credit card? n or y\n\n\nGuests\nNumber of people in the group\n\n\nDay\nDay of the week: m=Monday, t=Tuesday, w=Wednesday, th=Thursday, or f=Friday\n\n\nServer\nCode for specific waiter/waitress: A, B, or C\n\n\nPctTip\nTip as a percentage of the bill\n\n\nHadCoffee\nNumber of guests in the group who had coffee\n\n\nIQ1\nScore on IQ test for guest 1\n\n\nIQ2\nScore on IQ test for guest 2\n\n\n\n\n\n\nlibrary(tidyverse)\ntips2 &lt;- read_csv(\"https://uoepsy.github.io/data/RestaurantTips2.csv\")\nhead(tips2)\n\n# A tibble: 6 × 10\n   Bill   Tip Credit Guests Day   Server PctTip HadCoffee   IQ1   IQ2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  23.7 10    n           2 f     A        42.2         2    93   100\n2  32.0  5.01 y           2 f     A        15.7         2    96    98\n3  17.4  3.61 y           2 f     B        20.8         2    94    99\n4  15.4  3    n           2 f     B        19.5         2    99   108\n5  18.6  2.5  n           2 f     A        13.4         2   129   106\n6  21.6  3.44 n           2 f     B        16           2    82   118\n\n\n\nIf we were asked to describe what kind of variables IQ1 and IQ2 were, and to comment on the kind of probability distribution it may follow, we could say:\n\nIQ1 and IQ2 represent the IQ scores of each individual within a group of two. These are both continuous variables that could be modeled by a normal distribution.\n\nWe could transform the tips2 dataset from wide to long format using the following command:\n\n\ntips2_long &lt;- tips2 |&gt;\n    pivot_longer(IQ1:IQ2, names_to = \"ID\", values_to = \"IQ_Scores\")\ntips2_long\n\n# A tibble: 198 × 10\n    Bill   Tip Credit Guests Day   Server PctTip HadCoffee ID    IQ_Scores\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  23.7 10    n           2 f     A        42.2         2 IQ1          93\n 2  23.7 10    n           2 f     A        42.2         2 IQ2         100\n 3  32.0  5.01 y           2 f     A        15.7         2 IQ1          96\n 4  32.0  5.01 y           2 f     A        15.7         2 IQ2          98\n 5  17.4  3.61 y           2 f     B        20.8         2 IQ1          94\n 6  17.4  3.61 y           2 f     B        20.8         2 IQ2          99\n 7  15.4  3    n           2 f     B        19.5         2 IQ1          99\n 8  15.4  3    n           2 f     B        19.5         2 IQ2         108\n 9  18.6  2.5  n           2 f     A        13.4         2 IQ1         129\n10  18.6  2.5  n           2 f     A        13.4         2 IQ2         106\n# ℹ 188 more rows\n\n\n\n\npivot_longer()\nPivot longer takes “wide” data and transforms them to “long” form. An example is given below:\n\ntoy_data &lt;- tibble(\n    a = c(1, 2, 3),\n    x1 = c(11, 12, 13),\n    x2 = c(21, 22, 23)\n)\ntoy_data\n\n# A tibble: 3 × 3\n      a    x1    x2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    11    21\n2     2    12    22\n3     3    13    23\n\n\n\nlong_toy_data &lt;- toy_data |&gt;\n    pivot_longer(x1:x2, names_to = \"x\", values_to = \"score\")\nlong_toy_data\n\n# A tibble: 6 × 3\n      a x     score\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 x1       11\n2     1 x2       21\n3     2 x1       12\n4     2 x2       22\n5     3 x1       13\n6     3 x2       23\n\n\n\nWe can plot the sample distribution of the IQ_Scores variable either using a histogram or a density plot.\n\nLet’s plot both the histogram and density plot for instructional purposes only, in a report you would only report one of the two as they show similar information. We plot both now for instructional purposes and, in particular, we ask you to focus on the differences in the y axes.\n\nlibrary(patchwork)\nplt_hist &lt;- ggplot(tips2_long, aes(x = IQ_Scores)) + \n    geom_histogram(colour = 'white') +\n    labs(x = \"IQ scores\")\n\nplt_dens &lt;- ggplot(tips2_long, aes(x = IQ_Scores)) + \n    geom_density() + \n    labs(x = \"IQ scores\")\n\nplt_hist | plt_dens \n\n\n\n\n\n\n\nAs you will notice, the plot on the left panel shows counts on the y axis, i.e. the absolute frequency of each bin interval. The plot on the right panel, instead, shows the density on the y axis. If we wanted to plot on top of the histogram a normal curve, we would need to change the y axis of the histogram to be a comparable measure, i.e. also a density rather than a count.\nYou can do so by adding y = after_stat(density) in the aes() specification, and you can now see that both y axes agree:\n\nplt_hist &lt;- ggplot(tips2_long, aes(x = IQ_Scores, y = after_stat(density))) + \n    geom_histogram(colour = 'white') +\n    labs(x = \"IQ scores\")\n\nplt_dens &lt;- ggplot(tips2_long, aes(x = IQ_Scores)) + \n    geom_density() + \n    labs(x = \"IQ scores\")\n\nplt_hist | plt_dens \n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a distribution\n\n\n\n\n\nFitting a normal distribution to data involves estimating the parameters of the distribution from the data. In other words, we want to find values for \\(\\mu\\) and \\(\\sigma\\) from the variable IQ_Scores in the our data, and we denote the estimated parameters with \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\). You create those symbols, you can type in the Rmd file $\\hat{\\mu}$ and $\\hat{\\sigma}$.\n\n\n\n\n\nWe can now fit a normal distribution to the variable. To do so, we need to start by estimating the parameters of the normal distribution:\n\n\n\\(\\hat{\\mu}\\), the mean IQ score in the sample\n\n\\(\\hat{\\sigma}\\), the estimated standard deviation in the sample\n\n\n\n\n# The mean of the sample of IQ scores\ntips_mu_hat &lt;- mean(tips2_long$IQ_Scores) \ntips_mu_hat\n\n[1] 99.33333\n\n# The standard deviation of the sample of IQ scores\ntips_sigma_hat &lt;- sd(tips2_long$IQ_Scores)\ntips_sigma_hat\n\n[1] 15.7025\n\n\n\nWe can then compare the sample distribution to the normal distribution, and comment on whether the normal fit is good:\n\nFirst we will create a tibble with the normal distribution at a grid of x values.\n\n\nWhy M \\(\\pm\\) 3 * SD?\n\nTypically, the interval M \\(\\pm\\) 3 * SD includes approximately 100% of the distribution values (to be specific, 99.7%). See here for more information!\nIf you wanted, you can always extend the interval to be M \\(\\pm\\) 4 * SD or M \\(\\pm\\) 5 * SD if you want to extend your normal distribution plot.\n\nnormal_distr &lt;- tibble(\n    x_grid = seq(tips_mu_hat - 3 * tips_sigma_hat, \n                 tips_mu_hat + 3 * tips_sigma_hat, \n                 by = 0.1),\n    y_grid = dnorm(x_grid, mean = tips_mu_hat, sd = tips_sigma_hat)\n)\n\nWe can plot the sample distribution and put the fitted normal distribution on top:\n\nbistro_iq &lt;- ggplot() + \n    geom_histogram(data = tips2_long, aes(x = IQ_Scores, y = after_stat(density)), \n                   color = 'white') +\n    geom_line(data = normal_distr, aes(x = x_grid, y = y_grid),\n              color = 'red', linewidth = 2) +\n    labs(x = \"IQ scores\")\nbistro_iq\n\n\n\n\n\n\nFigure 1: Distribution of Bistro Customer IQ Scores.\n\n\n\n\nIn the code above, linewidth = 2 changes the default line thickness from 1 to 2. You can also change the line type to be dashed, if you wish to, using linetype = 2.\nThe normal distribution seems to be a good fit for the sample distribution, as the shape of the histogram tends to agree with the normal distribution across all possible values. The histogram appears to be symmetric around the mean and follows the normal bell-shaped curve.\n\nWe can calculate the value of IQ that a specific percentage of bistro customers have scores equal to or less than, for example:\n\n\ns.t. means such that\n\n\n\nx s.t. P(X &lt;= x) = 0.25\nx s.t. P(X &lt;= x) = 0.50\nx s.t. P(X &lt;= x) = 0.75\n\n\n\nTo calculate the value of IQ that 25% of bistro customers have scores equal to or less than, we can compute the following:\n\nqnorm(p = 0.25, mean = tips_mu_hat, sd = tips_sigma_hat)\n\n[1] 88.74216\n\n\n\n\nTo calculate the value of IQ that 50% of bistro customers have scores equal to or less than, we can compute the following:\n\nqnorm(0.5, tips_mu_hat, tips_sigma_hat)\n\n[1] 99.33333\n\n\n\n\nTo calculate the value of IQ that 75% of bistro customers have scores equal to or less than, we can compute the following:\n\nqnorm(0.75, tips_mu_hat, tips_sigma_hat)\n\n[1] 109.9245\n\n\n\n\n\n\nWe can also comment on how these normal quartiles values compare to those from the summary() output:\n\n\nsummary(tips2_long$IQ_Scores)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  57.00   90.25   98.00   99.33  109.75  154.00 \n\n\nThe quartiles from the fitted normal distribution are similar to the sample ones (i.e., those returned by summary()). This provides further evidence that a normal distribution is a good model for the sample of IQ scores.\n\nTo summarise the IQ scores of the bistro customers, we could compute the interval comprising the middle 95% of the customers IQ scores:\n\nIf we want 0.95 probability in the middle, we need to split the leftover 0.05 probability equally between the two tails: 0.025 in the left, and 0.025 in the right. However, qnorm() wants the probability to the left, so we will give 0.025 and 0.975 = 1 - 0.025 as values.\n\nqnorm(c(0.025, 0.975), mean = tips_mu_hat, sd = tips_sigma_hat)\n\n[1]  68.5570 130.1097\n\n\n95% of the bistro customers had IQ scores between 69 and 130.\n\n\n\n\n\n\nAlternatively\n\n\n\n\n\nAlternatively, to specify that one probability is on the left tail and the other is on the upper tail, you have to run two separate statements:\n\nqnorm(0.025, mean = tips_mu_hat, sd = tips_sigma_hat)\n\n[1] 68.557\n\nqnorm(0.025, mean = tips_mu_hat, sd = tips_sigma_hat, lower.tail = FALSE)\n\n[1] 130.1097\n\n\n\n\n\n\n\nFor a classification of IQ scores, see the Wechsler Intelligence Scales classification table.\n\nWe can use the fitted normal distribution to calculate the probabilities, for example:\n\n\n\nP(X &lt; 120)\nP(X &gt; 120)\nP(X &gt; 100)\nP(100 &lt; X &lt; 130)\n\n\n\nTo calculate the probability that a bistro customer has an IQ below 120, we can compute the following:\n\npnorm(120, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.9059362\n\n\n\n\nTo calculate the probability that a bistro customer has an IQ above 120, we can compute the following:\n\n# P(X &gt; 120)\npnorm(120, tips_mu_hat, tips_sigma_hat, lower.tail = FALSE)\n\n[1] 0.09406377\n\n\nor alternatively\n\n# 1 - P(X &lt; 120)\n1 - pnorm(120, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.09406377\n\n\n\n\nTo calculate the probability that a bistro customer has an IQ above 100, we can compute the following:\n\n# P(X &gt; 100)\npnorm(100, tips_mu_hat, tips_sigma_hat, lower.tail = FALSE)\n\n[1] 0.4830676\n\n\nor alternatively\n\n# 1 - P(X &lt; 100)\n1 - pnorm(100, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.4830676\n\n\n\n\nTo calculate the probability that a bistro customer has an IQ between 100 and 130, we can compute the following:\nP(a &lt; X &lt; b) = P(X &lt; b) - P(X &lt; a) when a = 100 and b = 130 so that P(100 &lt; X &lt; 130) = P(X &lt; 130) - P(X &lt; 100)\n\n# P(100 &lt; X &lt; 130) = \n# P(X &lt; 130) - P(X &lt; 100)\npnorm(130, tips_mu_hat, tips_sigma_hat) - pnorm(100, tips_mu_hat, tips_sigma_hat)\n\n[1] 0.4576566\n\n\n\n\n\n\n\n\n\n\n\nExample writeup\n\n\n\nFigure 1 displays the distribution of IQ scores, with a normal distribution superimposed as a red curve. Most of the bistro customers (95%) have IQ scores between 69 and 130. Based on our analysis above, 48% of bistro customers are likely to have IQ scores above 100, though there is less than a 10% chance that customers have superior IQ scores (i.e., above 120). Therefore the bistro owner has set the questions to his quiz at the desired level of difficulty - the vast majority of his customers should find the questions extremely challenging. However, if the Bistro owner did want to make some changes, they should only consider making the quiz more difficult."
  },
  {
    "objectID": "1_10_cont_dist.html#student-glossary",
    "href": "1_10_cont_dist.html#student-glossary",
    "title": "Random Variables (Continuous)",
    "section": "\n4 Student Glossary",
    "text": "4 Student Glossary\nTo conclude the lab, add the new functions to the glossary of R functions.\n\n\nFunction\nUse and package\n\n\n\ndnorm()\n?\n\n\npnorm()\n?\n\n\npivot_longer()\n?\n\n\nseq()\n?\n\n\ngeom_histogram()\n?\n\n\ngeom_density()\n?\n\n\ngeom_line()\n?\n\n\nafter_stat()\n?"
  },
  {
    "objectID": "1_10_cont_dist.html#footnotes",
    "href": "1_10_cont_dist.html#footnotes",
    "title": "Random Variables (Continuous)",
    "section": "Footnotes",
    "text": "Footnotes\n\nHint: access the Rmd file from the Group Discussion Space. If last week’s driver hasn’t uploaded it yet, please ask them to share it with the group via the Group Discussion Space, email, or Teams. To download the file from the server, go to the RStudio Files pane, tick the box next to the Rmd file, and select More &gt; Export.↩︎\nHint: Technically, the values you see in the data are discrete. In Psychology, however, IQ is assumed to be a continuous variable and it’s only discrete due to a limitation of our measuring ability.\nIn other words, if we were able to measure it more accurately, there would be no reason why this couldn’t be continuous and we could imagine getting an IQ of 126.4374325677…\nTo summarise, IQ is an example of a variable that is considered as coming from a continuous scale, but it can only be measured with discrete precision.↩︎\n\nHint: When the data is wide, we can make it long using pivot_longer(). When we make data longer, we’re essentially making lots of columns into 2 longer columns. Below, in the animation, the wide variable x, y and z go into a new longer column called \"name\" (specified by the argument names_to = \"name\") that specifies which (x/y/z) it came from, and the values get put into the \"val\" column (specified by the argument values_to = \"val\").\n\n\n\n\nSource: https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/\n\n\n\n↩︎\n\n\nHint: Here you will need to specify geom_histogram() or geom_density().\nTo save space, if you wish to plot this next to another plot, recall that you can use the patchwork package to arrange plots either adjacent to one another (via |) or stacked (via /).↩︎\n\nHint: A normal distribution has two parameters, the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)). The mean specifies where the normal distribution is centred at along the x-axis. The standard deviation specifies how spread out the normal distribution is.↩︎\n\nHint: Here you will need to consider using dnorm(), as you are interested in the normal distribution at different values of x.\nFirst, you will need to create a tibble with two columns:\n\nThe first column will contain a sequence of x-values (a grid) for which you want to compute the normal distribution curve at.\nThe second column will contain the y-values: the normal distribution at that sequence of x-values.\n\nnorm_distr &lt;- tibble(\n    IQ_Grid   = ... ,\n    IQ_Norm_Distr = ...\n)\nThis way, you will have the x-values (IQ_Grid) and the heights (IQ_Norm_Distr) at those x-values.\nWhen specifying your x-axis sequence, consider how many standard deviations above and below the mean to give. Typical values are 3, 4, or 2 - try! Alternatively, you can go from the minimum of your data to the maximum.\nFor the y-values, use the dnorm(x, mean, sd) function with inputs being the x-axis grid, the sample mean, and sample standard deviation.\nPlot this on top of the histogram/density using geom_line(data = ?, aes(x = ?, y = ?)). If you are using a histogram, make sure the y-axis are comparable: counts vs density isn’t appropriate!↩︎\n\n\nHint:\nYou are being asked to calculate the quantile x such that P(X &lt;= x) is 0.25, 0.50, and 0.75 respectively. That is, you are asked to find the first, second, and third quartiles respectively.\nTo obtain the quantile of a normal distribution, use the qnorm(p, mean, sd) function. This will return the x-value that has a probability p to its left.\n\n\n\n↩︎\n\n\nHint: Here you are being asked to calculate P(X &gt; x) = 1 - P(X &lt;= x).\nRecall that in a normal distribution:\n\nP(X &lt;= x) = P(X &lt; x)\nP(X &gt;= x) = P(X &gt; x)\n\n↩︎\n\nData adapted from Lock et al. (2020).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is the homepage of DAPR1 labs. Please use the sidebar to navigate to the lab materials for a specific week of teaching.\nIf you are looking for the lecture materials, please go to the course page on Blackboard Learn.\n\n\n\n Back to top"
  },
  {
    "objectID": "rd1_02.html",
    "href": "rd1_02.html",
    "title": "Categorical data",
    "section": "",
    "text": "Before we get started on the statistics, we’re going to briefly introduce a crucial bit of R code. We have seen already seen a few examples of R code such as:\n\n# show the dimensions of the data\ndim(somedata)\n\n# show a summary of the data\nsummary(somedata)\n\n# factorise and show the \"somevariable\" variable in the \"somedata\" dataframe \nas.factor(somedata$somevariable)\n\nAnd we can actually wrap functions inside functions:\n\n# factorise the \"somevariable\" variable in the \"somedata\" dataframe, \n# then show a summary of it\nsummary(as.factor(somedata$somevariable))\n\nR evaluates code from the inside-out!\nYou can end up with functions inside functions inside functions …\n\n# Don't worry about what all these functions do, \n# it's just an example -\nround(mean(log(cumsum(diff(1:10)))))\n\n[1] 1\n\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: |&gt;\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side. The |&gt; symbol gets called a “pipe”.\nLet’s see it in action with the starwars2 dataset. The data contains information on various characteristics of characters from Star Wars. Before we can use the pipe operator, |&gt;, we need to load the tidyverse packages, because that is where |&gt; is found.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nstarwars2 &lt;- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\nstarwars2 |&gt;\n    head()\n\n# A tibble: 6 × 6\n  name           height hair_color  eye_color homeworld species\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Skywalker    172 blond       blue      Tatooine  Human  \n2 C-3PO             167 &lt;NA&gt;        yellow    Tatooine  Human  \n3 R2-D2              96 &lt;NA&gt;        red       Naboo     Droid  \n4 Darth Vader       202 none        yellow    Tatooine  Human  \n5 Leia Organa       150 brown       brown     Alderaan  Human  \n6 Owen Lars         178 brown, grey blue      Tatooine  Human  \n\nstarwars2 |&gt;\n    summary()\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWe can now write code that requires reading it from the inside-out:\n\nsummary(as.factor(starwars2$homeworld))\n\nor which requires reading it from left to right:\n\nstarwars2$homeworld |&gt;\n    as.factor() |&gt;\n    summary()\n\n      Alderaan    Aleen Minor         Bespin     Bestine IV Cato Neimoidia \n             3              1              1              1              1 \n         Cerea       Champala      Chandrila   Concord Dawn       Corellia \n             1              1              1              1              2 \n     Coruscant       Dathomir          Dorin          Endor         Eriadu \n             3              1              1              1              1 \n      Geonosis    Glee Anselm     Haruun Kal        Iktotch       Iridonia \n             1              1              1              1              1 \n         Kalee         Kamino       Kashyyyk      Malastare         Mirial \n             1              3              2              1              2 \n      Mon Cala     Muunilinst          Naboo      Nal Hutta           Ojom \n             1              1              8              1              1 \n       Quermia          Rodia         Ryloth        Serenno          Shili \n             1              1              2              1              1 \n         Skako        Socorro    Springfield        Stewjon        Sullust \n             1              1              2              1              1 \n      Tatooine       Toydaria      Trandosha        Troiken           Tund \n            10              1              1              1              1 \n        Utapau        Vulpter          Zolan \n             1              1              1 \n\n\nAnd that long line of code from above:\n\n# again, don't worry about all these functions, \n# just notice the difference in the two styles.\nround(mean(log(cumsum(diff(1:10)))))\n\nbecomes:\n\n1:10 |&gt;\n    diff() |&gt;\n    cumsum() |&gt;\n    log() |&gt;\n    mean() |&gt;\n    round()\n\nWe’re going to use this way of writing a lot throughout the course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with |&gt;."
  },
  {
    "objectID": "rd1_02.html#a-different-style-of-r-code",
    "href": "rd1_02.html#a-different-style-of-r-code",
    "title": "Categorical data",
    "section": "",
    "text": "Before we get started on the statistics, we’re going to briefly introduce a crucial bit of R code. We have seen already seen a few examples of R code such as:\n\n# show the dimensions of the data\ndim(somedata)\n\n# show a summary of the data\nsummary(somedata)\n\n# factorise and show the \"somevariable\" variable in the \"somedata\" dataframe \nas.factor(somedata$somevariable)\n\nAnd we can actually wrap functions inside functions:\n\n# factorise the \"somevariable\" variable in the \"somedata\" dataframe, \n# then show a summary of it\nsummary(as.factor(somedata$somevariable))\n\nR evaluates code from the inside-out!\nYou can end up with functions inside functions inside functions …\n\n# Don't worry about what all these functions do, \n# it's just an example -\nround(mean(log(cumsum(diff(1:10)))))\n\n[1] 1\n\n\nWe can write in a different style, however, and this may help to keep code tidy and easily readable - we can write sequentially:\n\nNotice that what we are doing is using a new symbol: |&gt;\nThis symbol takes the output of whatever is on it’s left-hand side, and uses it as an input for whatever is on the right-hand side. The |&gt; symbol gets called a “pipe”.\nLet’s see it in action with the starwars2 dataset. The data contains information on various characteristics of characters from Star Wars. Before we can use the pipe operator, |&gt;, we need to load the tidyverse packages, because that is where |&gt; is found.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nstarwars2 &lt;- read_csv(\"https://uoepsy.github.io/data/starwars2.csv\")\nstarwars2 |&gt;\n    head()\n\n# A tibble: 6 × 6\n  name           height hair_color  eye_color homeworld species\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Skywalker    172 blond       blue      Tatooine  Human  \n2 C-3PO             167 &lt;NA&gt;        yellow    Tatooine  Human  \n3 R2-D2              96 &lt;NA&gt;        red       Naboo     Droid  \n4 Darth Vader       202 none        yellow    Tatooine  Human  \n5 Leia Organa       150 brown       brown     Alderaan  Human  \n6 Owen Lars         178 brown, grey blue      Tatooine  Human  \n\nstarwars2 |&gt;\n    summary()\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWe can now write code that requires reading it from the inside-out:\n\nsummary(as.factor(starwars2$homeworld))\n\nor which requires reading it from left to right:\n\nstarwars2$homeworld |&gt;\n    as.factor() |&gt;\n    summary()\n\n      Alderaan    Aleen Minor         Bespin     Bestine IV Cato Neimoidia \n             3              1              1              1              1 \n         Cerea       Champala      Chandrila   Concord Dawn       Corellia \n             1              1              1              1              2 \n     Coruscant       Dathomir          Dorin          Endor         Eriadu \n             3              1              1              1              1 \n      Geonosis    Glee Anselm     Haruun Kal        Iktotch       Iridonia \n             1              1              1              1              1 \n         Kalee         Kamino       Kashyyyk      Malastare         Mirial \n             1              3              2              1              2 \n      Mon Cala     Muunilinst          Naboo      Nal Hutta           Ojom \n             1              1              8              1              1 \n       Quermia          Rodia         Ryloth        Serenno          Shili \n             1              1              2              1              1 \n         Skako        Socorro    Springfield        Stewjon        Sullust \n             1              1              2              1              1 \n      Tatooine       Toydaria      Trandosha        Troiken           Tund \n            10              1              1              1              1 \n        Utapau        Vulpter          Zolan \n             1              1              1 \n\n\nAnd that long line of code from above:\n\n# again, don't worry about all these functions, \n# just notice the difference in the two styles.\nround(mean(log(cumsum(diff(1:10)))))\n\nbecomes:\n\n1:10 |&gt;\n    diff() |&gt;\n    cumsum() |&gt;\n    log() |&gt;\n    mean() |&gt;\n    round()\n\nWe’re going to use this way of writing a lot throughout the course, and it pairs really well with a group of functions in the tidyverse packages, which were designed to be used in conjunction with |&gt;."
  },
  {
    "objectID": "rd1_02.html#data-exploration",
    "href": "rd1_02.html#data-exploration",
    "title": "Categorical data",
    "section": "\n2 Data Exploration",
    "text": "2 Data Exploration\nOnce we have collected some data, one of the first things we want to do is explore it - and we can do this through describing (or summarising) and visualising variables.\nWe are already familiar with the function summary(), which provides high-level information about our data, showing us things such as the minimum and maximum and mean of continuous variables, or the numbers of entries falling into each possible response level for a categorical variable:\n\nsummary(starwars2)\n\n     name               height       hair_color         eye_color        \n Length:75          Min.   : 79.0   Length:75          Length:75         \n Class :character   1st Qu.:167.5   Class :character   Class :character  \n Mode  :character   Median :180.0   Mode  :character   Mode  :character  \n                    Mean   :176.1                                        \n                    3rd Qu.:191.0                                        \n                    Max.   :264.0                                        \n  homeworld           species         \n Length:75          Length:75         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\nWhat we are doing here is providing numeric descriptions of the distributions of values in each variable.\n\n\n\n\n\n\nDistribution\nThe distribution of a variable shows how often different values occur. We’re going to focus on describing and visualising distributions of categorical data.\nThe graph showing the distribution of a variable shows us where the values are centred, how the values vary, and gives some information about where a typical value might fall. It can also alert you to the presence of outliers (unexpected observations)."
  },
  {
    "objectID": "rd1_02.html#unordered-categorical-nominal-data",
    "href": "rd1_02.html#unordered-categorical-nominal-data",
    "title": "Categorical data",
    "section": "\n3 Unordered Categorical (Nominal) Data",
    "text": "3 Unordered Categorical (Nominal) Data\nFor variables with a discrete number of response options, we can easily measure “how often” values occur in terms of their frequency.\n\n\n\n\n\n\nFrequency distribution\nA frequency distribution is an overview of all distinct values in some variable and the number of times they occur.\n\n\n\nSupposing that we have surveyed the people working in a psychology department and asked them what sub-discipline of psychological research they most strongly identify as working within (If you would like to work along with the reading, the data is available at https://uoepsy.github.io/data/psych_survey.csv).\n\n\nVariable Name\nDescription\n\n\n\nparticipant\nSubject identifier\n\n\narea\nRespondent’s sub-discpline of psychology\n\n\n\n First, we read our data in to R and store it in an object called “psych_disciplines”:\n\npsych_disciplines &lt;- read_csv(\"https://uoepsy.github.io/data/psych_survey.csv\")\npsych_disciplines\n\n# A tibble: 74 × 2\n   participant   area                  \n   &lt;chr&gt;         &lt;chr&gt;                 \n 1 respondent_1  Differential          \n 2 respondent_2  Social                \n 3 respondent_3  Differential          \n 4 respondent_4  Social                \n 5 respondent_5  Differential          \n 6 respondent_6  Differential          \n 7 respondent_7  Language              \n 8 respondent_8  Language              \n 9 respondent_9  Cognitive Neuroscience\n10 respondent_10 Language              \n# ℹ 64 more rows\n\n\nWe can get the frequencies of different response levels of the discipline variable by using the following code:\n\n# start with the psych_disciplines dataframe \n# |&gt;\n# count() the values in the \"area\" variable \npsych_disciplines |&gt;\n    count(area)\n\n# A tibble: 5 × 2\n  area                       n\n  &lt;chr&gt;                  &lt;int&gt;\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\n\n\n\n\n\n\nExtra detail on how this works\n\n\n\n\n\nIn the code above, R knows to look for the area variable inside the psych_disciplines data because we used |&gt; to “pipe” in the psych_disciplines dataframe.\nWe could have also done:\n\n# count(data, variable)\ncount(psych_disciplines, area)\n\n# A tibble: 5 × 2\n  area                       n\n  &lt;chr&gt;                  &lt;int&gt;\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\nBut this would not work:\n\ncount(area)\n\n\nError in group_vars(x) : object ‘area’ not found\n\n\n\n\nFrequency table\nTo describe a distribution like this, we can simply provide the frequency table.\nLet’s store it as an object in R:\n\n# make a new object called \"freq_table\", and assign it:\n# the counts of values of \"area\" variable in \n# the psych_discipline dataframe.\nfreq_table &lt;- \n    psych_disciplines |&gt;\n    count(area)\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 2\n  area                       n\n  &lt;chr&gt;                  &lt;int&gt;\n1 Cognitive Neuroscience    24\n2 Developmental             10\n3 Differential              20\n4 Language                   9\n5 Social                    11\n\n\nFor a report, we might want to make it a little more easily readable:\nCentral tendency\nOften, we might want to summarise data into a single summary value, reflecting the point at (or around) which most of the values tend to cluster. This is known as a measure of central tendency. For numeric data, we can use measures such as the mean, which you will likely have heard of. For nominal data (unordered categorical data), however, our only option is to use the mode.\n\n\n\n\n\n\nMode\nThe most frequent value (the value that occurs the greatest number of times).\n\n\n\nIn our case, the mode is the “Cognitive Neuroscience” category.\nRelative frequencies\nWe might alternatively want to show the percentage of respondents in each category, rather than the raw frequencies.\nThe percentages show the relative frequency distribution\n\n\n\n\n\n\nRelative frequency distribution\nA relative frequency distribution shows the proportion of times each value occurs\n(contrast this with the frequency distribution which shows the number of times).\nRelative frequencies can be written as fractions, percents, or decimals.\n\n\n\nIn the object “freq_table”, we have a variable called n, which is the frequencies (the number in each category).\nThe total of this column is equal to the total number of respondents:\n\n# sum all the values in the \"n\" variable in the \"freq_table\" object\nsum(freq_table$n)\n\n[1] 74\n\n\nAnd therefore, each value in freq_table$n, divided by the total, is equal to the proportion in each category:\n( Tip: Proportions are percentages/100. So 0.4 is another way of expressing 40%)\n\n# take the values in the \"n\" variable from the \"freq_table\" object, \n# and divide by the sum of all the values in the \"n\" variable in \"freq_table\"\nfreq_table$n/sum(freq_table$n)\n\n[1] 0.3243243 0.1351351 0.2702703 0.1216216 0.1486486\n\n\nWe can then simply add the proportions as a new column to our table of frequencies by assigning the values we just calculated to a new variable:\n\n# the variable \"prop\" in the \"freq_table\" object is now assigned \n# the values we calculated above (the proportions)\nfreq_table$prop &lt;- freq_table$n/sum(freq_table$n)\n\n# print the \"freq_table\" object\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt;\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\n However, we can also do this within a sequence of pipes (|&gt;). To do so, we use a new function called mutate().\n\n\n\n\n\n\nmutate\n\n\n\nThe mutate() function is used to add or modify variables to data.\n\n# take the data\n# |&gt;\n# mutate it, such that there is a variable called \"newvariable\", which\n# has the values of a variable called \"oldvariable\" multiplied by two.\ndata |&gt;\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\nNote: Inside mutate(), we don’t have to keep using the dollar sign $, as we have already told it what data to look for variables in.\nTo ensure that our additions/modifications of variables are stored in R’s environment (rather than simply printed out), we need to reassign the name of our dataframe:\n\ndata &lt;- \n  data |&gt;\n  mutate(\n    newvariable = oldvariable * 2\n  )\n\n\n\nWe can actually add this step to our earlier code:\n\n# make a new object called \"freq_table\", and assign it:\n# the counts of values of \"area\" variable in \n# the psych_discipline dataframe.\n# from there, 'mutate' such that there is a variable called \"prop\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable.\nfreq_table &lt;- \n  psych_disciplines |&gt;\n  count(area) |&gt;\n  mutate(\n    prop = n/sum(n)\n  )\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt;\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\nVisualising\n\n“By visualizing information, we turn it into a landscape that you can explore with your eyes. A sort of information map. And when you’re lost in information, an information map is kind of useful.”_ David McCandless\n\nWe’re going to now make our first steps into the world of data visualisation. R is an incredibly capable language for creating visualisations of almost any kind. It is used by many media companies (e.g., the BBC), and has the capability of producing 3d visualisations, animations, interactive graphs, and more.\nWe are going to use the most popular R package for visualisation, ggplot2. This is actually part of the tidyverse, so if we have an Rmarkdown document and have loaded the tidyverse packages at the start (by using library(tidyverse)), then ggplot2 will be loaded too).\nRecall our frequency distribution table:\n\n# show the object called \"freq_table\"\nfreq_table\n\n# A tibble: 5 × 3\n  area                       n  prop\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt;\n1 Cognitive Neuroscience    24 0.324\n2 Developmental             10 0.135\n3 Differential              20 0.270\n4 Language                   9 0.122\n5 Social                    11 0.149\n\n\nWe can plot these values as a bar chart:\n\nggplot(data = freq_table, aes(x = area, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\n\n\n\n\n\n\nggplot components\nNote the key components of the ggplot code.\n\n\ndata = where we provide the name of the dataframe.\n\naes = where we provide the aesthetics. These are things which we map from the data to the graph. For instance, the x-axis, or if we wanted to colour the columns/bars according to some aspect of the data.\n\nThen we add (using +) some geometry. These are the shapes (in our case, the columns/bars), which will be put in the correct place according to what we specified in aes().\n\n\n+ geom_col() Adds columns to the plot.\n\n\n\n\n\n\n\n\n\n\nOptional - Different aes() and geoms, and labels\n\n\n\n\n\nUse these as reference for when you want to make changes to the plots you create.\nAdditionall, remember that google is your friend - there are endless forums with people asking how to do something in ggplot, and you can just copy and paste bits of code to add to your plots!\n\nFill the geoms:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()\n\n\n\n\n\n\n\n\nChange the axis labels:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")\n\n\n\n\n\n\n\n\nChange the geom:\n(Note that using geom_col had the y axis starting at 0, but geom_point starts just below the lowest value.\n\n\n# note that we also need to change \"fill = area\" to \"col = area\". \nggplot(data = freq_table, aes(x = area, y = n, col = area)) +\n    geom_point()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")\n\n\n\n\n\n\n\n\nChange the limits of the axes:\n\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)\n\n\n\n\n\n\n\n\nRemove (or reposition) the legend:\n\n\n# setting legend.position as \"bottom\" would put it at the bottom!\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)+\n    theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\nChanging the theme:\n\n\n# there are many predefine themes, including: \n# theme_bw(), theme_classic(), theme_light()\n\nggplot(data = freq_table, aes(x = area, y = n, fill = area)) +\n    geom_col()+\n    labs(title=\"Counts of respondents by sub-discipline\", y = \"Number of respondents\", x = \"Sub-discipline\")+\n    ylim(0,50)+\n    theme_minimal()"
  },
  {
    "objectID": "rd1_02.html#ordered-categorical-ordinal-data",
    "href": "rd1_02.html#ordered-categorical-ordinal-data",
    "title": "Categorical data",
    "section": "\n4 Ordered Categorical (Ordinal) Data",
    "text": "4 Ordered Categorical (Ordinal) Data\nRecall that ordinal data is categorical data which has a natural ordering of the possible responses. One of the most common examples of ordinal data which you will encounter in psychology is the Likert Scale. You will probably have come across these before, perhaps when completing online surveys or questionnaires.\n\n\n\n\n\n\nLikert Scale\nA five or seven point scale on which an individual express how much they agree or disagree with a particular statement.\n\n\n\nWith Likert data, there is a set of discrete response options (it is categorical data). The response options can be ranked, making it ordered categorical ( strongly disagree &lt; disagree &lt; neither &lt; agree &lt; strongly agree ). Importantly, the distance between responses is not measurable.\nFrequency table\nLet’s suppose that as well as collecting information on the sub-discipline of psychology they identified with, we also asked our respondents to rate their level of happiness from 1 to 5, as well as their job satisfaction from 1 to 5.\n\n\nVariable Name\nDescription\n\n\n\nparticipant\nSubject identifier\n\n\nhappiness\nRespondent’s level of happiness from 1 to 5\n\n\njob_sat\nRespondent’s level of job satisfaction from 1 to 5\n\n\n\n\npsych_survey &lt;- read_csv(\"https://uoepsy.github.io/data/psych_survey2.csv\")\npsych_survey\n\n# A tibble: 74 × 3\n   participant   happiness job_sat\n   &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 respondent_1          3       3\n 2 respondent_2          3       4\n 3 respondent_3          2       5\n 4 respondent_4          4       5\n 5 respondent_5          3       5\n 6 respondent_6          4       4\n 7 respondent_7          4       2\n 8 respondent_8          5       5\n 9 respondent_9          1       5\n10 respondent_10         3       4\n# ℹ 64 more rows\n\n\nFor these questions (variables happiness and job_sat), we could do the same thing as we did above for unordered categorical data, and summarise this into frequencies:\n\n# take the \"psych_survey\" dataframe |&gt;\n# count() the values in the \"happiness\" variable \npsych_survey |&gt;\n    count(happiness)\n\n# A tibble: 5 × 2\n  happiness     n\n      &lt;dbl&gt; &lt;int&gt;\n1         1     6\n2         2    13\n3         3    27\n4         4    21\n5         5     7\n\n# take the \"psych_survey\" dataframe |&gt;\n# count() the values in the \"job_sat\" variable \npsych_survey |&gt;\n    count(job_sat)\n\n# A tibble: 5 × 2\n  job_sat     n\n    &lt;dbl&gt; &lt;int&gt;\n1       1     3\n2       2     6\n3       3    11\n4       4    16\n5       5    38\n\n\nCentral tendency\nWe could again use the Mode - the most common value - to summarise this data. However, because the responses are ordered, it can be more useful to think about the percentages of respondents in and below/above each category. For instance, we might want to talk about asking which category has 50% of the observations in a lower category, and 50% of the observations in a higher category. This is mid-point is known as the Median.\n\n\n\n\n\n\nMedian\nThe value for which 50% of observations a lower and 50% are higher. It is the mid-point of a list of ordered values.\nTo find the median:\n\nrank order the values\nfind the middle value:\n\nIf there are \\(n\\) values, find the value at position \\(\\frac{n+1}{2}\\).\n\nIf \\(n\\) is even, \\(\\frac{n+1}{2}\\) will not be a whole number.\nFor instance, if \\(n = 20\\), you are looking for the \\(\\frac{n+1}{2} = \\frac{20+1}{2} = 10.5^{th}\\) value.\n\nWhen calculating the median for ordinal data, if the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{n+1}{2}^{th}\\) values are different, report both.\nWhen calculating the median for numeric data, report the midpoint of the \\(\\frac{n}{2}^{th}\\) and \\(\\frac{n+1}{2}^{th}\\) values.\n\n\n\n\n\n\n\n\nYou can tell R explicitly that a variable is of a certain type using functions such as as.factor(), as.numeric(), and so on.\nYou may notice that we haven’t done this yet with the data we have been working with in so far today:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and tell me what type/class it is\nclass(psych_survey$happiness)\n\n[1] \"numeric\"\n\n\nThis is because there are some benefits to letting R think your data is numeric, even when it is not. It means we can use functions such as median() to quickly find the median:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and find the median\nmedian(psych_survey$happiness)\n\n[1] 3\n\n\n\n\n\n\n\n\nCaution!\n\n\n\nWhile we can make R treat this data is numeric, it is important to remember that it is actually measured on an ordinal scale.\nFor example, if the median falls between levels, R will tell us that the median is the mid-point:\n\n# for the values 2,1,2,3,4,5, \n# find the median\nmedian(c(2,1,2,3,5,4))\n\n[1] 2.5\n\n\nBut because our data is ordinal, then we know that 2.5 is not a valid response.\n\n\nWe can also use functions such as min() and max() to find the minimum and maximum values:\n\n# inside the \"psych_survey\" dataframe, take ($) the \"happiness\" variable,\n# and find the minimum value\nmin(psych_survey$happiness)\n\n[1] 1\n\n# and find the maximum value\nmax(psych_survey$happiness)\n\n[1] 5\n\n\nCumulative percentages, Quartiles\nIn calculating the median, we are going beyond talking about the relative frequencies (i.e., the percentage in each category), to talking about the cumulative percentage.\n\n\n\n\n\n\nCumulative percentage\nCumulative percentages are another way of expressing a frequency distribution.\nThey are the successive addition of percentages in each category. For example, the cumulative percentage for the 3rd category is the percentage of respondents in the 1st, 2nd and 3rd category:\n\n\n\n\n\n\n\n\n\n\n\nCategory\nFrequency count (n)\nRelative frequency (%)\nCumulative frequency\nCumulative percentage\n\n\n\nResponse 1\n10\n13.33\n10\n13.33\n\n\nResponse 2\n10\n13.33\n20\n26.67\n\n\nResponse 3\n20\n26.67\n40\n53.33\n\n\nResponse 4\n25\n33.33\n65\n86.67\n\n\nResponse 5\n10\n13.33\n75\n100.00\n\n\n\n\n\n\n\n\nWe saw before how we can calculate the proportions/percentages in each category:\n( Note: We multiply by 100 here to turn the proportion into a percentage)\n\n# take the \"psych_survey\" dataframe |&gt;\n# count() the values in the \"happiness\" variable (creates an \"n\" column), and\n# from there, 'mutate' such that there is a variable called \"percent\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable.\npsych_survey |&gt;\n  count(happiness) |&gt;\n  mutate(\n    percent = n/sum(n)*100\n  )\n\n# A tibble: 5 × 3\n  happiness     n percent\n      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1         1     6    8.11\n2         2    13   17.6 \n3         3    27   36.5 \n4         4    21   28.4 \n5         5     7    9.46\n\n\nWe can add another variable, and make it the cumulative percentage, by using the cumsum() function.\n\n# take the \"psych_survey\" dataframe |&gt;\n# count() the values in the \"happiness\" variable (creates an \"n\" column), and\n# from there, 'mutate' such that there is a variable called \"percent\" which\n# has the values of the \"n\" variable divided by the sum of the \"n\" variable,\n# and also make a variable called \"cumulative_percent\" which is the \n# successive addition of the values in the \"percent\" variable\npsych_survey |&gt; \n  count(happiness) |&gt; \n  mutate(\n    percent = n/sum(n)*100,\n    cumulative_percent = cumsum(percent)\n  )\n\n# A tibble: 5 × 4\n  happiness     n percent cumulative_percent\n      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;              &lt;dbl&gt;\n1         1     6    8.11               8.11\n2         2    13   17.6               25.7 \n3         3    27   36.5               62.2 \n4         4    21   28.4               90.5 \n5         5     7    9.46             100   \n\n\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nThink about why this will not work:\n\npsych_survey |&gt; \n  count(happiness) |&gt; \n  mutate(\n    cumulative_percent = cumsum(percent),\n    percent = n/sum(n)*100\n  )\n\n\nError: object ‘percent’ not found\n\nAnswer: Inside the mutate() function, we are trying to assign the cumulative_percent variable based on the values of the percent variable. But in the code above, percent gets defined after cumulative_percent, and so it will not work. Hence the error message (“percent not found”).\n\n\n\nWhile the median splits the data in two (50% either side), you will often see data being split into four equal blocks.\nThe points which divide the four blocks are known as quartiles.\n\n\n\n\n\n\nQuartiles\nQuartiles are the points in rank-ordered data below which falls 25%, 50%, and 75% of the data.\n\nThe first quartile is the first category for which the cumulative percentage is \\(\\geq 25\\%\\).\n\nThe median is the first category for which the cumulative percentage is \\(\\geq 50\\%\\).\n\nThe third quartile is the first category for which the cumulative percentage is \\(\\geq 75\\%\\).\n\n\n\n\nBy looking at the quartiles, it gives us an idea of how spread out the data is.\nAs an example, if we had 10 categories A, B, C, D, E, F, G, H, I, J, and we knew that:\n\n\n\\(Q_1\\) (the \\(1^{st}\\) quartile) = G,\n\n\\(Q_2\\) (the \\(2^{nd}\\) quartile, the median) = H,\n\n\\(Q_3\\) (the \\(3^{rd}\\) quartile) = H,\n\nThis tells us that the first 25% of the data falls in one of the categories from A to G (quite a large range), the second 25% falls in categories G and H (a small range), and the third 25% of the data falls entirely in category H.\nSo a lot of the data is between G and H, with the data being more sparse in the lower and higher categories.\n\n\n\n\n\n\nLooking ahead to numeric data\nWe will talk about quartiles in numeric data too, where we commonly use the difference between the first and third quartile as a measure of how spread out the data are. This gets known as the inter-quartile range (IQR).\n\n\n\nVisualising\nWe can visualise ordered categorical data in the same way we did for unordered.\nFirst we save our frequencies/percentages as a new object:\n\nfreq_table2 &lt;- psych_survey |&gt;\n  count(happiness) |&gt;\n  mutate(\n    percent = n/sum(n)*100\n  )\n\nThen we give that object to our ggplot code, with the appropriate aes() mappings:\n\n# make a ggplot with the object \"freq_table2\". \n# on the x axis put the possible values in the \"happiness\" variable,\n# on the y axis put the possible values in the \"n\" variable.\n# add columns for each entry in the data. \nggplot(data = freq_table2, aes(x = happiness, y = percent)) + \n  geom_col()"
  },
  {
    "objectID": "rd1_02.html#glossary",
    "href": "rd1_02.html#glossary",
    "title": "Categorical data",
    "section": "\n5 Glossary",
    "text": "5 Glossary\n\n\ndistribution: How often different possible values in a variable occur.\n\nfrequency: Number of occurrences (count) in a given response value.\n\nrelative frequency: Percentage/proportion of occurrences in a given response value.\n\ncumulative percentage: Percentage of occurrences in or below a given reponse value (requires ordered data).\n\nmode: Most common value.\n\nmedian: Middle value. \n\n\n|&gt; Takes the output of whatever is on the LHS and gives it as the input of whatever is on the RHS.\n\ncount() Counts the number of occurrences of each unique value in a variable.\n\nmutate() Used to add variables to the dataframe, or modify existing variables.\n\nmin() Returns the minimum value of a variable.\n\nmax() Returns the maximum value of a variable.\n\nmedian() Returns the median value of a variable.\n\nggplot() Creates a plot. Takes data= and a set of mappings aes() from the data to properties of the plot (e.g., x/y axes, colours).\n\ngeom_col() Adds columns to a ggplot."
  },
  {
    "objectID": "rd1_04.html",
    "href": "rd1_04.html",
    "title": "Visualising and describing relationships",
    "section": "",
    "text": "In the previous couple of weeks, we looked at how to handle different types of data, and how to describe and visualise categorical and numeric distributions. More often than not, research involves investigating relationships between variables, rather than studying variables in isolation.\n\n\n\n\n\n\nIf we are using one variable to help us understand or predict values of another variable, we call the former the explanatory variable and the latter the outcome variable.\nOther names\n\noutcome variable = dependent variable = response variable = Y\nexplanatory variable = independent variable = predictor variable = X\n\n(referring to outcome/explanatory variables as Y and X respectively matches up with how we often want to plot them - the outcome variable on the y-axis, and the explanatory variable on the x-axis)\n\n\n\nThe distinction between explanatory and outcome variables is borne out in how we design experimental studies - the researcher manipulates the explanatory variable for each unit before the response variable is measured (for instance, we might randomly allocate participants to one of two conditions). This contrasts with observational studies in which the researcher does not control the value of any variable, but simply observes the values as they naturally exist.\nWe’re going to use data from a Stroop task.\n\nThe data we are going to use for these exercises is from an experiment using one of the best known tasks in psychology, the “Stroop task”. 130 participants completed an online task in which they saw two sets of coloured words. Participants spoke out loud the colour of each word, and timed how long it took to complete each set. In the first set of words, the words matched the colours they were presented in (e.g., word “blue” was coloured blue). In the second set of words, the words mismatched the colours (e.g., the word “blue” was coloured red, see Figure @ref(fig:stroop)). Participants’ recorded their times for each set (matching and mismatching).\nParticipants were randomly assigned to either do the task once only, or to record their times after practicing the task twice.\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html.\nThe data is available at https://uoepsy.github.io/data/strooptask.csv\n\n\n\n\nStroop Task - Color word interference. Images from https://faculty.washington.edu/chudler/java/ready.html\n\n\n\n\nlibrary(tidyverse)\nstroopdata &lt;- read_csv(\"https://uoepsy.github.io/data/strooptask.csv\")\n\n# calculate the \"stroop effect\" - the difference in time taken to complete\n# the matching vs mismatching sets \nstroopdata &lt;- \n    stroopdata |&gt; \n        mutate(\n            stroop_effect = mismatching - matching\n        )\n\nThe data is experimental - researchers controlled the presentation of the stimuli (coloured words) and the assignment of whether or not participants received practice.\nThe researchers are interested in two relationships:\n\nthe relationship between receiving practice (categorical) and the stroop-effect (numeric)\nthe relationship between age (numeric) and the stroop-effect (numeric)"
  },
  {
    "objectID": "rd1_04.html#outcome-vs-explanatory",
    "href": "rd1_04.html#outcome-vs-explanatory",
    "title": "Visualising and describing relationships",
    "section": "",
    "text": "In the previous couple of weeks, we looked at how to handle different types of data, and how to describe and visualise categorical and numeric distributions. More often than not, research involves investigating relationships between variables, rather than studying variables in isolation.\n\n\n\n\n\n\nIf we are using one variable to help us understand or predict values of another variable, we call the former the explanatory variable and the latter the outcome variable.\nOther names\n\noutcome variable = dependent variable = response variable = Y\nexplanatory variable = independent variable = predictor variable = X\n\n(referring to outcome/explanatory variables as Y and X respectively matches up with how we often want to plot them - the outcome variable on the y-axis, and the explanatory variable on the x-axis)\n\n\n\nThe distinction between explanatory and outcome variables is borne out in how we design experimental studies - the researcher manipulates the explanatory variable for each unit before the response variable is measured (for instance, we might randomly allocate participants to one of two conditions). This contrasts with observational studies in which the researcher does not control the value of any variable, but simply observes the values as they naturally exist.\nWe’re going to use data from a Stroop task.\n\nThe data we are going to use for these exercises is from an experiment using one of the best known tasks in psychology, the “Stroop task”. 130 participants completed an online task in which they saw two sets of coloured words. Participants spoke out loud the colour of each word, and timed how long it took to complete each set. In the first set of words, the words matched the colours they were presented in (e.g., word “blue” was coloured blue). In the second set of words, the words mismatched the colours (e.g., the word “blue” was coloured red, see Figure @ref(fig:stroop)). Participants’ recorded their times for each set (matching and mismatching).\nParticipants were randomly assigned to either do the task once only, or to record their times after practicing the task twice.\nYou can try out the experiment at https://faculty.washington.edu/chudler/java/ready.html.\nThe data is available at https://uoepsy.github.io/data/strooptask.csv\n\n\n\n\nStroop Task - Color word interference. Images from https://faculty.washington.edu/chudler/java/ready.html\n\n\n\n\nlibrary(tidyverse)\nstroopdata &lt;- read_csv(\"https://uoepsy.github.io/data/strooptask.csv\")\n\n# calculate the \"stroop effect\" - the difference in time taken to complete\n# the matching vs mismatching sets \nstroopdata &lt;- \n    stroopdata |&gt; \n        mutate(\n            stroop_effect = mismatching - matching\n        )\n\nThe data is experimental - researchers controlled the presentation of the stimuli (coloured words) and the assignment of whether or not participants received practice.\nThe researchers are interested in two relationships:\n\nthe relationship between receiving practice (categorical) and the stroop-effect (numeric)\nthe relationship between age (numeric) and the stroop-effect (numeric)"
  },
  {
    "objectID": "rd1_04.html#numeric-and-categorical",
    "href": "rd1_04.html#numeric-and-categorical",
    "title": "Visualising and describing relationships",
    "section": "\n2 Numeric and Categorical",
    "text": "2 Numeric and Categorical\nRecall that the “stroop-effect” is the difference (in seconds) between participants’ times on the mismatching set of words vs the matching set. We know how to describe a numeric variable such as the stroop-effect, for instance by calculating the mean and standard deviation, or median and IQR. We saw how to produce visualisations of numeric variables in the form of density curves, histogram, and boxplots.\n\n# take the \"stroopdata\" dataframe |&gt;\n# summarise() it, such that there is a value called \"mean_stroop\", which\n# is the mean() of the \"stroop_effect\" variable, and a value called \"sd_stroop\", which\n# is the standard deviation of the \"stroop_effect\" variable.\nstroopdata |&gt; \n  summarise(\n    mean_stroop = mean(stroop_effect),\n    sd_stroop = sd(stroop_effect)\n  )\n\n# A tibble: 1 × 2\n  mean_stroop sd_stroop\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        2.40      5.02\n\n\n\nggplot(data = stroopdata, aes(x = stroop_effect)) + \n  geom_histogram()\n\n\n\n\n\n\n\nTo understand the relationship between categorical (practice) and the numeric (stroop effect), for now we will simply calculate these summary statistics for the numeric variable when it is split by the different levels in the categorical variable.\nIn other words, we want to calculate the mean and standard deviation of the stroop_effect variable separately for those observations where practice is “no”, and for those where practice is “yes”:\n\n\n\n\npractice\nstroop_effect\n\n\n\nno\n9.69\n\n\nno\n10.07\n\n\nyes\n-2.97\n\n\nyes\n-0.23\n\n\nyes\n-5.59\n\n\nno\n3.67\n\n\nyes\n1.41\n\n\nyes\n2.1\n\n\nyes\n-0.33\n\n\n…\n…\n\n\n\n\n\nWe can do this using the group_by() function.\n\n\n\n\n\n\ngroup_by()\nThe group_by() function creates a grouping in the dataframe, so that subsequent functions will be computed on each group.\nIt is most useful in combination with summarise(), to reduce a variable into a summary value for each group in a grouping variable:\n\n# take the data |&gt;\n# make it grouped by each unique value in the \"grouping_variable\" |&gt;\n# summarise() it FOR EACH GROUP, creating a value called \"summary_value\" ()\ndata |&gt; \n  group_by(grouping_variable) |&gt;\n  summarise(\n    summary_value = ...\n  )\n\n\n\n\nLet’s do this for the Stroop Task data - we will summarise() the stroop_effect variable, after grouping the data by the practice variable:\n\n# take the \"stroopdata\" |&gt;\n# and group it by each unique value in the \"practice\" variable (yes/no) |&gt;\n# then summarise() it FOR EACH GROUP, creating summary values called \n# \"mean_stroop\" and \"sd_stroop\" which are the means and standard deviations of \n# the \"stroop_effect\" variable entries for each group of \"practice\".\nstroopdata |&gt;\n  group_by(practice) |&gt;\n  summarise(\n    mean_stroop = mean(stroop_effect),\n    sd_stroop = sd(stroop_effect)\n  )\n\n# A tibble: 2 × 3\n  practice mean_stroop sd_stroop\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 no            4.54        4.25\n2 yes           0.0229      4.75\n\n\nVisualising - Colours\nGiven the output above, which of the following visualisations is most representative of these statistics?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know that the stroop effect for those with practice (blue line) was on average less than those without practice (red line). Both figures A and C don’t fit with this.\nIn both of the figures B and D, the blue (with practice) distribution peaks at about 0, and the red (without practice) distribution peaks at about 5. However, in the figure D, the red distribution is much flatter and wider. It has a larger standard deviation than the blue distribution. In our calculations above, the distributions have very similar standard deviations.\nSo the best visualisation of the two means and standard deviations we calculated is figure B.\n\n\n\nWe can visualise the data using the same code we had before, but with one small addition - we tell ggplot to colour the data according to the different values in the practice variable.\nNote we add this inside the aes() mappings, because we are mapping something on the plot (the colour) to something in the data (the practice variable). If we just wanted to make the line blue, we could put col = \"blue\" outside the aes().\n\nggplot(data = stroopdata, aes(x = stroop_effect, col = practice)) +\n  geom_density()\n\n\n\n\n\n\n\nVisualising - Facets\nInterpreting two density curves on top of one another works well, but overlaying two histograms on top of one another doesn’t. Instead, we might want to create separate histograms for each set of values (the stroop_effect variable values for each of practice/no practice groups).\nfacet_wrap() is a handy part of ggplot which allows us to easily split one plot into many:\n\nggplot(data = stroopdata, aes(x = stroop_effect)) +\n  geom_histogram() +\n  facet_wrap(~practice)"
  },
  {
    "objectID": "rd1_04.html#numeric-and-numeric",
    "href": "rd1_04.html#numeric-and-numeric",
    "title": "Visualising and describing relationships",
    "section": "\n3 Numeric and Numeric",
    "text": "3 Numeric and Numeric\nWhen we are interested in the relationship between two numeric variables, such as the one we have between age and the stroop-effect, the most easily interpreted visualisation of this relationship is in the form of a scatterplot:\n\n# make a ggplot with the stroopdata\n# put the possible values of the \"age\" variable on the x axis,\n# and put the possible values of the \"stroop_effect\" variable on the y axis.\n# for each entry in the data, add a \"tomato1\" coloured geom_point() to the plot, \nggplot(data = stroopdata, aes(x = age, y = stroop_effect)) +\n  geom_point(col=\"tomato1\")\n\n\n\n\n\n\n\nThe visual pattern that these points make on the plot tells us something about the data - it looks like the older participants tended to have a greater stroop-effect.\nBut we can also have relationships between two numeric variables that look the opposite, or have no obvious pattern, or have a more consistent patterning (see Figure @ref(fig:numnumrels))\n\n\n\n\nRelationships between two numeric variables can look very different\n\n\n\nAs a means of summarising these different types of relationships, we can calculate the covariance to describe in what direction, and how strong (i.e., how clear and consistent) the pattern is.\nCovariance\nWe know that variance is the measure of how much a single numeric variable varies around its mean.Covariance is a measure of how two numeric variables vary together, and can express the directional relationship between them.\n\n\n\n\n\n\nCovariance is the measure of how two variables vary together.\nFor samples, covariance is calculated using the following formula:\n\\[\\mathrm{cov}(x,y)=\\frac{1}{n-1}\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})\\]\nwhere:\n\n\n\\(x\\) and \\(y\\) are two variables;\n\n\\(i\\) denotes the observational unit, such that \\(x_i\\) is value that the \\(x\\) variable takes on the \\(i\\)th observational unit, and similarly for \\(y_i\\);\n\n\\(n\\) is the sample size.\n\n\n\n\nIt often helps to understand covariance by working through a visual explanation. Consider the following scatterplot:\n\n\n\n\n\n\n\n\n Now let’s superimpose a vertical dashed line at the mean of \\(x\\) (\\(\\bar{x}\\)) and a horizontal dashed line at the mean of \\(y\\) (\\(\\bar{y}\\)):\n\n\n\n\n\n\n\n\n Now let’s pick one of the points, call it \\(x_i\\), and show \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\).\nNotice that this makes a rectangle.\nAs \\((x_{i}-\\bar{x})\\) and \\((y_{i}-\\bar{y})\\) are both positive values, their product - \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) - is positive.\n\n\n\n\n\n\n\n\n In fact, for all these points in red, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is positive (remember that a negative multiplied by a negative gives a positive):\n\n\n\n\n\n\n\n\n And for these points in blue, the product \\((x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) is negative:\n\n\n\n\n\n\n\n\n Now take another look at the formula for covariance:\n\\[\\mathrm{cov}(x,y)=\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}\\]\nIt is the sum of all these products divided by \\(n-1\\). It is the average of the products! We can easily calculate the covariance between variables in R using the cov() function. cov() takes two variables cov(x = , y = ).\nWe can either use the $ to pull out the variables from the datset:\n\ncov(stroopdata$age, stroopdata$stroop_effect)\n\n[1] 23.9597\n\n\nOr we can specify the dataframe, use the |&gt; symbol, and call cov() inside summarise():\n\nstroopdata |&gt;\n  summarise(\n    mean_age = mean(age),\n    mean_stroop = mean(stroop_effect),\n    cov_agestroop = cov(age, stroop_effect)\n  )\n\n# A tibble: 1 × 3\n  mean_age mean_stroop cov_agestroop\n     &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1     42.8        2.40          24.0"
  },
  {
    "objectID": "rd1_04.html#categorical-and-categorical",
    "href": "rd1_04.html#categorical-and-categorical",
    "title": "Visualising and describing relationships",
    "section": "\n4 Categorical and Categorical",
    "text": "4 Categorical and Categorical\nWhat if we are interested in the relationship between two variables that are both categorical?\nAs a quick example, let’s read in a dataset containing information on passengers from the Titanic. We can see from the first few rows of the dataset that there are quite a few categorical variables here:\n\ntitanic &lt;- read_csv(\"https://uoepsy.github.io/data/titanic.csv\")\nhead(titanic)\n\n# A tibble: 6 × 5\n   ...1 class     age    sex   survived\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   \n1     1 1st class adults man   yes     \n2     2 1st class adults man   yes     \n3     3 1st class adults man   yes     \n4     4 1st class adults man   yes     \n5     5 1st class adults man   yes     \n6     6 1st class adults man   yes     \n\n\nRecall that we summarised one categorical variable using a frequency table:\n\ntitanic |&gt;\n  count(survived)\n\n# A tibble: 2 × 2\n  survived     n\n  &lt;chr&gt;    &lt;int&gt;\n1 no         817\n2 yes        499\n\n\nWe can also achieve this using the table() function:\n\n#thes two lines of code do exactly the same thing!\ntable(titanic$survived)\ntitanic |&gt; select(survived) |&gt; table()\n\n\n\n\n no yes \n817 499 \n\n\nContingency Tables\nLet’s suppose we are interested in how the Class of passengers’ tickets (1st Class, 2nd Class, 3rd Class) can be used to understand their survival.\nWe can create two-way table, where we have each variable on either dimension of the table:\n\n#Either this:\ntable(titanic$class, titanic$survived)\n# or this:\ntitanic |&gt; select(class, survived) |&gt; table()\n\n\n\n           \n             no yes\n  1st class 122 203\n  2nd class 167 118\n  3rd class 528 178\n\n\nAnd we can pass this to prop.table() to turn these into proportions.\nWe can turn them into:\n\nproportions of the total:\n\n\ntitanic |&gt; \n    select(class, survived) |&gt; \n    table() |&gt;\n    prop.table()\n\n           survived\nclass               no        yes\n  1st class 0.09270517 0.15425532\n  2nd class 0.12689970 0.08966565\n  3rd class 0.40121581 0.13525836\n\n\n\nproportions of each row:\n\n\ntitanic |&gt; \n    select(class, survived) |&gt; \n    table() |&gt;\n    prop.table(margin = 1)\n\n           survived\nclass              no       yes\n  1st class 0.3753846 0.6246154\n  2nd class 0.5859649 0.4140351\n  3rd class 0.7478754 0.2521246\n\n\n\nproportions of each column:\n\n\ntitanic |&gt; \n    select(class, survived) |&gt; \n    table() |&gt;\n    prop.table(margin = 2)\n\n           survived\nclass              no       yes\n  1st class 0.1493268 0.4068136\n  2nd class 0.2044064 0.2364729\n  3rd class 0.6462668 0.3567134\n\n\nMosaic Plots\nThe equivalent way to visualise a contingency table is in the form of a mosaic plot.\n\ntitanic |&gt; \n  select(class, survived) |&gt; \n  table() |&gt;\n  plot()\n\n\n\n\n\n\n\nYou can think of the prop.table(margin = ) as scaling the areas of one of the variables to be equal:\n\ntitanic |&gt; \n  select(class, survived) |&gt; \n  table() |&gt;\n  prop.table(margin = 1)\n\n           survived\nclass              no       yes\n  1st class 0.3753846 0.6246154\n  2nd class 0.5859649 0.4140351\n  3rd class 0.7478754 0.2521246\n\n\nIn the table above, each row (representing each level of the “Class” variable) sums to 1. The equivalent plot would make each of level of the “Class” variable as the same area:\n\ntitanic |&gt; \n  select(class, survived) |&gt; \n  table() |&gt;\n  prop.table(margin = 1) |&gt;\n  plot()"
  },
  {
    "objectID": "rd1_04.html#glossary",
    "href": "rd1_04.html#glossary",
    "title": "Visualising and describing relationships",
    "section": "\n5 Glossary",
    "text": "5 Glossary\n\n\nExplanatory variable: A variable used to understand or predict values of an outcome variable.\n\nOutcome variable: A variable which we are aiming to understand or predict via some explanatory variable(s).\n\nScatterplot: A plot in which the values of two variables are plotted along the two axes, the pattern of the resulting points revealing any relationship which is present.\n\nCovariance: A measure of the extent to which two variables vary together. \n\n\ncov() To calculate the covariance between two variables.\n\ngroup_by() To apply a grouping in a dataframe for each level of a given variable. Grouped dataframes will retain their grouping, so that if we use summarise() it will provide a summary calculation for each group.\n\ngeom_point() To add points/dots to a ggplot.\n\nfacet_wrap() To split a ggplot into multiple plots (facets) for each level of a given variable."
  },
  {
    "objectID": "rd1_07.html",
    "href": "rd1_07.html",
    "title": "Probability 1",
    "section": "",
    "text": "Think about flipping a coin once. Can you predict the outcome?\nThink about flipping a coin many times, one million say. Are you able to predict roughly how many heads or tails will show up?\nIt’s hard to guess the outcome of just one coin flip because the outcome could be one of two possible outcomes. Hence, we say that flipping a coin is a random experiment or random process.\nIf you flip it over and over, however, you can predict the proportion of heads you’re likely to see in the long run. In the long run simply means if you were to repeat the same experiment over and over many times under the same conditions.\nThis discussion leads us to define the specific type of randomness that we will be studying in this course.\n\n\n\n\n\n\nWhat is randomness?\nWe will say that a repeatable process is random if its outcome is\n\nunpredictable in the short run, and\npredictable in the long run.\n\n\n\n\nIt is this long-term predictability of randomness that we will use throughout the rest of the course. To do that, we will need to talk about the probabilities of different outcomes and learn some rules for dealing with them."
  },
  {
    "objectID": "rd1_07.html#introduction",
    "href": "rd1_07.html#introduction",
    "title": "Probability 1",
    "section": "",
    "text": "Think about flipping a coin once. Can you predict the outcome?\nThink about flipping a coin many times, one million say. Are you able to predict roughly how many heads or tails will show up?\nIt’s hard to guess the outcome of just one coin flip because the outcome could be one of two possible outcomes. Hence, we say that flipping a coin is a random experiment or random process.\nIf you flip it over and over, however, you can predict the proportion of heads you’re likely to see in the long run. In the long run simply means if you were to repeat the same experiment over and over many times under the same conditions.\nThis discussion leads us to define the specific type of randomness that we will be studying in this course.\n\n\n\n\n\n\nWhat is randomness?\nWe will say that a repeatable process is random if its outcome is\n\nunpredictable in the short run, and\npredictable in the long run.\n\n\n\n\nIt is this long-term predictability of randomness that we will use throughout the rest of the course. To do that, we will need to talk about the probabilities of different outcomes and learn some rules for dealing with them."
  },
  {
    "objectID": "rd1_07.html#video-activity",
    "href": "rd1_07.html#video-activity",
    "title": "Probability 1",
    "section": "\n2 Video activity",
    "text": "2 Video activity\nPlease watch the following video, explaining you to the concept of “randomness”."
  },
  {
    "objectID": "rd1_07.html#random-experiments-and-probability",
    "href": "rd1_07.html#random-experiments-and-probability",
    "title": "Probability 1",
    "section": "\n3 Random experiments and probability",
    "text": "3 Random experiments and probability\n\n3.1 Example 1: Flipping a coin\nThe process of flipping a coin is an example of a random experiment as its outcome is uncertain. We do not know beforehand whether the coin will land heads (H) or tails (T).\nThe collection of all possible outcomes is known as the outcome space or sample space. We typically denote the sample space by \\(S\\). In the coin example, this is: \\[\nS = \\{ H, T \\}\n\\]\nOne particular repetition (i.e. instance) of such experiment is known as a trial.\n\n3.2 Example 2: Throwing a die\nAnother example of a random experiment is throwing a six-faced die as, for each trial, we can not exactly predict which face will appear.\nThe list of possible outcomes for the die experiment is \\(1, 2, ..., 6\\). Hence, the sample space can be written: \\[\nS = \\{1, 2, 3, 4, 5, 6 \\}\n\\]\n\n3.3 Events\nConsider again the die experiment. Often, we are not interested in the probability of observing a particular outcome, such as 3, but rather in a collection of outcomes together. For example, we might be interested in the probability of observing an even number.\nSuch collections of outcomes are called events. More formally, an event is a set of outcomes.\nEach individual outcome is also considered an event. To distinguish, some people call simple events the individual outcomes, and compound events a collection of two or more outcomes.\nThe event “an even number appears” is simply the collection of even outcomes. We could call it “E” for “even” and write it as: \\[E = \\{ 2, 4, 6\\}\\]\nTwo important events are:\n\nthe empty set: the set of no outcomes, denoted \\(\\emptyset\\);\nthe sample space: the set of all outcomes, denoted \\(S\\).\n\nIn general, a finite sample space is written \\[\nS = \\{s_1, s_2, ..., s_n\\}\n\\]\nwhere each \\(s_i\\) represents an outcome or simple event.\n\n3.4 Example 3: Flipping 2 coins\nConsider flipping 2 coins simultaneously. The sample space is: \\[\nS = \\{ (H,H), (H,T), (T,H), (T,T)\\}\n\\]\nTypical events could be:\n\nObserving tails at least once, \\(A = \\{(H,T), (T,H), (T,T)\\}\\)\n\nObserving the same face twice, \\(B = \\{(H,H), (T,T)\\}\\)"
  },
  {
    "objectID": "rd1_07.html#defining-probability",
    "href": "rd1_07.html#defining-probability",
    "title": "Probability 1",
    "section": "\n4 Defining probability",
    "text": "4 Defining probability\nConsider repeating the random process of throwing a die many times, 500 say, and recording whether or not an even number appears. We will now create a table listing the result of each trial.\nFirst, we load the tidyverse package:\n\nlibrary(tidyverse)\n\nNext, we create the sample space:\n\nS &lt;- 1:6\nS\n\n[1] 1 2 3 4 5 6\n\n\nThe following code defines the event “an even number appears”:\n\nE &lt;- c(2, 4, 6)\nE\n\n[1] 2 4 6\n\n\nSay, now, that the outcome of a roll is 3. How can we check whether the outcome belongs to the event of interest \\(E\\)?\nWe can use the %in% function to ask R: “Is 3 in E?” The answer can be TRUE or FALSE.\n\n3 %in% E\n\n[1] FALSE\n\n\nSuppose instead, that the outcome of a roll is 2. Is 2 in E?\n\n2 %in% E\n\n[1] TRUE\n\n\nWe now specify how many trials we will be performing:\n\nnum_trials &lt;- 500\n\nNext, we repeat the experiment num_trials times and compute the accumulated percentage of even outcomes:\n\nexperiment &lt;- tibble(\n    trial = 1:num_trials,\n    outcome = sample(S, num_trials, replace = TRUE),\n    is_even = outcome %in% E,\n    cumul_even = cumsum(is_even),\n    cumul_perc_even = 100 * cumsum(is_even) / trial\n)\n\nThe following code displays the top 10 rows of the experiment:\n\nhead(experiment, n = 10)\n\n# A tibble: 10 × 5\n   trial outcome is_even cumul_even cumul_perc_even\n   &lt;int&gt;   &lt;int&gt; &lt;lgl&gt;        &lt;int&gt;           &lt;dbl&gt;\n 1     1       5 FALSE            0             0  \n 2     2       2 TRUE             1            50  \n 3     3       4 TRUE             2            66.7\n 4     4       4 TRUE             3            75  \n 5     5       2 TRUE             4            80  \n 6     6       3 FALSE            4            66.7\n 7     7       4 TRUE             5            71.4\n 8     8       2 TRUE             6            75  \n 9     9       5 FALSE            6            66.7\n10    10       2 TRUE             7            70  \n\n\n\nUnderstanding the code. Let’s inspect each column in turn:\n\n\ntrial records the number of each trial: 1, 2, …, 500;\n\noutcome lists the result of each trial: 1, or 2, …, or 6;\n\nis_even checks whether the outcome of each trial belongs to the event \\(E\\) (=TRUE) or not (=FALSE);\n\ncumul_even computes the cumulative sum of is_even.\n\nAs we saw, the is_even column contains either TRUE or FALSE. This was created with the function %in%, which is equivalent to asking a question: Is the outcome in \\(E\\)? The result will be either TRUE or FALSE. We note that, when summed, R considers a TRUE as 1, and a FALSE as 0. For example, the cumulative sum of c(TRUE, FALSE, TRUE, TRUE, FALSE) is c(1, 1, 2, 3, 3). Finally, the column cumul_perc_even computes the accumulate percentage of even outcomes.\n\nThe first trial’s outcome was 5, which is not an even number. Hence the cumulative percentage of even outcomes is 0 out of 100, or 0%. The next four trials lead to 2, 4, 4, and 2 respectively, which all are even numbers. The cumulative percentages will be 1 out of 2 (50%), 2 out of 3 (66.67%), 3 out of 4 (75%), and 4 out of 5 (80%). Next, we observe 3, which is odd, hence the cumulative percentage of even outcomes is now 4 out of 66.67%, and so on.\nFinally, we plot the accumulated percentage of even numbers against the trial number:\n\nggplot(experiment, aes(x = trial, \n                       y = cumul_perc_even)) +\n    geom_point(color = 'darkolivegreen4') +\n    geom_line(color = 'darkolivegreen4') +\n    geom_hline(aes(yintercept = 50), color = 'red', linetype = 2) +\n    labs(x = \"Trial number\", y = \"Accumulated percent even\")\n\n\n\n\n\n\n\nAs the number of trials increase, we see that the curve approaches 50%, which is 0.5, and that the cumulative percentage of even outcomes keeps fluctuating around 50%.\n\n\n\n\n\n\nCheckpoint\n\n\n\nWhat’s the probability of obtaining an even number when throwing a fair die?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf you answered 0.5 (or 50%), then you are on the right track!\n\n\n\nBased on the graph, it looks like the relative frequency of an even number settles down to about 50%, so saying that the probability is about 0.5 seems like a reasonable answer.\nBut do random experiments always behave well enough for this definition of probability to always apply? Perhaps the relative frequency of an event can bounce back and forth between two values forever, never settling on just one number?"
  },
  {
    "objectID": "rd1_07.html#the-law-of-large-numbers",
    "href": "rd1_07.html#the-law-of-large-numbers",
    "title": "Probability 1",
    "section": "\n5 The law of large numbers",
    "text": "5 The law of large numbers\nFortunately, Jacob Bernoulli proved the Law of large numbers (LLN) in the 18th century, giving us the peace of mind that we need.\n\n\n\n\n\n\nLaw of large numbers\nThe law of large numbers (LLN) states that as we repeat a random experiment over and over, the proportion of times that an event occurs does settle down to a single number. We call this number the probability of that event.\n\n\n\nHowever, it is not that simple. The LLN requires two key assumptions:\n\n\nIdentical distribution: The outcomes of the random experiment must have the same probabilities of occurring in each trial. That is, we can not have in trial 1 a 50% chance of observing an even number, and then a 80% chance in trial 2. The underlying chance needs to be the same. This is accomplished by not changing the random experiment we are studying, the repeated trials must happen in the same conditions.\n\n\nIndependence: The outcome of one trial must not affect the outcomes of other trials.\n\nFor the die experiment, we can now write that the probability of observing an even number is 0.5 as follows. First, we need to define the event of interest, \\(E = \\{2, 4, 6\\}\\), and then we can write: \\[\nP(E) = 0.5\n\\]\nIf you do not give a name to the event, you must specify it inside of the parentheses. Note the use of round parentheses for probability \\(P()\\) and the curly brackets to list the outcomes of interest. \\[\nP(\\{2, 4, 6\\}) = 0.5\n\\]\nWe reached this definition of the probability of the event \\(E\\). In the long run, \\[\nP(E) = \\frac{\\text{number of times outcome was in the event } E}{\\text{total number of trials}}\n\\]\n\n\n\n\n\n\nNotation\n\n\n\nWe typically use the first few capital letters of the alphabet to name events. The letter \\(P\\) will always be reserved for probability.\nWhen we write \\(P(A) = 0.5\\) we mean “the probability of the event \\(A\\) is 0.5”.\nWe use proportions (or decimal numbers) when reporting probability values in a formal situation like writing a report or a paper. However, when discussing probability informally, we often use percentages."
  },
  {
    "objectID": "rd1_07.html#law-of-averages-unicorn",
    "href": "rd1_07.html#law-of-averages-unicorn",
    "title": "Probability 1",
    "section": "\n6 Law of averages = unicorn",
    "text": "6 Law of averages = unicorn\nYou might have heard from friends or TV shows that sometimes the random experiment “owes” you a particular outcome. Let’s try to entangle this in more detail and understand where the pitfall of this reasoning is.\nThe law of large numbers tells us that the probability of an event is the proportion of times we would observe it in the long run. The long run is really long - infinitely long. We, as humans and finite entities, can not generate an infinitely long sequence of trials and memorise it.\nMany people believe that if you flip a fair coin, where fair means that the chance of getting heads is the same as the chance of getting tails (0.5) we expect the coin to “even out” the results in the coming trials if heads has not appeared in the recent ones.\nSay, for example, that in 10 trials you only observed 1 head. This is quite a low proportion, 0.1 (1 out of 10) compared to the 0.5 (5 out of 10) that the player expected. Does this mean that the coin due to show heads in the near future, as the coin “owes” us some heads to even out the proportions?\nThe answer is no.\nThe long run means that the proportions will eventually even out in the infinite sequence of trials, but you will not know when this happens and there is absolutely no requirement for the coin to show heads again in the upcoming trials in order to keep a probability of 0.5."
  },
  {
    "objectID": "rd1_07.html#modelling-probability",
    "href": "rd1_07.html#modelling-probability",
    "title": "Probability 1",
    "section": "\n7 Modelling probability",
    "text": "7 Modelling probability\nTo assign a probability value to different events, we should make sure that these coherence principles are satisfied:\nRule 1: Probability assignment rule\nThe probability of an impossible event (an event which never occurs) is 0 and the probability of a certain event (an event which always occurs) is 1.\nHence, we have that the probability is a number between 0 and 1: \\[\\text{for any event }A, \\\\ 0 \\leq P(A) \\leq 1\\]\nRule 2: Total probability rule\nIf an experiment has a single possible outcome, it is not random as that outcome will happen with certainty (i.e. probability 1).\nWhen dealing with two or more possible outcomes, we need to be sure to distribute the entire probability among all of the possible outcomes in the sample space \\(S\\).\nThe sample space must have probability 1: \\[P(S) = 1\\]\nIt must be that the will observe one of the outcomes listed within the collection of all possible outcomes of the experiment.\nRule 3: Complement rule\nIf the probability of observing the face “2” in a die is 1/6 = 0.17, what’s the probability of not observing the face “2”? It must be 1 - 1/6 = 5/6 = 0.83.\nIf \\(A = \\{2\\}\\), the event “not A” is the complement of A and is written \\(A^c\\), which is a shortcut for \\(S\\) without \\(A\\), that is \\(\\{1, 3, 4, 5, 6\\}\\).\n\\[P(A^c) = 1 - P(A)\\]\nRule 4: Addition rule for disjoint events\nSuppose the probability that a randomly picked person in a town is \\(A\\) = “a high school student” is \\(P(A) = 0.3\\) and that the probability of being \\(B\\) = “a university student” is \\(P(B) = 0.5\\).\nWhat is the probability that a randomly picked person from the town is either a high school student or a university student? We write the event “either A or B” as \\(A \\cup B\\), pronounced “A union B”.\nIf you said 0.8, because it is 0.3 + 0.5, then you just applied the addition rule: \\[\n\\text{If }A \\text{ and } B \\text{ are mutually exclusive events,}\\\\\nP(A \\cup B) = P(A) + P(B)\n\\]\nRule 5: Multiplication rule for independent events\nWe saw that probability of observing an even number (\\(E\\)) when throwing a die is 0.5.\nYou also know that the probability of observing heads (\\(H\\)) when throwing a fair coin is 0.5.\nWhat’s the probability of observing an even number and heads (that is, \\(E\\) and \\(H\\), written \\(E \\cap H\\)) when throwing both items together?\nThe rule simply says that in this case we multiply the two probabilities together: 0.5 * 0.5 = 0.25.\nThe multiplication rule for independent events says: \\[\n\\text{If }A \\text{ and } B \\text{ are independent events,}\\\\\nP(A \\cap B) = P(A) \\times P(B)\n\\]\n\n7.1 Probability in the case of equally likely outcomes\nConsider a sample space of \\(n\\) outcomes\n\\[\nS = \\{s_1, s_2, \\dots, s_n \\}\n\\]\nand suppose these are all equally likely, with \\(p\\) denoting the probability of each outcome: \\[\nP(\\{s_1\\}) = P(\\{s_2\\}) = \\cdots = P(\\{s_n\\}) = p\n\\]\nAs the outcomes in the sample space are mutually exclusive events,1 we can compute the probability of the sample space as\n\\[\n1 = P(S) = P(\\{s_1\\}) + P(\\{s_2\\}) + \\cdots + P(\\{s_n\\}) = p + p + \\cdots + p = n p\n\\]\nwhich leads to \\[\np = P(\\{s_i\\}) = \\frac{1}{n}\n\\]\nNext, consider an event \\(A\\) comprising a few of the outcomes from \\(S\\)\n\\[\nA = \\{s_2, s_5, s_9\\}\n\\] which can be also written as the union of disjoint events\n\\[\nA = \\{s_2\\} \\cup \\{s_5\\} \\cup \\{s_9\\}\n\\]\nWe can compute the probability of \\(A\\) as follows\n\\[\n\\begin{aligned}\nP(A) &= P(\\{s_2\\}) + P(\\{s_5\\}) + P(\\{s_9\\}) \\\\\n&= p + p + p \\\\\n&= 3 p \\\\\n&= 3 \\left(\\frac{1}{n}\\right) \\\\\n&= \\frac{3}{n} \\\\\n&= \\frac{n_A}{n} \\\\\n&= \\frac{\\text{number of outcomes within }A}{\\text{number of possible outcomes}}\n\\end{aligned}\n\\] where \\(n_A\\) is the number of outcomes within \\(A\\) and \\(n\\) is the total number of possible outcomes in \\(S\\)."
  },
  {
    "objectID": "rd1_07.html#glossary",
    "href": "rd1_07.html#glossary",
    "title": "Probability 1",
    "section": "\n8 Glossary",
    "text": "8 Glossary\n\n\nRandom experiment. A process or phenomenon which can have two ore more possible outcomes.\n\nTrial. A single repetition of the experiment\n\nOutcome. The value observed after running a trial\n\nSample space. The collection of all possible outcomes. The sample space is denoted \\(S\\) and has probability 1.\n\nEvent. Either a single outcome (simple event) or a collection of outcomes (compound event).\n\nProbability. A number reporting how likely is an even to occur when performing a trial (that is, obtaining an outcome within that event or satisfying the proposition of the event)\n\nDisjoint (or mutually exclusive) events. Two events \\(A\\) and \\(B\\) are disjoint if they share no outcomes in common. For disjoint events, knowing that one event occurs tells us that the other cannot occur.\n\nIndependent events. Two events are independent if learning that one event occurs does not change the probability that the other event occurs.\n\nProbability assignment rule. Says that the probability of any event must be between 0 (the probability of an impossible event) and 1 (the probability of a certain event).\n\nTotal probability rule. The probability that the experiment’s outcome is in the sample space must be 1.\n\nComplement rule. If \\(P(A)\\) denotes the probability of the event \\(A\\) occurring, the probability of the complement of A is \\(P(A^c) = 1 - P(A)\\).\n\nAddition rule for disjoint events. If \\(A\\) and \\(B\\) are disjoint events, then \\(P(A \\cup B) = P(A) + P(B)\\).\n\nMultiplication rule for independent events. If A and B are independent events, then \\(P(A \\cap B) = P(A) \\times P(B)\\)."
  },
  {
    "objectID": "rd1_07.html#footnotes",
    "href": "rd1_07.html#footnotes",
    "title": "Probability 1",
    "section": "Footnotes",
    "text": "Footnotes\n\nIn the die example, 1 has nothing in common with 2, 2 has nothing in common with 3, and so on.↩︎"
  },
  {
    "objectID": "rd1_09.html",
    "href": "rd1_09.html",
    "title": "Discrete random variables",
    "section": "",
    "text": "Consider throwing three fair coins. The sample space of this random experiment is\n\\[\nS = \\{\n    TTT, \\\n    TTH, \\\n    THT, \\\n    HTT, \\\n    THH, \\\n    HTH, \\\n    HHT, \\\n    HHH\n\\}\n\\]\nEach outcome has an equal chance of occurring of \\(1 / 8 = 0.125\\), computed as one outcome divided by the total number of possible outcomes.\nOften, we are only interested in a numerical summary of the random experiment. One such summary could be the total number of heads.\n\nRandom variable\nWe call a numerical summary of a random process a random variable.\nRandom variables are typically denoted using the last uppercase letters of the alphabet (\\(X, Y, Z\\)). Sometimes we might also use an uppercase letter with a subscript to distinguish them, e.g. \\(X_1, X_2, X_3\\).\n\nA random variable, like a random experiment, also has a sample space and this is called the support or range of \\(X\\), written \\(R_X\\). This represents the set of possible values that the random variable can take."
  },
  {
    "objectID": "rd1_09.html#random-variables",
    "href": "rd1_09.html#random-variables",
    "title": "Discrete random variables",
    "section": "",
    "text": "Consider throwing three fair coins. The sample space of this random experiment is\n\\[\nS = \\{\n    TTT, \\\n    TTH, \\\n    THT, \\\n    HTT, \\\n    THH, \\\n    HTH, \\\n    HHT, \\\n    HHH\n\\}\n\\]\nEach outcome has an equal chance of occurring of \\(1 / 8 = 0.125\\), computed as one outcome divided by the total number of possible outcomes.\nOften, we are only interested in a numerical summary of the random experiment. One such summary could be the total number of heads.\n\nRandom variable\nWe call a numerical summary of a random process a random variable.\nRandom variables are typically denoted using the last uppercase letters of the alphabet (\\(X, Y, Z\\)). Sometimes we might also use an uppercase letter with a subscript to distinguish them, e.g. \\(X_1, X_2, X_3\\).\n\nA random variable, like a random experiment, also has a sample space and this is called the support or range of \\(X\\), written \\(R_X\\). This represents the set of possible values that the random variable can take."
  },
  {
    "objectID": "rd1_09.html#discrete-vs-continuous-random-variables",
    "href": "rd1_09.html#discrete-vs-continuous-random-variables",
    "title": "Discrete random variables",
    "section": "\n2 Discrete vs continuous random variables",
    "text": "2 Discrete vs continuous random variables\nThere are two different types of random variables, and the type is defined by their range.\nWe call a variable discrete or continuous depending on the “gappiness” of its range, i.e. depending on whether or not there are gaps between successive possible values of a random variable.\n\n\n\n\n\n\n\n\n\nA discrete random variable has gaps in between its possible values. An example is the number of children in a randomly chosen family (0, 1, 2, 3, …). Clearly, you can’t have 2.3 children…\nA continuous random variable has no gaps in between the its possible values. An example is the height in cm of a randomly chosen individual.\n\nIn this week’s exercises we will study discrete random variables."
  },
  {
    "objectID": "rd1_09.html#three-coins-example-continued",
    "href": "rd1_09.html#three-coins-example-continued",
    "title": "Discrete random variables",
    "section": "\n3 Three coins example (continued)",
    "text": "3 Three coins example (continued)\nIn the 3 coins example, the possible values of the random variable \\(X\\) = “number of heads in 3 tosses” are \\[\nR_X = \\{0, 1, 2, 3\\}\n\\]\nmeaning that \\(X\\) is a discrete random variable.\nWe denote a potential value of the random variable using a lowercase \\(x\\) and a subscript to number the possible values.\nThis is obtained as follows:\n\n\n\n\n\n\n\n\nAs we can see, each value of the random variable is computed from the underlying random experiment. There is one outcome only (TTT) leading to zero heads, i.e. \\(X = 0\\). There are three outcomes (TTH, THT, HTT) leading to one head, i.e. \\(X = 1\\). And so on…\nThe experiment’s outcomes are all equally likely, each having a \\(1/8\\) chance of occurring. However, since the random variable aggregates the experiment’s outcomes, the probability of the random variable taking a particular value is computed by summing the probabilities of the outcomes leading to that value.\nLet’s try and obtain the same diagram as that shown above using R. We will be using the function expand_grid, which creates the sample space by listing all possible combinations.\n\nlibrary(tidyverse)\n\nexperiment &lt;- expand_grid(coin1 = c('T', 'H'),\n                          coin2 = c('T', 'H'),\n                          coin3 = c('T', 'H'))\nexperiment\n\n# A tibble: 8 × 3\n  coin1 coin2 coin3\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 T     T     T    \n2 T     T     H    \n3 T     H     T    \n4 T     H     H    \n5 H     T     T    \n6 H     T     H    \n7 H     H     T    \n8 H     H     H    \n\nexperiment &lt;- experiment |&gt; \n    mutate(\n        prob = rep( 1/n(), n() )\n    )\nexperiment\n\n# A tibble: 8 × 4\n  coin1 coin2 coin3  prob\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 T     T     T     0.125\n2 T     T     H     0.125\n3 T     H     T     0.125\n4 T     H     H     0.125\n5 H     T     T     0.125\n6 H     T     H     0.125\n7 H     H     T     0.125\n8 H     H     H     0.125\n\nrv &lt;- experiment |&gt;\n    mutate(\n        value = (coin1 == 'H') + (coin2 == 'H') + (coin3 == 'H')\n    ) |&gt;\n    group_by(value) |&gt;\n    summarise(prob = sum(prob))\nrv\n\n# A tibble: 4 × 2\n  value  prob\n  &lt;int&gt; &lt;dbl&gt;\n1     0 0.125\n2     1 0.375\n3     2 0.375\n4     3 0.125\n\n\nwhere \\(1/8 = 0.125\\) and \\(3/8 = 0.375\\).\n\nWe can provide a concise representation of a random variable \\(X\\), the set of all its possible values, and the probabilities of those values by providing the probability distribution of \\(X\\). You can think of the probability distribution of a random variable as a succinct way to provide a global picture of the random variable.\n\n\n\n\n\n\nProbability distribution\n\n\n\nThe probability distribution of a discrete random variable \\(X\\) provides the possible values of the random variable and their corresponding probabilities.\nA probability distribution can be in the form of a table, graph, or mathematical formula.\n\n\nWe visualise the distribution of a discrete random variable via a line graph. This graph gives us, with just a glance, an immediate representation of the distribution of that random variable.\n\nggplot(data = rv) +\n    geom_segment(aes(x = value, xend = value, y = 0, yend = prob)) +\n    geom_point(aes(x = value, y = prob)) +\n    labs(x = \"Possible values, x\", y = \"Probabilities, P(X = x)\")\n\n\n\n\n\n\n\nAs you can see, a line graph has gaps in between the possible values the random variable can take, exactly to remind us that the random variable can’t take values that are different from 0, 1, 2, and 3.\nAlternatively, you could provide the probability distribution of the random variable in tabular form:\n\n\n\n\nx\nP(X = x)\n\n\n\n0\n1/8\n\n\n1\n3/8\n\n\n2\n3/8\n\n\n3\n1/8\n\n\n\n\n\nStatisticians have also spent lots of time trying to find a mathematical formula for that probability distribution. The formula is the most concise way to obtain the probabilities as it gives you a generic rule which you can use to compute the probability of any possible value of that random variable. All you have to do is substitute to \\(x\\) the value you are interested in, e.g. 0, 1, 2, or 3.\n\n\n\n\n\n\nProbability mass function\n\n\n\nThe probability mass function (pmf) of \\(X\\) assigns a probability, between 0 and 1, to every value of the discrete random variable \\(X\\).\nEither of the following symbols are often used: \\[\nf(x) = P(x) = P(X = x) \\qquad \\text{for all possible }x\n\\] where \\(P(X = x)\\) reads as “the probability that the random variable \\(X\\) equals \\(x\\)”.\nThe sum of all of these probabilities must be one, i.e. \\[\n\\sum_{i} f(x_i) = \\sum_{i} P(X = x_i) = 1\n\\]\n\n\nBefore we define the mathematical function, I need to tell you what a number followed by an exclamation mark means.\nIn mathematics \\(n!\\), pronounced “\\(n\\) factorial”, is the product of all the integers from 1 to \\(n\\). For example, \\(4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\\), and \\(3! = 3 \\cdot 2 \\cdot 1 = 6\\). By convention, mathematician have decided that \\(0! = 1\\).\nThe probability function of \\(X\\) = “number of heads in 3 tosses” makes use of the following numbers:\n\n\n\\(3\\), representing the number of coin flips\n\n\\(\\frac{1}{2}\\), the probability of observing heads in a single flip of a fair coin\n\nFor the three coins example, the probability function of \\(X\\) is \\[\nP(X = x) = \\frac{3!}{x!\\ (3-x)!} \\cdot (1/2)^x \\cdot (1/2)^{3-x}\n\\]\nLet’s see if the formula gives back the table we created above.\n\n\nFor \\(x=0\\) we have:\n\\[\nP(X = 0) = \\frac{3!}{0!\\ 3!} \\cdot (1/2)^0 \\cdot (1/2)^3 = \\frac{6}{6} \\cdot 1 \\cdot (1/8) = 1/8\n\\]\n\n\nAnd so on… If you want to see the rest, check the optional box below.\n\n\n\n\n\n\nOptional: I want to see the other probabilities\n\n\n\n\n\n\n\nFor \\(x = 1\\) we have\n\\[\nP(X = 1) = \\frac{3!}{1!\\ 2!} \\cdot (1/2)^1 \\cdot (1/2)^2 = \\frac{6}{2} \\cdot (1/2) \\cdot (1/4) = 3 \\cdot (1/8) = 3/8\n\\]\n\n\nFor \\(x = 2\\) we have\n\\[\nP(X = 2) = \\frac{3!}{2!\\ 1!} \\cdot (1/2)^2 \\cdot (1/2)^1= \\frac{6}{2} \\cdot (1/4) \\cdot (1/2) = 3 \\cdot (1/8) = 3/8\n\\]\n\n\nFor \\(x=3\\) we have\n\\[\nP(X = 3) = \\frac{3!}{3!\\ 0!} \\cdot (1/2)^3 \\cdot (1/2)^0 = \\frac{6}{6} \\cdot (1/8) \\cdot 1 = 1/8\n\\]\n\n\n\n\n\nAs you can see, this formula will provide you the same values that are listed in the tabular representation of the probability distribution."
  },
  {
    "objectID": "rd1_09.html#centre-the-expected-value",
    "href": "rd1_09.html#centre-the-expected-value",
    "title": "Discrete random variables",
    "section": "\n4 Centre: the expected value",
    "text": "4 Centre: the expected value\nConsider a discrete random variable with range \\(R_X = \\{x_1, x_2, \\dots, x_n\\}\\)\nThe expected value (or mean) of a random variable \\(X\\), denoted by \\(E(X)\\), \\(\\mu\\), or \\(\\mu_X\\), describes where the probability distribution of \\(X\\) is centred.\nWe tend to prefer the name “expected value” to “mean” as the random variable is not something the has happened yet, it’s a potentially observable value. So, the expected value is the typical value we expect to observe.\nThe expected value of \\(X\\) is computed by multiplying each value by its probability and then summing everything:\n\\[\n\\begin{aligned}\n\\mu = E(X) &= x_1 \\cdot P(x_1) + x_2 \\cdot P(x_2) + \\cdots + x_n \\cdot P(x_n) \\\\\n&= \\sum_{i} x_i \\cdot P(x_i)\n\\end{aligned}\n\\]\nFor the three coins, the expected value is: \\[\n\\mu = 0 \\cdot \\frac{1}{8} + 1 \\cdot \\frac{3}{8} + 2 \\cdot \\frac{3}{8} + 3 \\cdot \\frac{1}{8} = \\frac{3}{2} = 1.5\n\\]\nAs you can see, 1.5 is not one of the possible values that \\(X\\) can take in that case, as it lies in the gap between the values 1 and 2. However, it is a fictitious number which seems to well represent the centre of that distribution and hence a typical value from that distribution."
  },
  {
    "objectID": "rd1_09.html#spread-the-standard-deviation",
    "href": "rd1_09.html#spread-the-standard-deviation",
    "title": "Discrete random variables",
    "section": "\n5 Spread: the standard deviation",
    "text": "5 Spread: the standard deviation\nThe variability of a random variable \\(X\\) is measured by its standard deviation.\n\n\n\n\n\n\nVariance and standard deviation\n\n\n\nIf \\(X\\) has expected value \\(\\mu\\), the variance of \\(X\\) is \\[\n\\sigma^2 = \\sum_i (x_i - \\mu)^2 \\cdot P(x_i)\n\\] and the standard deviation is defined as \\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]"
  },
  {
    "objectID": "rd1_09.html#underlying-random-experiments",
    "href": "rd1_09.html#underlying-random-experiments",
    "title": "Discrete random variables",
    "section": "\n6 Underlying random experiments",
    "text": "6 Underlying random experiments\nAs we saw, each random variable is a numerical summary of a random experiment and, as such, it arises from an underlying random experiment.\nIn this section we will analyse different random experiments, also called models, commonly arising in every day situations.\n\n6.1 Binomial model\n\n\n\n\n\n\nNotation\n\n\n\n\\(p\\) is the probability of a success on any one trial, and \\(n\\) is the number of trials.\n\n\nSuppose you have a series of trials that satisfy these conditions:\n\nB: They are Bernoulli — that is, each trial must have one of two different outcomes, one called a “success” and the other a “failure.”\nI: Each trial is independent of the others — that is, the probability of a success doesn’t change depending on what has happened before.\nN: There is a fixed number, \\(n\\), of trials.\nS: The probability, \\(p\\), of a success is the same on each trial, with \\(0 \\leq p \\leq 1\\).\n\nThen the distribution of the random variable \\(X\\) that counts the number of successes in \\(n\\) trials (each with a probability of success = \\(p\\)) is called a binomial distribution.\nThe numbers \\(n\\) and \\(p\\) are called the parameters of the binomial distribution. We write that \\(X\\) follows a binomial distribution with parameters \\(n\\) and \\(p\\) as follows: \\[\nX \\sim \\text{Binomial}(n,p)\n\\]\nFurther, the probability that you get exactly \\(X = x\\) successes is \\[\nP(X = x) = \\frac{n!}{x!\\ (n-x)!} \\cdot p^x \\cdot (1-p)^{n-x}, \\qquad R_X = \\{0, 1, 2, ..., n\\}\n\\] where \\(n! = n (n-1) (n-2) \\cdots 3 \\cdot 2 \\cdot 1\\).\nDo you recognise it from the coins example?\nVisual exploration\nThe figure below displays different binomial distributions as \\(n\\) and \\(p\\) vary:\n\n\n\n\nThe binomial probability distribution as n and p vary.\n\n\n\nCentre and spread\nFor a random variable \\(X\\) having a binomial distribution with \\(n\\) trials and probability of success \\(p\\), the mean (expected value) and standard deviation for the distribution are given by \\[\n\\mu_X = n p \\qquad \\text{and} \\qquad \\sigma_X = \\sqrt{n p (1 - p)}\n\\]\nBinomial distribution in R\nThe function to compute the binomial probability distribution is\ndbinom(x, size, prob)\nwhere:\n\n\nx is the values for which we want to compute the probabilities\n\nsize is \\(n\\) in our notation, the number of trials\n\nprob is \\(p\\) in our notation, the probability of success in each trial.\nExample\nA student is attempting a 10-questions multiple choice test. Each question has four different options. If the student answers at random, what is the chance that they correctly answers 2 out of the 10 questions?\nAs we know that the student is randomly guessing the answers, the probability of a correct answer is \\(p = 1/4\\). The probability of answering 2 questions correctly out of the 10 in the test is \\(P(X = 2)\\):\n\ndbinom(x = 2, size = 10, prob = 1/4)\n\n[1] 0.2815676\n\n\nIn a multiple choice test comprising 10 questions having each 4 possible answers, there is a 28% chance of answering exactly 2 questions out of 10 correctly just by random guessing.\n\nNote that you can also compute the probabilities for all possible values of \\(X\\) at once:\n\ntibble(\n    values = 0:10,\n    prob = dbinom(x = 0:10, size = 10, prob = 1/4)\n)\n\n# A tibble: 11 × 2\n   values        prob\n    &lt;int&gt;       &lt;dbl&gt;\n 1      0 0.0563     \n 2      1 0.188      \n 3      2 0.282      \n 4      3 0.250      \n 5      4 0.146      \n 6      5 0.0584     \n 7      6 0.0162     \n 8      7 0.00309    \n 9      8 0.000386   \n10      9 0.0000286  \n11     10 0.000000954\n\n\n\n6.2 Geometric model\nSuppose you have a series of trials that satisfy these conditions:\n\nThey are Bernoulli — that is, each trial must have one of two different outcomes, one called a “success” and the other a “failure”.\nEach trial is independent of the others; that is, the probability of a success doesn’t change depending on what has happened before.\nThe trials continue until the first success.\nThe probability, \\(p\\), of a success is the same on each trial, \\(0 \\leq p \\leq 1\\).\n\nThen the distribution of the random variable \\(X\\) that counts the number of failures before the first “success” is called a geometric distribution.\nThe probability that the first success occurs after \\(X = x\\) failures is \\[\nP(X = x) = (1 - p)^{x} p, \\qquad R_X = \\{0, 1, 2, ...\\}\n\\]\nWe write that \\(X\\) follow a geometric distribution with parameter \\(p\\) as follows: \\[\nX \\sim \\text{Geometric}(p)\n\\]\nVisual exploration\nThe figure below displays different geometric distributions as \\(p\\) varies:\n\n\n\n\n\n\n\n\nCentre and spread\nA random variable \\(X\\) that has a geometric distribution with probability of success \\(p\\) has an expected value (mean) and standard deviation of \\[\n\\mu_X = \\frac{1 - p}{p} \\qquad \\text{and} \\qquad \\sigma_X = \\sqrt{\\frac{1-p}{p^2}}\n\\]\nGeometric distribution in R\nThe function to compute the geometric probability distribution is\ndgeom(x, prob)\nwhere:\n\n\nx is the number of failures before the first success\n\nprob is \\(p\\) in our notation, the probability of success in each trial.\nExample\nConsider rolling a fair six-sided die until a five appears. What is the probability of rolling the first five on the third roll?\nFirst, note that the probability of “success” (observing a five) is \\(p = 1/6\\). We are asked to compute the probability of having the first “success” on the 3rd trial. We want to compute \\(P(X = 2)\\) because we need to have 2 failures followed by a success:\n\ndgeom(x = 2, prob = 1/6)\n\n[1] 0.1157407\n\n\nThus, there is a 12% chance of obtaining the first five on the 3rd roll of a die."
  },
  {
    "objectID": "rd1_09.html#glossary",
    "href": "rd1_09.html#glossary",
    "title": "Discrete random variables",
    "section": "\n7 Glossary",
    "text": "7 Glossary\n\n\nRandom variable. A numerical summary of a random experiment.\n\nRange of a random variable. The set of possible values the random variable can take.\n\nProbability distribution. A table, graph, or formula showing how likely each possible value of a random variable is to occur.\n\nProbability (mass) function. A function providing the probabilities, between 0 and 1, for each value that the random variable can take. These probabilities must sum to 1.\n\nBinomial random variable. \\(X\\) represents the number of successes in \\(n\\) trials where the probability of success, \\(p\\), is constant from trial to trial. It has range \\(R_X = \\{0, 1, 2, ..., n\\}\\).\n\nGeometric random variable. \\(X\\) represents the number of failures until the first success, where the probability of success, \\(p\\), is constant from trial to trial. It has range \\(R_X = \\{0, 1, 2, ...\\}\\)."
  },
  {
    "objectID": "rd1_11.html",
    "href": "rd1_11.html",
    "title": "Sampling distributions",
    "section": "",
    "text": "This section contains essential terminology and functions that are needed to complete the exercises provided.\n\n\n\n\n\n\nPopulation vs sample\n\n\n\n\n\nTypically:\n\nWe do not have data for the entire population. There are different possible reasons:\n\nIt’s too expensive to collect them\nBecause of deadlines, there is not sufficient time\nIt’s not possible to reach the entire population\n\n\nIt’s much easier to obtain data by taking a sample from that population of interest and measuring only the units chosen in the sample.\n\nNote that units do not necessarily have to be individuals, but they could be schools, companies, etc.\n\n\nWe wish to use the sample data to:\n\nInvestigate a claim about the whole population.\nTest an hypothesis about the entire population.\nAnswer a question about the whole population.\n\n\n\nThe process of using information from a sample (the part) in order to draw conclusions about the entire population (the whole) is known as statistical inference.\n\n\n\n\n\n\n\n\n\nParameters vs statistics\n\n\n\n\n\nAs we do not typically have the data for the entire population, the population is considered as “unknown” with respect the data that we wish to investigate. We could shortly say that the population data are unknown.\nFor example, if we are interested in the average IQ in the population, we don’t have the resources to go to every single individual and test their IQ score. So, in this respect, the population IQ scores are unknown.\nAs a consequence of this, any numerical summary of the population data is also unknown. In the above example, the population mean IQ score is unknown and needs to be estimated.\nSample data are more readily available or feasible to collect. Imagine collecting a sample of 50 individuals, chosen at random from the population, and testing each to obtain their IQ score. If you performed the random experiment, you would then obtain a sequence of 50 IQ measurements.\nAt the same time, it is also feasible to compute any numerical summary of the sample data. For example, you can compute the mean IQ score for those 50 individuals in the sample.\n\n\n\n\n\n\nWe typically use this terminology to distinguish a numerical summary when computed in the population (unknown) or in the collected sample (known).\n\nA parameter is a numerical summary of a population.\nA statistic is a numerical summary of the sample.\n\nA statistic is often used as a “best guess” or “estimate” for the unknown parameter. That is, we use the (sample) statistic to estimate a (population) parameter.\n\n\n\nIn the above example, the population mean IQ score is the parameter of interest, while the sample mean IQ score is the statistic.\nIt is typical to use special notation to distinguish between parameters and statistics in order to convey with a single letter: (1) which numerical summary is being computed, and (2) if it is computed on the population or on the sample data.\nThe following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.\n\nNotation for common parameters and statistics.\n\n\n\n\n\n\nNumerical summary\nPopulation parameter\nSample statistic\n\n\n\nMean\n\\(\\mu\\)\n\n\\(\\bar{x}\\) or \\(\\hat{\\mu}\\)\n\n\n\nStandard deviation\n\\(\\sigma\\)\n\n\\(s\\) or \\(\\hat{\\sigma}\\)\n\n\n\nProportion\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\nThe Greek letter \\(\\mu\\) (mu) represents the population mean (parameter), while \\(\\bar{x}\\) (x-bar) or \\(\\hat{\\mu}\\) (mu-hat) is the mean computed from the sample data (sample statistic).\nThe Greek letter \\(\\sigma\\) (sigma) represents the population standard deviation (parameter), while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) is the standard deviation computed from the sample data (sample statistic).\nThe Greek letter \\(p\\) represents the population proportion (parameter), while \\(\\hat{p}\\) (p-hat) is the proportion computed from the sample data (sample statistic).\n\nThe process of sampling \\(n\\) people at random from the population is a random experiment, as it leads to an uncertain outcome. Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable (random number).\nBefore actually performing the random experiment and picking individuals for the sample, the sample mean is a random number, as its value is uncertain. Once we actually perform the random experiment and measure the individuals in the sample, we have an observed value for the sample mean, i.e. a known number.\nWe will then distinguish between:\n\na statistic - a random variable (random number) that is uncertain because it involves a random experiment.\nan observed statistic - the actual number that is computed once the sample data have been collected by performing the chance experiment.\n\nNotation-wise we distinguish between a statistic (random variable) and an observed statistic (observed number) by using, respectively, an uppercase letter or a lowercase letter.\nUppercase letters refer to random variables. Recall that a random variable represents a well defined number, but whose value is uncertain as it involves an experiment of chance in order to reach to an actual value.\nFor example, \\(\\bar X\\) = “mean IQ score in a random sample of 50 individuals” is a random variable. It is clearly defined in operational terms by: (1) take a sample of 50 individuals at random, (2) measure their IQ score, and (3) compute the mean of their 50 IQ scores. However, the actual value that we can obtain is uncertain as it is the result of an experiment of chance involving random sampling from a population. There are many possible values we could obtain, so we are not sure which one we will see.\nAnother example: \\(X\\) = “number of heads in 10 flips of a coin”. This is also a clearly defined experiment: (1) flip a coin 10 times and (2) count the number of heads. However, the result is a random number, which is uncertain and will be unknown until we actually perform the experiment and flip a coin 10 times.\nLowercase letters refer to observed (i.e., realised) values of the random variable. An observed value of a random variable is just a number.\nFor example, once we actually collect 50 individuals and measure their IQ scores, we can sum those 50 numbers and divide the sum by 50 to obtain the observed sample mean. Say the sample mean IQ score is 102.3, we would write \\(\\bar x = 102.3\\).\nIn short, we would use for a sample mean:\n\nStatistic (sample mean): an uppercase letter before the value is actually known: \\(\\bar X\\)\n\nObserved statistic (observed sample mean): a lowercase letter once the value is actually known: \\(\\bar x\\)\n\n\n\n\n\n\n\n\n\n\n\nAvoiding bias due to sampling\n\n\n\n\n\nSampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.\nIf sampling bias exists, we cannot generalise our sample conclusions to the population.\n\n\n\n\n\n\n\n\nTo be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is random sampling. Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn. Random sampling is a strategy to avoid sampling bias.\n\n\n\n\n\n\nSimple random sampling\nWhen we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.\nWhen instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.\n\n\n\nIn general, we have bias when the method of collecting data causes the data to inaccurately reflect the population.\n\n\n\n\n\n\n\n\n\nSampling distribution\n\n\n\n\n\nThe natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. This is a special case which rarely happens in practice: we actually know what the distribution looks like in the population.\nWe will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. Remember, however, that in practice the population parameter would not be known.\nWe will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv\nFirst, we read the population data:\n\nlibrary(tidyverse)\ngest &lt;- read_csv('https://uoepsy.github.io/data/pregnancies.csv')\ndim(gest)\n\n[1] 49863     2\n\n\nThe data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy Look at the top six rows of the data set (the “head”):\n\nhead(gest)\n\n# A tibble: 6 × 2\n     id gest_period\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1        256.\n2     2        269.\n3     3        253.\n4     4        292.\n5     5        271.\n6     6        253.\n\n\nWe now want to investigate how much the sample means will vary from sample to sample. To do so, we will take many samples from the population of all gestation periods, and compute for each sample the mean.\nTo do so, we will load a function which we prepared for you called rep_sample_n(). This function is used to take a sample of \\(n\\) units from the population, and it lets you repeat this process many times.\nTo get the function in your computer, run this code:\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\n\n\n\n\n\n\nNOTE\nYou need to copy and paste the line\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\nat the top of each file in which you want to use the rep_sample_n() function.\n\n\n\nThe function takes the following arguments:\nrep_sample_n(data, n = &lt;sample size&gt;, samples = &lt;how many samples&gt;)\n\ndata is the population\nn is the sample size\nsamples is how many samples of size \\(n\\) you want to take\n\nBefore doing anything involving random sampling, it is good practice to set the random seed. This is to ensure reproducibility of the results. Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.\nSet the random seed to any number you wish. Depending on the number, you will get the same results as me or not:\n\nset.seed(1234)\n\nObtain 10 samples of \\(n = 5\\) individuals each:\n\nsamples &lt;- rep_sample_n(gest, n = 5, samples = 10)\nsamples\n\n# A tibble: 50 × 3\n   sample    id gest_period\n    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 40784        255.\n 2      1 40854        275.\n 3      1 41964        281.\n 4      1 15241        246.\n 5      1 33702        247.\n 6      2 35716        267.\n 7      2 17487        289.\n 8      2 15220        266.\n 9      2 19838        238.\n10      2  2622        281.\n# ℹ 40 more rows\n\n\nThe samples data frame contains 3 columns:\n\nsample, telling us which sample each row refers to\nid, telling us the units chosen to enter each sample\ngest_period, telling us the gestation period (in days) of each individual\n\nNote that the tibble samples has 50 rows, which is given by 5 individuals in each sample * 10 samples.\nYou can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, …, and 10th sample:\n\n\n\n\n\n\nNow, imagine computing the mean of the five observation in each sample. This will lead to 10 means, one for each of the 10 samples (of 5 individuals each).\n\nsample_means &lt;- samples |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      261.\n 2      2      268.\n 3      3      273.\n 4      4      269.\n 5      5      261.\n 6      6      271.\n 7      7      252.\n 8      8      262.\n 9      9      258.\n10     10      267.\n\n\nAs you can see this leads to a tibble having 10 rows (one for each sample), where each row is a mean computed from the 5 individuals which were chosen to enter the sample.\nThe gestation period (in days) for the first five women sampled were\n\n255.15, 275.34, 281.04, 245.62, 247.5\n\nThis sample has a mean of \\(\\bar x\\) = 260.93 days.\nThe second sample of 5 women had gestation periods\n\n267.03, 288.74, 265.56, 238, 280.56\n\nThe second sample has a mean gestation period of \\(\\bar x\\) = 267.98 days.\nIn Figure 1 we display the individual gestation periods in each sample as dots, along with the means gestation period \\(\\bar x\\) of each sample. The position of the sample mean is given by a red vertical bar.\nWe then increased the sample size to 50 women and took 10 samples each of 50 individuals. This set of samples together with their means is also plotted in Figure 1.\n\n\n\n\n\n\n\nFigure 1: Gestation period (in days) of samples of individuals.\n\n\n\n\nTwo important points need to be made from Figure Figure 1. First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample. The sample means vary in an unpredictable way, illustrating the fact that \\(\\bar X\\) is a summary of a random experiment (randomly choosing a sample) and hence is a random variable. Secondly, as we increase the sample size from 5 to 50, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel (b)). That is, with a larger sample size, the sample means fluctuate less and are more “consistent”.\nTo further investigate the variability of sample means, we will now generate many more sample means computed on:\n\n1,000 samples of \\(n = 5\\) women\n1,000 samples of \\(n = 50\\) women\n1,000 samples of \\(n = 500\\) women\n\nWe will also add at the end of each tibble a column specifying the sample size. In the first tibble, mutate(n = 5) creates a column called n where all values will be 5, to remind ourselves that those means were computed with samples of size \\(n = 5\\). Remember that mutate() takes a tibble and creates a new column or changes an existing one.\n\n# (a) 1,000 means from 1,000 samples of 5 women each\nsample_means_5 &lt;- rep_sample_n(gest, n = 5, samples = 1000) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period)) |&gt;\n  mutate(n = 5)\nhead(sample_means_5)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      267.     5\n2      2      263.     5\n3      3      255.     5\n4      4      269.     5\n5      5      267.     5\n6      6      266.     5\n\n# (b) 1,000 means from 1,000 samples of 50 women each\nsample_means_50 &lt;- rep_sample_n(gest, n = 50, samples = 1000) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period)) |&gt;\n  mutate(n = 50)\nhead(sample_means_50)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      263.    50\n2      2      262.    50\n3      3      269.    50\n4      4      266.    50\n5      5      267.    50\n6      6      268.    50\n\n# (c) 1,000 means from 1,000 samples of 500 women each\nsample_means_500 &lt;- rep_sample_n(gest, n = 500, samples = 1000) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period)) |&gt;\n  mutate(n = 500)\nhead(sample_means_500)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      265.   500\n2      2      265.   500\n3      3      267.   500\n4      4      266.   500\n5      5      265.   500\n6      6      265.   500\n\n\nWe now combine the above datasets of sample means for different sample sizes into a unique tibble. The function bind_rows() takes multiple tibbles and stacks them under each other.\n\nsample_means_n &lt;- bind_rows(sample_means_5, \n                            sample_means_50, \n                            sample_means_500)\n\nWe now plot three different density histograms showing the distribution of 1,000 sample means computed from samples of size 5, 50, and 500.\nThis would correspond to creating a histogram of the “red vertical bars” from Figure Figure 1, the only difference is that we have many more samples (1,000).\n\nggplot(sample_means_n) +\n  geom_histogram(aes(mean_gest, after_stat(density)), \n                 color = 'white', binwidth = 1) +\n  facet_grid(n ~ ., labeller = label_both) +\n  theme_bw() + \n  labs(x = 'Sample mean of gestation period (days)', y = 'Density')\n\n\n\n\n\n\nFigure 2: Density histograms of the sample means from 1,000 samples of women (\\(n\\) women per sample).\n\n\n\n\nEach of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.\nSuch a distribution is called the sampling distribution of the sample mean.\n\n\n\n\n\n\nSampling distribution\nThe sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.\nA sampling distribution shows how the statistic varies from sample to sample due to sampling variation.\n\n\n\n\n\n\n\n\n\n\n\n\nCentre and spread of a sampling distribution\n\n\n\n\n\nWhat is the mean and standard deviation of each histogram?\n\nsample_means_n |&gt;\n  group_by(n) |&gt;\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest))\n\n# A tibble: 3 × 3\n      n mean_xbar sd_xbar\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     5      266.   7.38 \n2    50      266.   2.23 \n3   500      266.   0.734\n\n\nCompare these quantities to the population mean and standard deviation: \\(\\mu\\) = 266 and \\(\\sigma\\) = 16.1.\nRegardless of the size of the samples we were drawing (5, 50, or 500), the average of the sample means was equal to the population mean. However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.\nThere is an interesting pattern in the decrease, which we will now verify. It can be proved that the standard deviation of the sample mean \\(\\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\\), i.e. the population standard deviation divided by \\(\\sqrt{n}\\) with \\(n\\) being the sample size.\nObtain the population standard deviation. Remember the entire population data were called gest and in this case we are very lucky to have the data for the entire population, typically we wouldn’t have those and neither the population standard deviation.\n\nsigma &lt;- sd(gest$gest_period)\n\nNow compute add a column that compares the SD from sampling with the theory-based one:\n\nsample_means_n |&gt;\n  group_by(n) |&gt;\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest)) |&gt;\n  mutate(sd_theory = sigma / sqrt(n))\n\n# A tibble: 3 × 4\n      n mean_xbar sd_xbar sd_theory\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1     5      266.   7.38      7.20 \n2    50      266.   2.23      2.28 \n3   500      266.   0.734     0.720\n\n\nThe last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)\nThe following result holds: \\[\n\\begin{aligned}\n\\mu_{\\bar X} &= \\mu = \\text{Population mean} \\\\\n\\sigma_{\\bar X} &= \\frac{\\sigma}{\\sqrt{n}} = \\frac{\\text{Population standard deviation}}{\\sqrt{\\text{Sample size}}}\n\\end{aligned}\n\\]\nBecause on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean \\(\\bar X\\) is an unbiased estimator of the population mean. In other words, it does not consistently “miss” the target. (However, if your sampling method is biased, the sample mean will be biased too.)\nThe standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because \\(\\sqrt{4} = 2\\) we halve \\(\\sigma_{\\bar X}\\) by making the sample size 4 times as large. Similarly, as \\(\\sqrt{9} = 3\\), we reduce \\(\\sigma_{\\bar X}\\) by one third by making the sample size 9 times as large.\nThe variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.\nRecall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter \\(\\mu\\), and a deviation of \\(\\bar x\\) from \\(\\mu\\) is called an estimation error. Hence, the standard deviation of the sample mean is called the standard error of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.\n\n\n\n\n\n\nStandard error\nThe standard error of a statistic, denoted \\(SE\\), is the standard deviation of its sampling distribution.\n\n\n\n\n\n\n\n\nSo, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula \\[\nSE = \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\nThe sample mean is normally distributed\n\n\n\n\n\nWe also notice that the density histograms in Figure 2 are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.\n\n\n\n\n\n\n\n\nThe random variable \\(\\bar X\\) follows a normal distribution: \\[\n\\bar X \\sim N(\\mu,\\ SE)\n\\]\nWe can also compute a z-score. We have that: \\[\nZ = \\frac{\\bar X - \\mu}{SE} \\sim N(0, 1)\n\\]\nWe know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. Thus, for approximately 95% of all samples, the sample means falls within \\(\\pm 2 SE\\) of the population mean \\(\\mu\\). Similarly, since \\(P(-3 &lt; Z &lt; 3) = 0.997\\), it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).\nThis suggests that:\n\nThe standard error \\(SE\\) is a measure of precision of \\(\\bar x\\) as an estimate of \\(\\mu\\).\nIf is a pretty safe bet to say that the true value of \\(\\mu\\) lies somewhere between \\(\\bar x - 2 SE\\) and \\(\\bar x + 2 SE\\).\nWe will doubt any hypothesis specifying that the population mean is \\(\\mu\\) when the value \\(\\mu\\) is more than \\(2 SE\\) away from the sample mean we got from our data, \\(\\bar x\\). We shall be even more suspicious when the hypothesised value \\(\\mu\\) is more than \\(3 SE\\) away from \\(\\bar x\\).\n\n\n\n\n\n\n\nCentre and shape of a sampling distribution\n\nCentre: If samples are randomly selected, the sampling distribution will be centred around the population parameter. (No bias)\nShape: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. (Central Limit Theorem)\n\n\n\n\nClearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, …\nThis requires the following steps:\n\nObtaining multiple samples, all of the same size, from the same population;\nFor each sample, calculate the value of the statistic;\nPlot the distribution of the computed statistics.\n\n\n\n\n\n\n\n\n\n\nWhy sample size matters\n\n\n\n\n\nYou might be wondering: why did we take multiple samples of size \\(n\\) from the population when, in practice, we can only afford to take one?\nThis is a good question. We have taken multiple samples to show how the estimation error varies with the sample size. We saw in Figure 2, shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more precise statistics, i.e. the estimates are more concentrated around the true parameter value.\n\n\n\n\nDensity histograms of the sample means from 5,000 samples of women (\\(n\\) women per sample).\n\n\n\nThis teaches us that, when we have to design a study, it is better to obtain just one sample with size \\(n\\) as large as we can afford.\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nWhat would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size \\(n = N\\)?\n\n\n\n\n\n\n\n\n\nThink about it: Answer\n\n\n\n\n\nIf you can, it is best to measure the entire population.\nIf we could afford to measure the entire population, then we would find the exact value of the parameter all the time. By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.\n\npop_means &lt;- gest |&gt;\n  rep_sample_n(n = nrow(gest), samples = 10) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period))\npop_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      266.\n 2      2      266.\n 3      3      266.\n 4      4      266.\n 5      5      266.\n 6      6      266.\n 7      7      266.\n 8      8      266.\n 9      9      266.\n10     10      266.\n\n\nThe following is a dotplot of the means computed above:\n\n\n\n\n\n\n\n\n\n\n\nTo summarize:\n\nWe have high precision when the estimates are less variable, and this happens for a large sample size.\nWe have no bias when we select samples that are representative of the population, and this happens when we do random sampling. No bias means that the estimates will be centred at the true population parameter to be estimated."
  },
  {
    "objectID": "rd1_11.html#fundamentals-of-inference",
    "href": "rd1_11.html#fundamentals-of-inference",
    "title": "Sampling distributions",
    "section": "",
    "text": "This section contains essential terminology and functions that are needed to complete the exercises provided.\n\n\n\n\n\n\nPopulation vs sample\n\n\n\n\n\nTypically:\n\nWe do not have data for the entire population. There are different possible reasons:\n\nIt’s too expensive to collect them\nBecause of deadlines, there is not sufficient time\nIt’s not possible to reach the entire population\n\n\nIt’s much easier to obtain data by taking a sample from that population of interest and measuring only the units chosen in the sample.\n\nNote that units do not necessarily have to be individuals, but they could be schools, companies, etc.\n\n\nWe wish to use the sample data to:\n\nInvestigate a claim about the whole population.\nTest an hypothesis about the entire population.\nAnswer a question about the whole population.\n\n\n\nThe process of using information from a sample (the part) in order to draw conclusions about the entire population (the whole) is known as statistical inference.\n\n\n\n\n\n\n\n\n\nParameters vs statistics\n\n\n\n\n\nAs we do not typically have the data for the entire population, the population is considered as “unknown” with respect the data that we wish to investigate. We could shortly say that the population data are unknown.\nFor example, if we are interested in the average IQ in the population, we don’t have the resources to go to every single individual and test their IQ score. So, in this respect, the population IQ scores are unknown.\nAs a consequence of this, any numerical summary of the population data is also unknown. In the above example, the population mean IQ score is unknown and needs to be estimated.\nSample data are more readily available or feasible to collect. Imagine collecting a sample of 50 individuals, chosen at random from the population, and testing each to obtain their IQ score. If you performed the random experiment, you would then obtain a sequence of 50 IQ measurements.\nAt the same time, it is also feasible to compute any numerical summary of the sample data. For example, you can compute the mean IQ score for those 50 individuals in the sample.\n\n\n\n\n\n\nWe typically use this terminology to distinguish a numerical summary when computed in the population (unknown) or in the collected sample (known).\n\nA parameter is a numerical summary of a population.\nA statistic is a numerical summary of the sample.\n\nA statistic is often used as a “best guess” or “estimate” for the unknown parameter. That is, we use the (sample) statistic to estimate a (population) parameter.\n\n\n\nIn the above example, the population mean IQ score is the parameter of interest, while the sample mean IQ score is the statistic.\nIt is typical to use special notation to distinguish between parameters and statistics in order to convey with a single letter: (1) which numerical summary is being computed, and (2) if it is computed on the population or on the sample data.\nThe following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.\n\nNotation for common parameters and statistics.\n\n\n\n\n\n\nNumerical summary\nPopulation parameter\nSample statistic\n\n\n\nMean\n\\(\\mu\\)\n\n\\(\\bar{x}\\) or \\(\\hat{\\mu}\\)\n\n\n\nStandard deviation\n\\(\\sigma\\)\n\n\\(s\\) or \\(\\hat{\\sigma}\\)\n\n\n\nProportion\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\nThe Greek letter \\(\\mu\\) (mu) represents the population mean (parameter), while \\(\\bar{x}\\) (x-bar) or \\(\\hat{\\mu}\\) (mu-hat) is the mean computed from the sample data (sample statistic).\nThe Greek letter \\(\\sigma\\) (sigma) represents the population standard deviation (parameter), while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) is the standard deviation computed from the sample data (sample statistic).\nThe Greek letter \\(p\\) represents the population proportion (parameter), while \\(\\hat{p}\\) (p-hat) is the proportion computed from the sample data (sample statistic).\n\nThe process of sampling \\(n\\) people at random from the population is a random experiment, as it leads to an uncertain outcome. Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable (random number).\nBefore actually performing the random experiment and picking individuals for the sample, the sample mean is a random number, as its value is uncertain. Once we actually perform the random experiment and measure the individuals in the sample, we have an observed value for the sample mean, i.e. a known number.\nWe will then distinguish between:\n\na statistic - a random variable (random number) that is uncertain because it involves a random experiment.\nan observed statistic - the actual number that is computed once the sample data have been collected by performing the chance experiment.\n\nNotation-wise we distinguish between a statistic (random variable) and an observed statistic (observed number) by using, respectively, an uppercase letter or a lowercase letter.\nUppercase letters refer to random variables. Recall that a random variable represents a well defined number, but whose value is uncertain as it involves an experiment of chance in order to reach to an actual value.\nFor example, \\(\\bar X\\) = “mean IQ score in a random sample of 50 individuals” is a random variable. It is clearly defined in operational terms by: (1) take a sample of 50 individuals at random, (2) measure their IQ score, and (3) compute the mean of their 50 IQ scores. However, the actual value that we can obtain is uncertain as it is the result of an experiment of chance involving random sampling from a population. There are many possible values we could obtain, so we are not sure which one we will see.\nAnother example: \\(X\\) = “number of heads in 10 flips of a coin”. This is also a clearly defined experiment: (1) flip a coin 10 times and (2) count the number of heads. However, the result is a random number, which is uncertain and will be unknown until we actually perform the experiment and flip a coin 10 times.\nLowercase letters refer to observed (i.e., realised) values of the random variable. An observed value of a random variable is just a number.\nFor example, once we actually collect 50 individuals and measure their IQ scores, we can sum those 50 numbers and divide the sum by 50 to obtain the observed sample mean. Say the sample mean IQ score is 102.3, we would write \\(\\bar x = 102.3\\).\nIn short, we would use for a sample mean:\n\nStatistic (sample mean): an uppercase letter before the value is actually known: \\(\\bar X\\)\n\nObserved statistic (observed sample mean): a lowercase letter once the value is actually known: \\(\\bar x\\)\n\n\n\n\n\n\n\n\n\n\n\nAvoiding bias due to sampling\n\n\n\n\n\nSampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.\nIf sampling bias exists, we cannot generalise our sample conclusions to the population.\n\n\n\n\n\n\n\n\nTo be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is random sampling. Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn. Random sampling is a strategy to avoid sampling bias.\n\n\n\n\n\n\nSimple random sampling\nWhen we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.\nWhen instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.\n\n\n\nIn general, we have bias when the method of collecting data causes the data to inaccurately reflect the population.\n\n\n\n\n\n\n\n\n\nSampling distribution\n\n\n\n\n\nThe natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. This is a special case which rarely happens in practice: we actually know what the distribution looks like in the population.\nWe will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. Remember, however, that in practice the population parameter would not be known.\nWe will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv\nFirst, we read the population data:\n\nlibrary(tidyverse)\ngest &lt;- read_csv('https://uoepsy.github.io/data/pregnancies.csv')\ndim(gest)\n\n[1] 49863     2\n\n\nThe data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy Look at the top six rows of the data set (the “head”):\n\nhead(gest)\n\n# A tibble: 6 × 2\n     id gest_period\n  &lt;dbl&gt;       &lt;dbl&gt;\n1     1        256.\n2     2        269.\n3     3        253.\n4     4        292.\n5     5        271.\n6     6        253.\n\n\nWe now want to investigate how much the sample means will vary from sample to sample. To do so, we will take many samples from the population of all gestation periods, and compute for each sample the mean.\nTo do so, we will load a function which we prepared for you called rep_sample_n(). This function is used to take a sample of \\(n\\) units from the population, and it lets you repeat this process many times.\nTo get the function in your computer, run this code:\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\n\n\n\n\n\n\nNOTE\nYou need to copy and paste the line\n\nsource('https://uoepsy.github.io/files/rep_sample_n.R')\n\nat the top of each file in which you want to use the rep_sample_n() function.\n\n\n\nThe function takes the following arguments:\nrep_sample_n(data, n = &lt;sample size&gt;, samples = &lt;how many samples&gt;)\n\ndata is the population\nn is the sample size\nsamples is how many samples of size \\(n\\) you want to take\n\nBefore doing anything involving random sampling, it is good practice to set the random seed. This is to ensure reproducibility of the results. Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.\nSet the random seed to any number you wish. Depending on the number, you will get the same results as me or not:\n\nset.seed(1234)\n\nObtain 10 samples of \\(n = 5\\) individuals each:\n\nsamples &lt;- rep_sample_n(gest, n = 5, samples = 10)\nsamples\n\n# A tibble: 50 × 3\n   sample    id gest_period\n    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1      1 40784        255.\n 2      1 40854        275.\n 3      1 41964        281.\n 4      1 15241        246.\n 5      1 33702        247.\n 6      2 35716        267.\n 7      2 17487        289.\n 8      2 15220        266.\n 9      2 19838        238.\n10      2  2622        281.\n# ℹ 40 more rows\n\n\nThe samples data frame contains 3 columns:\n\nsample, telling us which sample each row refers to\nid, telling us the units chosen to enter each sample\ngest_period, telling us the gestation period (in days) of each individual\n\nNote that the tibble samples has 50 rows, which is given by 5 individuals in each sample * 10 samples.\nYou can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, …, and 10th sample:\n\n\n\n\n\n\nNow, imagine computing the mean of the five observation in each sample. This will lead to 10 means, one for each of the 10 samples (of 5 individuals each).\n\nsample_means &lt;- samples |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period))\n\nsample_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      261.\n 2      2      268.\n 3      3      273.\n 4      4      269.\n 5      5      261.\n 6      6      271.\n 7      7      252.\n 8      8      262.\n 9      9      258.\n10     10      267.\n\n\nAs you can see this leads to a tibble having 10 rows (one for each sample), where each row is a mean computed from the 5 individuals which were chosen to enter the sample.\nThe gestation period (in days) for the first five women sampled were\n\n255.15, 275.34, 281.04, 245.62, 247.5\n\nThis sample has a mean of \\(\\bar x\\) = 260.93 days.\nThe second sample of 5 women had gestation periods\n\n267.03, 288.74, 265.56, 238, 280.56\n\nThe second sample has a mean gestation period of \\(\\bar x\\) = 267.98 days.\nIn Figure 1 we display the individual gestation periods in each sample as dots, along with the means gestation period \\(\\bar x\\) of each sample. The position of the sample mean is given by a red vertical bar.\nWe then increased the sample size to 50 women and took 10 samples each of 50 individuals. This set of samples together with their means is also plotted in Figure 1.\n\n\n\n\n\n\n\nFigure 1: Gestation period (in days) of samples of individuals.\n\n\n\n\nTwo important points need to be made from Figure Figure 1. First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample. The sample means vary in an unpredictable way, illustrating the fact that \\(\\bar X\\) is a summary of a random experiment (randomly choosing a sample) and hence is a random variable. Secondly, as we increase the sample size from 5 to 50, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel (b)). That is, with a larger sample size, the sample means fluctuate less and are more “consistent”.\nTo further investigate the variability of sample means, we will now generate many more sample means computed on:\n\n1,000 samples of \\(n = 5\\) women\n1,000 samples of \\(n = 50\\) women\n1,000 samples of \\(n = 500\\) women\n\nWe will also add at the end of each tibble a column specifying the sample size. In the first tibble, mutate(n = 5) creates a column called n where all values will be 5, to remind ourselves that those means were computed with samples of size \\(n = 5\\). Remember that mutate() takes a tibble and creates a new column or changes an existing one.\n\n# (a) 1,000 means from 1,000 samples of 5 women each\nsample_means_5 &lt;- rep_sample_n(gest, n = 5, samples = 1000) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period)) |&gt;\n  mutate(n = 5)\nhead(sample_means_5)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      267.     5\n2      2      263.     5\n3      3      255.     5\n4      4      269.     5\n5      5      267.     5\n6      6      266.     5\n\n# (b) 1,000 means from 1,000 samples of 50 women each\nsample_means_50 &lt;- rep_sample_n(gest, n = 50, samples = 1000) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period)) |&gt;\n  mutate(n = 50)\nhead(sample_means_50)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      263.    50\n2      2      262.    50\n3      3      269.    50\n4      4      266.    50\n5      5      267.    50\n6      6      268.    50\n\n# (c) 1,000 means from 1,000 samples of 500 women each\nsample_means_500 &lt;- rep_sample_n(gest, n = 500, samples = 1000) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period)) |&gt;\n  mutate(n = 500)\nhead(sample_means_500)\n\n# A tibble: 6 × 3\n  sample mean_gest     n\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1      265.   500\n2      2      265.   500\n3      3      267.   500\n4      4      266.   500\n5      5      265.   500\n6      6      265.   500\n\n\nWe now combine the above datasets of sample means for different sample sizes into a unique tibble. The function bind_rows() takes multiple tibbles and stacks them under each other.\n\nsample_means_n &lt;- bind_rows(sample_means_5, \n                            sample_means_50, \n                            sample_means_500)\n\nWe now plot three different density histograms showing the distribution of 1,000 sample means computed from samples of size 5, 50, and 500.\nThis would correspond to creating a histogram of the “red vertical bars” from Figure Figure 1, the only difference is that we have many more samples (1,000).\n\nggplot(sample_means_n) +\n  geom_histogram(aes(mean_gest, after_stat(density)), \n                 color = 'white', binwidth = 1) +\n  facet_grid(n ~ ., labeller = label_both) +\n  theme_bw() + \n  labs(x = 'Sample mean of gestation period (days)', y = 'Density')\n\n\n\n\n\n\nFigure 2: Density histograms of the sample means from 1,000 samples of women (\\(n\\) women per sample).\n\n\n\n\nEach of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.\nSuch a distribution is called the sampling distribution of the sample mean.\n\n\n\n\n\n\nSampling distribution\nThe sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.\nA sampling distribution shows how the statistic varies from sample to sample due to sampling variation.\n\n\n\n\n\n\n\n\n\n\n\n\nCentre and spread of a sampling distribution\n\n\n\n\n\nWhat is the mean and standard deviation of each histogram?\n\nsample_means_n |&gt;\n  group_by(n) |&gt;\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest))\n\n# A tibble: 3 × 3\n      n mean_xbar sd_xbar\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     5      266.   7.38 \n2    50      266.   2.23 \n3   500      266.   0.734\n\n\nCompare these quantities to the population mean and standard deviation: \\(\\mu\\) = 266 and \\(\\sigma\\) = 16.1.\nRegardless of the size of the samples we were drawing (5, 50, or 500), the average of the sample means was equal to the population mean. However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.\nThere is an interesting pattern in the decrease, which we will now verify. It can be proved that the standard deviation of the sample mean \\(\\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\\), i.e. the population standard deviation divided by \\(\\sqrt{n}\\) with \\(n\\) being the sample size.\nObtain the population standard deviation. Remember the entire population data were called gest and in this case we are very lucky to have the data for the entire population, typically we wouldn’t have those and neither the population standard deviation.\n\nsigma &lt;- sd(gest$gest_period)\n\nNow compute add a column that compares the SD from sampling with the theory-based one:\n\nsample_means_n |&gt;\n  group_by(n) |&gt;\n  summarise(mean_xbar = mean(mean_gest),\n            sd_xbar = sd(mean_gest)) |&gt;\n  mutate(sd_theory = sigma / sqrt(n))\n\n# A tibble: 3 × 4\n      n mean_xbar sd_xbar sd_theory\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1     5      266.   7.38      7.20 \n2    50      266.   2.23      2.28 \n3   500      266.   0.734     0.720\n\n\nThe last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)\nThe following result holds: \\[\n\\begin{aligned}\n\\mu_{\\bar X} &= \\mu = \\text{Population mean} \\\\\n\\sigma_{\\bar X} &= \\frac{\\sigma}{\\sqrt{n}} = \\frac{\\text{Population standard deviation}}{\\sqrt{\\text{Sample size}}}\n\\end{aligned}\n\\]\nBecause on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean \\(\\bar X\\) is an unbiased estimator of the population mean. In other words, it does not consistently “miss” the target. (However, if your sampling method is biased, the sample mean will be biased too.)\nThe standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because \\(\\sqrt{4} = 2\\) we halve \\(\\sigma_{\\bar X}\\) by making the sample size 4 times as large. Similarly, as \\(\\sqrt{9} = 3\\), we reduce \\(\\sigma_{\\bar X}\\) by one third by making the sample size 9 times as large.\nThe variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.\nRecall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter \\(\\mu\\), and a deviation of \\(\\bar x\\) from \\(\\mu\\) is called an estimation error. Hence, the standard deviation of the sample mean is called the standard error of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.\n\n\n\n\n\n\nStandard error\nThe standard error of a statistic, denoted \\(SE\\), is the standard deviation of its sampling distribution.\n\n\n\n\n\n\n\n\nSo, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula \\[\nSE = \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\nThe sample mean is normally distributed\n\n\n\n\n\nWe also notice that the density histograms in Figure 2 are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.\n\n\n\n\n\n\n\n\nThe random variable \\(\\bar X\\) follows a normal distribution: \\[\n\\bar X \\sim N(\\mu,\\ SE)\n\\]\nWe can also compute a z-score. We have that: \\[\nZ = \\frac{\\bar X - \\mu}{SE} \\sim N(0, 1)\n\\]\nWe know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. Thus, for approximately 95% of all samples, the sample means falls within \\(\\pm 2 SE\\) of the population mean \\(\\mu\\). Similarly, since \\(P(-3 &lt; Z &lt; 3) = 0.997\\), it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).\nThis suggests that:\n\nThe standard error \\(SE\\) is a measure of precision of \\(\\bar x\\) as an estimate of \\(\\mu\\).\nIf is a pretty safe bet to say that the true value of \\(\\mu\\) lies somewhere between \\(\\bar x - 2 SE\\) and \\(\\bar x + 2 SE\\).\nWe will doubt any hypothesis specifying that the population mean is \\(\\mu\\) when the value \\(\\mu\\) is more than \\(2 SE\\) away from the sample mean we got from our data, \\(\\bar x\\). We shall be even more suspicious when the hypothesised value \\(\\mu\\) is more than \\(3 SE\\) away from \\(\\bar x\\).\n\n\n\n\n\n\n\nCentre and shape of a sampling distribution\n\nCentre: If samples are randomly selected, the sampling distribution will be centred around the population parameter. (No bias)\nShape: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. (Central Limit Theorem)\n\n\n\n\nClearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, …\nThis requires the following steps:\n\nObtaining multiple samples, all of the same size, from the same population;\nFor each sample, calculate the value of the statistic;\nPlot the distribution of the computed statistics.\n\n\n\n\n\n\n\n\n\n\nWhy sample size matters\n\n\n\n\n\nYou might be wondering: why did we take multiple samples of size \\(n\\) from the population when, in practice, we can only afford to take one?\nThis is a good question. We have taken multiple samples to show how the estimation error varies with the sample size. We saw in Figure 2, shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more precise statistics, i.e. the estimates are more concentrated around the true parameter value.\n\n\n\n\nDensity histograms of the sample means from 5,000 samples of women (\\(n\\) women per sample).\n\n\n\nThis teaches us that, when we have to design a study, it is better to obtain just one sample with size \\(n\\) as large as we can afford.\n\n\n\n\n\n\nThink about it\n\n\n\n\n\nWhat would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size \\(n = N\\)?\n\n\n\n\n\n\n\n\n\nThink about it: Answer\n\n\n\n\n\nIf you can, it is best to measure the entire population.\nIf we could afford to measure the entire population, then we would find the exact value of the parameter all the time. By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.\n\npop_means &lt;- gest |&gt;\n  rep_sample_n(n = nrow(gest), samples = 10) |&gt;\n  group_by(sample) |&gt;\n  summarise(mean_gest = mean(gest_period))\npop_means\n\n# A tibble: 10 × 2\n   sample mean_gest\n    &lt;dbl&gt;     &lt;dbl&gt;\n 1      1      266.\n 2      2      266.\n 3      3      266.\n 4      4      266.\n 5      5      266.\n 6      6      266.\n 7      7      266.\n 8      8      266.\n 9      9      266.\n10     10      266.\n\n\nThe following is a dotplot of the means computed above:\n\n\n\n\n\n\n\n\n\n\n\nTo summarize:\n\nWe have high precision when the estimates are less variable, and this happens for a large sample size.\nWe have no bias when we select samples that are representative of the population, and this happens when we do random sampling. No bias means that the estimates will be centred at the true population parameter to be estimated."
  },
  {
    "objectID": "rd1_11.html#glossary",
    "href": "rd1_11.html#glossary",
    "title": "Sampling distributions",
    "section": "\n2 Glossary",
    "text": "2 Glossary\n\n\nStatistical inference. The process of drawing conclusions about the population from the data collected in a sample.\n\nPopulation. The entire collection of units of interest.\n\nSample. A subset of the entire population.\n\nRandom sample. A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population.\n\nRepresentation bias. Happens when some units of the population are systematically underrepresented in samples.\n\nGeneralisability. When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random).\n\nParameter. A fixed but typically unknown quantity describing the population.\n\nStatistic. A quantity computed on a sample.\n\nSampling distribution. The distribution of the values that a statistic takes on different samples of the same size and from the same population.\n\nStandard error. The standard error of a statistic is the standard deviation of the sampling distribution of the statistic."
  }
]