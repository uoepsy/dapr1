---
title: "Sampling distributions"
callout-appearance: simple
---


```{r setup}
#| include: false

source('assets/setup.R')
```


```{r include=FALSE}
library(tidyverse)
library(patchwork)

set.seed(3)

knitr::opts_chunk$set(out.width = '70%',
                      fig.align = 'center')
```


## Fundamentals of inference

This section contains essential terminology and functions that are needed to complete the exercises provided.

:::{.callout-note collapse="true"}
### Population vs sample

Typically:

- We do not have data for the entire population. There are different possible reasons:
    - It's too expensive to collect them
    - Because of deadlines, there is not sufficient time
    - It's not possible to reach the entire population
- It's much easier to obtain data by taking a sample from that population of interest and measuring only the units chosen in the sample. 
    - Note that units do not necessarily have to be individuals, but they could be schools, companies, etc.
- We wish to use the sample data to:
    - Investigate a claim about the whole population.
    - Test an hypothesis about the entire population.
    - Answer a question about the whole population.

The process of using information from a sample (the part) in order to draw conclusions about the entire population (the whole) is known as statistical inference.
:::


:::{.callout-note collapse="true"}
### Parameters vs statistics

As we do not typically have the data for the entire population, the population is considered as "unknown" with respect the data that we wish to investigate. We could shortly say that the population data are unknown.

For example, if we are interested in the average IQ in the population, we don't have the resources to go to every single individual and test their IQ score. So, in this respect, the population IQ scores are unknown.

As a consequence of this, any numerical summary of the population data is also unknown. In the above example, the population mean IQ score is unknown and needs to be estimated.

Sample data are more readily available or feasible to collect. Imagine collecting a sample of 50 individuals, chosen at random from the population, and testing each to obtain their IQ score. If you performed the random experiment, you would then obtain a sequence of 50 IQ measurements.

At the same time, it is also feasible to compute any numerical summary of the sample data. For example, you can compute the mean IQ score for those 50 individuals in the sample.

::: callout-tip
We typically use this terminology to distinguish a numerical summary when computed in the population (unknown) or in the collected sample (known).

- A **parameter** is a numerical summary of a population.

- A **statistic** is a numerical summary of the sample.

A statistic is often used as a "best guess" or "estimate" for the unknown parameter. That is, we use the (sample) statistic to estimate a (population) parameter.
:::

In the above example, the population mean IQ score is the parameter of interest, while the sample mean IQ score is the statistic.

It is typical to use special notation to distinguish between parameters and statistics in order to convey with a single letter: (1) which numerical summary is being computed, and (2) if it is computed on the population or on the sample data.

The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding estimates computed on a sample.

|Numerical summary  | Population parameter   | Sample statistic         |
|:------------------|:----------------------:|:------------------------:|
|Mean               | $\mu$                  | $\bar{x}$ or $\hat{\mu}$ |
|Standard deviation | $\sigma$               | $s$ or $\hat{\sigma}$    |
|Proportion         | $p$                    | $\hat{p}$                |

Table: Notation for common parameters and statistics.


The Greek letter $\mu$ (mu) represents the population mean (parameter), while $\bar{x}$ (x-bar) or $\hat{\mu}$ (mu-hat) is the mean computed from the sample data (sample statistic).

The Greek letter $\sigma$ (sigma) represents the population standard deviation (parameter), while $s$ or $\hat{\sigma}$ (sigma-hat) is the standard deviation computed from the sample data (sample statistic).

The Greek letter $p$ represents the population proportion (parameter), while $\hat{p}$ (p-hat) is the proportion computed from the sample data (sample statistic).


#### (Optional) Statistics are random variables {-}

The process of sampling $n$ people at random from the population is a random experiment, as it leads to an uncertain outcome. Hence, a statistic is a numerical summary of a random experiment and for this reason it is a random variable (random number).

Before actually performing the random experiment and picking individuals for the sample, the sample mean is a random number, as its value is uncertain. 
Once we actually perform the random experiment and measure the individuals in the sample, we have an observed value for the sample mean, i.e. a known number.

We will then distinguish between:

- a *statistic* - a random variable (random number) that is uncertain because it involves a random experiment.
- an *observed statistic* - the actual number that is computed once the sample data have been collected by performing the chance experiment.

Notation-wise we distinguish between a statistic (random variable) and an observed statistic (observed number) by using, respectively, an uppercase letter or a lowercase letter.

Uppercase letters refer to random variables. Recall that a random variable represents a well defined number, but whose value is uncertain as it involves an experiment of chance in order to reach to an actual value.  
For example, $\bar X$ = "mean IQ score in a random sample of 50 individuals" is a random variable. It is clearly defined in operational terms by: (1) take a sample of 50 individuals at random, (2) measure their IQ score, and (3) compute the mean of their 50 IQ scores. However, the actual value that we can obtain is uncertain as it is the result of an experiment of chance involving random sampling from a population. There are many possible values we could obtain, so we are not sure which one we will see.  
Another example: $X$ = "number  of heads in 10 flips of a coin". This is also a clearly defined experiment: (1) flip a coin 10 times and (2) count the number of heads. However, the result is a random number, which is uncertain and will be unknown until we actually perform the experiment and flip a coin 10 times.

Lowercase letters refer to observed (i.e., realised) values of the random variable. An observed value of a random variable is just a number.  
For example, once we actually collect 50 individuals and measure their IQ scores, we can sum those 50 numbers and divide the sum by 50 to obtain the observed sample mean. Say the sample mean IQ score is 102.3, we would write $\bar x = 102.3$.

In short, we would use for a sample mean:

- Statistic (sample mean): an uppercase letter before the value is actually known: $\bar X$
- Observed statistic (observed sample mean): a lowercase letter once the value is actually known: $\bar x$
:::


:::{.callout-note collapse="true"}
### Avoiding bias due to sampling

Sampling bias occurs when the method used to select which units enter the sample causes the sample to not be a good representation of the population.

If sampling bias exists, we cannot generalise our sample conclusions to the population.

```{r echo=FALSE, out.width = '95%'}
knitr::include_graphics('images/prob/sampling_bias.png')
```

To be able to draw conclusions about the population, we need a representative sample. The key in choosing a representative sample is _random sampling_. 
Imagine an urn with tickets, where each ticket has the name of each population unit. Random sampling would involve mixing the urn and blindly drawing out some tickets from the urn.
Random sampling is a strategy to avoid sampling bias.


::: callout-tip
__Simple random sampling__

When we select the units entering the sample via simple random sampling, each unit in the population has an equal chance of being selected, meaning that we avoid sampling bias.

When instead some units have a higher chance of entering the same, we have misrepresentation of the population and sampling bias.
:::


In general, we have _bias_ when the method of collecting data causes the data to inaccurately reflect the population.
:::


:::{.callout-note collapse="true"}
### Sampling distribution

The natural gestation period (in days) for human births is normally distributed in the population with mean 266 days and standard deviation 16 days. 
This is a special case which **rarely** happens in practice: we actually know what the distribution looks like in the population. 

We will use this unlikely example to study how well does the sample mean estimate the population mean and, to do so, we need to know what the population mean is so that we can compare the estimate and the true value. 
Remember, however, that in practice the population parameter would _not_ be known.

We will consider data about the gestation period of the 49,863 women who gave birth in Scotland in 2019. These can be found at the following address: https://uoepsy.github.io/data/pregnancies.csv

First, we read the population data:
```{r}
library(tidyverse)
gest <- read_csv('https://uoepsy.github.io/data/pregnancies.csv')
dim(gest)
```

The data set contains information about 49,863 cases. For each case an identifier and the length of pregnancy
Look at the top six rows of the data set (the "head"):
```{r}
head(gest)
```


We now want to investigate how much the sample means will vary from sample to sample. To do so, we will take many samples from the population of all gestation periods, and compute for each sample the mean.

To do so, we will load a function which we prepared for you called `rep_sample_n()`. This function is used to take a sample of $n$ units from the population, and it lets you repeat this process many times.

To get the function in your computer, run this code:
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```

::: callout-caution
__NOTE__

You need to copy and paste the line 
```{r}
source('https://uoepsy.github.io/files/rep_sample_n.R')
```
at the top of each file in which you want to use the `rep_sample_n()` function.
:::

The function takes the following arguments:
```
rep_sample_n(data, n = <sample size>, samples = <how many samples>)
```

- `data` is the population

- `n` is the sample size

- `samples` is how many samples of size $n$ you want to take

Before doing anything involving random sampling, it is good practice to _set the random seed_. This is to ensure reproducibility of the results. 
Random number generation in R works by specifying a starting seed, and then numbers are generated starting from there.

Set the random seed to any number you wish. Depending on the number, you will get the same results as me or not:
```{r}
set.seed(1234)
```

Obtain 10 samples of $n = 5$ individuals each:
```{r}
samples <- rep_sample_n(gest, n = 5, samples = 10)
samples
```

The `samples` data frame contains 3 columns:

- `sample`, telling us which sample each row refers to

- `id`, telling us the units chosen to enter each sample

- `gest_period`, telling us the gestation period (in days) of each individual

Note that the tibble `samples` has `r nrow(samples)` rows, which is given by 5 individuals in each sample * 10 samples.

You can inspect the sample data in the following interactive table in which the data corresponding to each sample have been colour-coded so that you can distinguish the rows belonging to the 1st, 2nd, ..., and 10th sample:

```{r echo=FALSE}
library(DT)

datatable(samples,
          options = list(autoWidth = TRUE, pageLength = 10,
                         columnDefs = list(list(className = 'dt-center', 
                                                targets = 1:3)))) |>
  formatSignif('gest_period', 5) |>
  formatStyle('sample',
              backgroundColor = styleEqual(
                unique(samples$sample), 
                c("#8dd3c7", "#ffffb3", "#bebada", 
                  "#fb8072", "#80b1d3", "#fdb462", 
                  "#b3de69", "#fccde5", "#d9d9d9", 
                  "#bc80bd")
              ))
```

Now, imagine computing the mean of the five observation in each sample. This will lead to 10 means, one for each of the 10 samples (of 5 individuals each).

```{r}
sample_means <- samples |>
  group_by(sample) |>
  summarise(mean_gest = mean(gest_period))

sample_means
```

As you can see this leads to a tibble having 10 rows (one for each sample), where each row is a mean computed from the 5 individuals which were chosen to enter the sample.  

The gestation period (in days) for the first five women sampled were
<center>
`r filter(samples, sample == 1)$gest_period |> round(2)`
</center>
This sample has a mean of $\bar x$ = `r filter(samples, sample == 1)$gest_period |> mean() |> round(2)` days.  

The second sample of 5 women had gestation periods
<center>
`r filter(samples, sample == 2)$gest_period |> round(2)`
</center>
The second sample has a mean gestation period of $\bar x$ = `r filter(samples, sample == 2)$gest_period |> mean() |> round(2)` days.  

In @fig-sampl-10-5-50 we display the individual gestation periods in each sample as dots, along with the means gestation period $\bar x$ of each sample. The position of the sample mean is given by a red vertical bar.

We then increased the sample size to 50 women and took 10 samples each of 50 individuals. This set of samples together with their means is also plotted in @fig-sampl-10-5-50.


```{r fig-sampl-10-5-50, echo = FALSE, fig.height = 5, fig.width=9, out.width = '100%'}
#| label: fig-sampl-10-5-50
#| fig-cap: "Gestation period (in days) of samples of individuals."
library(patchwork)

# data
da <- samples |> 
  group_by(sample) |>
  mutate(mean_gest = mean(gest_period))

db <- gest |>
  rep_sample_n(n = 50, samples = 10) |>
  group_by(sample) |>
  mutate(mean_gest = mean(gest_period))

# theme
all_theme <- 
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.line.x = element_line(),
        axis.ticks.x = element_line(),
        plot.title = element_text(hjust = 0.5, face = 'bold'))

# plots
pa <- ggplot(da, aes(x = gest_period, y = as.factor(sample))) +
  # geom_vline(aes(xintercept = 266), color = 'gray') + 
  geom_point(color = 'black', size = 2, pch = 1) +
  geom_segment(aes(x = mean_gest, xend = mean_gest, 
                   y = sample - 0.25, yend = sample + 0.25),
               size = 1, color = 'tomato3') +
  all_theme +
  # scale_x_continuous(
  #   breaks = round(seq(min(da$y, 230),max(da$y), by = 10))
  # ) + 
  labs(x = 'Gestation period (days)',
       y = 'Sample number',
       title = '(a) 10 samples of size n = 5') +
    xlim(min(db$gest_period, da$gest_period), 
         max(db$gest_period, da$gest_period))


pb <- ggplot(db, aes(x = gest_period, y = as.factor(sample))) +
  # geom_vline(aes(xintercept = 266), color = 'gray') + 
  geom_point(color = 'black', size = 2, pch = 1) +
  geom_segment(aes(x = mean_gest, xend = mean_gest, 
                   y = sample - 0.25, yend = sample + 0.25),
               size = 1, color = 'tomato3') +
  all_theme +
  # scale_x_continuous(
  #   breaks = round(seq(min(df$y, 230),max(df$y), by = 10))
  # ) + 
  labs(x = 'Gestation period (days)',
       y = 'Sample number',
       title = '(b) 10 samples of size n = 50') +
    xlim(min(db$gest_period, da$gest_period), 
         max(db$gest_period, da$gest_period))

pa | pb
```


Two important points need to be made from Figure @fig-sampl-10-5-50. 
First, each sample (and therefore each sample mean) is different. This is due to the randomness of which individuals end up being in each sample.
The sample means vary in an unpredictable way, illustrating the fact that $\bar X$ is a summary of a random experiment (randomly choosing a sample) and hence is a random variable.
Secondly, as we increase the sample size from 5 to 50, there appears to be a decrease in the variability of sample means (compare the variability in the vertical bars in panel (a) and panel (b)). That is, with a larger sample size, the sample means fluctuate less and are more "consistent".

To further investigate the variability of sample means, we will now generate many more sample means computed on:

(a) 1,000 samples of $n = 5$ women
(b) 1,000 samples of $n = 50$ women
(c) 1,000 samples of $n = 500$ women

We will also add at the end of each tibble a column specifying the sample size. In the first tibble, `mutate(n = 5)` creates a column called n where all values will be 5, to remind ourselves that those means were computed with samples of size $n = 5$. Remember that `mutate()` takes a tibble and creates a new column or changes an existing one. 

```{r}
# (a) 1,000 means from 1,000 samples of 5 women each
sample_means_5 <- rep_sample_n(gest, n = 5, samples = 1000) |>
  group_by(sample) |>
  summarise(mean_gest = mean(gest_period)) |>
  mutate(n = 5)
head(sample_means_5)

# (b) 1,000 means from 1,000 samples of 50 women each
sample_means_50 <- rep_sample_n(gest, n = 50, samples = 1000) |>
  group_by(sample) |>
  summarise(mean_gest = mean(gest_period)) |>
  mutate(n = 50)
head(sample_means_50)

# (c) 1,000 means from 1,000 samples of 500 women each
sample_means_500 <- rep_sample_n(gest, n = 500, samples = 1000) |>
  group_by(sample) |>
  summarise(mean_gest = mean(gest_period)) |>
  mutate(n = 500)
head(sample_means_500)
```

We now combine the above datasets of sample means for different sample sizes into a unique tibble. The function `bind_rows()` takes multiple tibbles and stacks them under each other.

```{r}
sample_means_n <- bind_rows(sample_means_5, 
                            sample_means_50, 
                            sample_means_500)
```

We now plot three different density histograms showing the distribution of 1,000 sample means computed from samples of size 5, 50, and 500.

This would correspond to creating a histogram of the "red vertical bars" from Figure @fig-sampl-10-5-50, the only difference is that we have many more samples (1,000).

```{r, fig.height=6, fig.width=5}
#| label: fig-sampl-dist-mean
#| fig-cap: "Density histograms of the sample means from 1,000 samples of women ($n$ women per sample)."
ggplot(sample_means_n) +
  geom_histogram(aes(mean_gest, after_stat(density)), 
                 color = 'white', binwidth = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  theme_bw() + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density')
```

Each of the density histograms above displays the distribution of the sample mean, computed on samples of the same size and from the same population.

Such a distribution is called the _**sampling distribution**_ of the sample mean.

::: callout-tip
__Sampling distribution__

The sampling distribution of a statistic is the distribution of a sample statistic computed on many different samples of the same size from the same population.

A sampling distribution shows how the statistic varies from sample to sample due to sampling variation.
:::

:::


:::{.callout-note collapse="true"}
### Centre and spread of a sampling distribution

What is the mean and standard deviation of each histogram?

```{r}
sample_means_n |>
  group_by(n) |>
  summarise(mean_xbar = mean(mean_gest),
            sd_xbar = sd(mean_gest))
```

Compare these quantities to the population mean and standard deviation: $\mu$ = `r mean(gest$gest_period) |> signif(3)` and $\sigma$ = `r sd(gest$gest_period) |> signif(3)`.

Regardless of the size of the samples we were drawing (5, 50, or 500), the average of the sample means was equal to the population mean.
However, the standard deviation of the sample means was smaller than the population mean. The variability in sample means also decreases as the sample size increases.

There is an interesting pattern in the decrease, which we will now verify. 
It can be proved that the standard deviation of the sample mean $\sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}$, i.e. the population standard deviation divided by $\sqrt{n}$ with $n$ being the sample size.

Obtain the population standard deviation. Remember the entire population data were called `gest` and  in this case we are very lucky to have the data for the entire population, typically we wouldn't have those and neither the population standard deviation.

```{r}
sigma <- sd(gest$gest_period)
```

Now compute add a column that compares the SD from sampling with the theory-based one:

```{r}
sample_means_n |>
  group_by(n) |>
  summarise(mean_xbar = mean(mean_gest),
            sd_xbar = sd(mean_gest)) |>
  mutate(sd_theory = sigma / sqrt(n))
```

The last two columns will be closer and closer as we increase the number of different samples we take from the population (e.g. 5,000 or 10,000 or even more samples.)

The following result holds:
$$
\begin{aligned}
\mu_{\bar X} &= \mu = \text{Population mean} \\
\sigma_{\bar X} &= \frac{\sigma}{\sqrt{n}} = \frac{\text{Population standard deviation}}{\sqrt{\text{Sample size}}}
\end{aligned}
$$

Because on average the sample mean (i.e. the estimate) is equal to the population mean (i.e. the parameter), the sample mean $\bar X$ is an _unbiased_ estimator of the population mean. In other words, it does not consistently "miss" the target. (However, if your sampling method is biased, the sample mean will be biased too.)

The standard deviation of the sample means tells us that the variability in the sample means gets smaller smaller as the sample size increases. Because $\sqrt{4} = 2$ we halve $\sigma_{\bar X}$ by making the sample size 4 times as large. Similarly, as $\sqrt{9} = 3$, we reduce $\sigma_{\bar X}$ by one third by making the sample size 9 times as large.


The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.

Recall that the standard deviation tells us the size of a typical deviation from the mean. Here, the mean is the population parameter $\mu$, and a deviation of $\bar x$ from $\mu$ is called an _estimation error_.
Hence, the standard deviation of the sample mean is called the _**standard error**_ of the mean. This tells us the typical estimation error that we commit when we estimate a population mean with a sample mean.

::: callout-tip
__Standard error__

The standard error of a statistic, denoted $SE$, is the standard deviation of its sampling distribution.
:::


<!-- In practice, we can only afford to take one sample from the population and we do not know the population standard deviation $\sigma$. -->
<!-- So, the standard error of the mean is computed using the standard deviation of the data in your sample: -->
<!-- $$ -->
<!-- SE = s_{\bar X} = \frac{s}{\sqrt{n}} -->
<!-- $$ -->

So, the standard error of the mean can be either computed as the standard deviation of the sampling distribution, or using the formula
$$
SE = \sigma_{\bar X} = \frac{\sigma}{\sqrt{n}}
$$

:::


:::{.callout-note collapse="true"}
### The sample mean is normally distributed

We also notice that the density histograms in @fig-sampl-dist-mean are symmetric and bell-shaped. Hence, they follow the shape of the normal curve.

```{r echo=FALSE, fig.height=6, fig.width=5}
all_theme <- theme_bw() + 
  theme(
      plot.title = element_text(hjust = 0.5, face = 'bold')
  )

a <- ggplot(sample_means_5) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_5$mean_gest), 
                            sd = sd(sample_means_5$mean_gest)), size = 1, color = 'red') + 
  all_theme +
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 5')

b <- ggplot(sample_means_50) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_50$mean_gest), 
                            sd = sd(sample_means_50$mean_gest)), size = 1, color = 'red') + 
  all_theme + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 50')

c <- ggplot(sample_means_500) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  geom_function(fun = dnorm, 
                args = list(mean = mean(sample_means_500$mean_gest), 
                            sd = sd(sample_means_500$mean_gest)), size = 1, color = 'red') + 
  all_theme + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density',
       title = 'n: 500')

(a + xlim(240, 290)) /(b + xlim(240, 290)) / (c + xlim(240, 290))
```


The random variable $\bar X$ follows a normal distribution:
$$
\bar X \sim N(\mu,\ SE)
$$

We can also compute a z-score. We have that:
$$
Z = \frac{\bar X - \mu}{SE} \sim N(0, 1)
$$

We know that for a normally distributed random variable, approximately 95% of all values fall within two standard deviations of its mean. 
Thus, for approximately 95% of all samples, the sample means falls within $\pm 2 SE$ of the population mean $\mu$.
Similarly, since $P(-3 < Z < 3) = 0.997$, it is even more rare to get a sample mean which is more than three standard errors away from the population mean (only 0.3% of the times).

This suggests that:

- The standard error $SE$ is a measure of precision of $\bar x$ as an estimate of $\mu$.

- If is a pretty safe bet to say that the true value of $\mu$ lies somewhere between $\bar x - 2 SE$ and $\bar x + 2 SE$.

- We will doubt any hypothesis specifying that the population mean is $\mu$ when the value $\mu$ is more than $2 SE$ away from the sample mean we got from our data, $\bar x$. We shall be even more suspicious when the hypothesised value $\mu$ is more than $3 SE$ away from $\bar x$.




::: callout-tip
__Centre and shape of a sampling distribution__

- _Centre_: If samples are randomly selected, the sampling distribution will be centred around the population parameter. _(No bias)_

- _Shape_: For most of the statistics we consider, if the sample size is large enough, the sampling distribution will follow a normal distribution, i.e. it is symmetric and bell-shaped. _(Central Limit Theorem)_
:::


Clearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, ...

This requires the following steps:

- Obtaining multiple samples, all of the same size, from the same population;

- For each sample, calculate the value of the statistic;

- Plot the distribution of the computed statistics.

:::



:::{.callout-note collapse="true"}
### Why sample size matters

You might be wondering: why did we take multiple samples of size $n$ from the population when, in practice, we can only afford to take one?

This is a good question. We have taken multiple samples to show how the estimation error varies with the sample size.
We saw in @fig-sampl-dist-mean, shown again below, that smaller sample sizes lead to more variable statistics, while larger sample sizes lead to more **precise** statistics, i.e. the estimates are more concentrated around the true parameter value.

```{r echo=FALSE, fig.cap = "Density histograms of the sample means from 5,000 samples of women ($n$ women per sample).", fig.height=6, fig.width=5}
ggplot(sample_means_n) +
  geom_histogram(aes(mean_gest, after_stat(density)), color = 'white', binwidth = 1) +
  facet_grid(n ~ ., labeller = label_both) +
  theme_bw() + 
  labs(x = 'Sample mean of gestation period (days)', y = 'Density')
```

__This teaches us that, when we have to design a study, it is better to obtain just one sample with size $n$ as large as we can afford.__


:::{.callout-caution collapse="true"}
#### Think about it
What would the sampling distribution of the mean look like if we could afford to take samples as big as the entire population, i.e. of size $n = N$?
:::

:::{.callout-tip collapse="true"}
#### Think about it: Answer
_If you can_, it is best to measure the entire population. 

If we could afford to measure the entire population, then we would find the exact value of the parameter all the time.
By taking multiple samples of size equal to the entire population, every time we would obtain the population parameter exactly, so the distribution would look like a histogram with a single bar on top of the true value: we would find the true parameter with a probability of one, and the estimation error would be 0.

```{r}
pop_means <- gest |>
  rep_sample_n(n = nrow(gest), samples = 10) |>
  group_by(sample) |>
  summarise(mean_gest = mean(gest_period))
pop_means
```

The following is a dotplot of the means computed above:
```{r echo=FALSE, fig.height=3, fig.width=4}
ggplot(pop_means, aes(x = mean_gest)) +
  geom_dotplot(binwidth = 1, dotsize = 0.05) +
  theme_classic() +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) + 
  labs(x = 'Sample mean', y = '')
```
:::

To summarize:

- We have __high precision__ when the estimates are less variable, and this happens for a __large sample size__. 

- We have __no bias__ when we select samples that are representative of the population, and this happens when we do __random sampling__. No bias means that the estimates will be centred at the true population parameter to be estimated.


```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('images/prob/bias_prec.png')
```

:::



## Glossary

- **Statistical inference.** The process of drawing conclusions about the population from the data collected in a sample.
- **Population.** The entire collection of units of interest.
- **Sample.** A subset of the entire population.
- **Random sample.** A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalised to the entire population.
- **Representation bias.** Happens when some units of the population are systematically underrepresented in samples.
- **Generalisability.** When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as those drawn at random).
- **Parameter.** A fixed but typically unknown quantity describing the population.
- **Statistic.** A quantity computed on a sample.
- **Sampling distribution.** The distribution of the values that a statistic takes on different samples of the same size and from the same population.
- **Standard error.** The standard error of a statistic is the standard deviation of the sampling distribution of the statistic.

