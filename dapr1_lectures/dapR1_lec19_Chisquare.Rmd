---
title: "<b>Chi-square Tests</b>"
subtitle: "Data Analysis for Psychology in R 1<br><br> "
author: "dapR1 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: TRUE
      countIncrementalSlides: FALSE
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
    base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

```{r packages, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(kableExtra)
library(patchwork)
library(moderndive)
library(ggmosaic)

knitr::opts_chunk$set(dev = "png", dpi = 300, 
                      fig.height = 4.5, fig.width = 5,
                      out.width = '100%',
                      warning = FALSE, message = FALSE)
```


# Weeks Learning Objectives
1. Understand how to perform a $\chi^2$ goodness-of-fit and interpret the results. 

2. Understand how to perform a $\chi^2$ test of independence and interpret the results.

3. Conduct and interpret the assumption checks for $\chi^2$ tests. 


---
# Topics for today


- Recording 1: 
  - Types of $\chi^2$ test
  - Worked example of $\chi^2$ goodness-of-fit
  - Relative, observed and expected frequencies

--

- Recording 2: 
  - Worked example of $\chi^2$ goodness-of-fit
  - Inferential testing, and write up.

--

- Recording 3: 
  - Worked example of $\chi^2$ test of independence.

--

- Recording 4: 
  - Residuals, assumptions and effect size measures.

--

- Bonus slides: For those who are interested, the full calculations for recording 2 are given in slides.


---
# Purpose
- $\chi^2$ goodness of fit test
  - The primary purpose is to test whether the collected data (observed frequencies) are consistent with a hypothesized/known distribution (expected frequencies).

- $\chi^2$ test of independence:
  - We have 2 categorical variables, drawn from a single population.
  - We want to know if the variables are independent or not.
  - If the category membership is dependent, then knowing what category someone is in on variable 1, helps us predict what category they would be in for variable 2.


---
# Data Requirements
- $\chi^2$ goodness of fit test
  - Single categorical variable
  
- $\chi^2$ test of independence:
  - Two categorical variables.
 
---
# Example: Goodness of fit
- Suppose we are interested in the distribution of students across three final year psychology options (Social, Differential, Developmental). 

- We have data from 2014-15, and we want to know if the distribution is the same in 2015-16.

---
# Data

```{r, echo=FALSE}
set.seed(007)
class <- tibble(
  ID = paste("ID", 1:150, sep = ""),
  course = as_factor(sample(c("Social", "Differential", "Developmental"), 150, replace = T, prob = c(.45, .20, .35)))
)
```

```{r}
head(class)
```

- `ID` = Unique ID variable
- `course` = factor with 3 levels (Social, Differential, Developmental)

---
# Observed frequencies

```{r}
tab1 <- class %>%
  group_by(course) %>%
  tally()
```

```{r}
tab1
```


---
# Relative frequencies
- In 2014-15, the department had the following proportions:
  - Social = 0.50, or 50%
  - Differential = 0.30, or 30%
  - Developmental = 0.20, or 20%


---
# Relative frequencies

```{r}
tab1 <- tab1 %>%
  transmute(
    course = course,
    relative = c(0.30, 0.50, 0.20),
    observed = n
  )
```

```{r}
tab1
```


---
# Expected frequencies
- Given this, and a total number of students (n=150) for the current year, we can calculate the expected frequencies for each area.
  - $Expected = Relative*N$


---
# Put it together

```{r}
tab1 <- tab1 %>%
  mutate(
    expected = relative*sum(observed)
  )
```

```{r}
tab1
```

---
class: center, middle
# Time for a break

---
class: center, middle
# Welcome Back!

**Now we have discussed how to calculate the core values from our data, let's think about our hypotheses, test statistic, and inferential testing.** 

---
# Hypotheses
$$
\begin{matrix}
H_0 = P(0.20, 0.50, 0.30) \\
H_1 \neq P(0.20, 0.50, 0.30)
\end{matrix}
$$

- $H_0$ says that the data follow a specific and known pattern or probabilities (frequencies)
- $H_1$ says they don't


---
# Test statistic

$$
\chi^2 = \sum_{i=1}^{k} \frac{(E_i - O_i)^2}{E_i}
$$

- $E_i$ = expected frequencies
- $O_i$ = observed frequencies
- $\sum_{i=1}^{k}$ = do the calculation starting from cell 1 through to cell $k$ (k=number groups) and add them up.

---
# Null Distribution
- Sampling distribution for $\chi^2$ test is a $\chi^2$ distribution.

- $\chi^2$ distribution describes the distribution of the sum of $k$ squared independent standard normal variables.
  - Huh?
  
$$
\chi^2 = \sum_{i=1}^{k} \frac{(E_i - O_i)^2}{E_i}
$$


---
# Null Distribution
- Parameter of the $\chi^2$ distribution is degrees of freedom (df) 
  - Just like $t$-test.
  
- df are determined by the number of categories ( $k$ )

- Goodness of fit test has $k-1$ degrees of freedom.
  - Why?

---
# Null Distribution

.pull-left[
```{r, echo=FALSE}
ggplot(tibble(x = c(-1,15)), aes(x = x)) +
  stat_function(fun=dchisq,
                geom = "line",
                args = list(df=2)) +
  stat_function(fun=dchisq,
                geom = "line",
                colour = "red",
                args = list(df=3)) +
  stat_function(fun=dchisq,
                geom = "line",
                colour = "blue",
                args = list(df=5)) +
  xlab("\n x2") +
  ylab("") +
  ggtitle("Chi-square Distribution (df = 2, 3, 5)")
```

]

.pull-right[

- The plot shows $\chi^2$ distributions for 2 (black), 3 (red), and 5 (blue) df's

- Note that as the df increase, the area under the curve for smaller values increases.

- What does that mean?
  - It means as we add up more things, we would expect the random fluctuations from 0 to to also increase.
  - In any given sample, even if the null is true in the population, sampling variability would mean we have some non-zero values.
  - So we need to account for this.

]

---
# Calculation

```{r}
tab1 <- tab1 %>%
  mutate(
    step1 = expected - observed,
    step2 = step1^2,
    step3 = step2/expected
  )
tab1
```

- Step1 = $E_i - O_i$
- Step2 = $(E_i - O_i)^2$
- Step3 = $\frac{(E_i - O_i)^2}{E_i}$

---
# Calculation
- Last step is to sum the values for step 3 to get the $\chi^2$

```{r}
x2 <- sum(tab1$step3)
x2
```

---
# Is my test significant?
- $\chi^2$ = `r round(x2,2)`

- Degrees of freedom = 3-1 = 2

- $\alpha$ = 0.05

---
# Is my test significant?

```{r, echo=FALSE, out.width = '45%'}
ggplot(tibble(x = c(-1,10)), aes(x = x)) +
  stat_function(fun=dchisq,
                geom = "line",
                args = list(df=2)) +
  stat_function(fun = dchisq, 
                geom = "area",
                xlim = c(qchisq(0.95, 2),10),
                alpha=.25,
                fill = "blue",
                args = list(df=2)) +
  xlab("\n x2") +
  ylab("") +
  ggtitle("Chi-Square -distribution (df=2, alpha = 0.05)")
```

---
# Is my test significant?

```{r}
tibble(
  CritValue = round(qchisq(0.95, 2),2),
  Exactp = round(1-pchisq(x2, 2),5)
)
```


---
# In R

```{r}
gof_res <- chisq.test(tab1$observed, p = c(0.3, 0.5, 0.2))
gof_res
```

---
# Write up

A $\chi^2$ goodness of fit test was conducted in order to investigate whether the distribution of students across Social, Developmental and Differential classes was equivalent in 2014- 15 and 2015-16. The goodness of fit test was significant ( $\chi^2$(`r gof_res$parameter`) = `r round(gof_res$statistic,2)`, $p$<.05) and thus the null hypothesis was rejected. The distribution of student's across courses differs between the two academic years.

---
class: center, middle
# Time for a break

---
class: center, middle
# Welcome Back!

**We will now follow the same steps for a test of independence.**

---
# Example: Independence
- I have conducted an experiment with three conditions (n=120, 40 per group)

- I want to check whether my participants are equally distributed based on some demographic variables.
  - Let's focus on whether English is participants first language

- Recall from an experimental design perspective, I want such things to be randomized across my groups.
  - So I would expect an even distribution.

---
# Data

```{r, echo=FALSE}
set.seed(7284)
exp <- tibble(
  ID = paste("ID", 1:120, sep = ""),
  condition = c(rep("control", 40), rep("group1", 40), rep("group2", 40)),
  lang = c(sample(c("Yes", "No"), 40, replace = T, prob = c(0.5, 0.5)),
           sample(c("Yes", "No"), 40, replace = T, prob = c(0.3, 0.7)),
           sample(c("Yes", "No"), 40, replace = T, prob = c(0.6, 0.4)))
)
```

```{r}
head(exp)
```

- `ID` = Unique ID variable
- `condition` = experimental conditions (control, group1, group2)
- `lang` = binary Yes/No for English as first language

---
# Tabular format

- It can be very useful to display data for two categorical variables as a contingency table.

```{r}
tabs <- addmargins(table(exp$condition, exp$lang))
tabs
```

---
# Visualizing Data: Mosaic Plot

.pull-left[
```{r, eval=FALSE, warning = FALSE}
#install.packages("ggmosaic")
#library(ggmosaic)

ggplot(data = exp) +
  geom_mosaic(aes(x=product(condition, lang), 
                  fill = condition)) +
  labs(x = "\n First Language", y = "")
```

]

.pull-right[
```{r, echo=FALSE}
#install.packages("ggmosaic")
#library(ggmosaic)

ggplot(data = exp) +
  geom_mosaic(aes(x = product(condition, lang), fill = condition)) +
  labs(x = "\n First Language", y = "")
```

]

---
# Hypotheses

.pull-left[
$$
\begin{matrix}
H_0: P_{11} = P_{12}, P_{21} = P_{22}, P_{31} = P_{32} \\
H_1: P_{11} \neq P_{12} | P_{21} \neq P_{22} | P_{31} \neq P_{32}
\end{matrix}
$$

- $H_0$ says the proportion of each cell in each row are equal.

- $H_1$ says at least one of these pairs are not equal.

]


.pull.right[

```{r, echo=FALSE}
tibble(
  Name = c("Control", "Group1", "Group2"),
  No = c("P11","P21", "P31"),
  Yes = c("P12","P22","P32")
) %>%
  kable(col.names = c("", "No", "Yes")) %>%
  kable_styling(full_width = F)
```

]


---
# Intuition about the null

.pull-left[

```{r, echo=FALSE}
tibble(
  condition = c(rep("control", 10), rep("Group1", 10), rep("Group2", 10)),
  lang = c(rep("yes", 5), rep("no", 5), rep("yes", 5), rep("no", 5), rep("yes", 5), rep("no", 5))
) %>%
  ggplot(.) +
  geom_mosaic(aes(x = product(condition, lang), fill = condition)) +
  labs(x = "\n First Language", y = "") +
  ggtitle("Mosaic Plot for Null")

```


]


.pull-left[

```{r, echo=FALSE}
#install.packages("ggmosaic")
#library(ggmosaic)

ggplot(data = exp) +
  geom_mosaic(aes(x = product(condition, lang), fill = condition)) +
  labs(x = "\n First Language", y = "") +
  ggtitle("Mosiac Plot of Data")
```

]

---
# Test statistic
- The test statistic looks much the same as the statistic for the GoF test.

$$\chi^2 = \sum_{i=1}^{r}\sum_{i=1}^{c} \frac{(\hat{E_{ij}} - O_{ij})^2}{\hat{E_{ij}}}$$

- What is different?
  - $\sum_{i=1}^{r}\sum_{i=1}^{c}$ simply means sum the quantities for all cells in all rows (r) and columns (c)
  - But why $\hat{E_{ij}}$? Why the hat?

---
# Expected frequencies
- Remember in the GoF test we knew the expected frequencies because we had known proportions and known sample size.
  - Here we do not have that.
  
- So we have to estimate the expected frequencies from the data. 
  - Hence we use $\hat{E}$ to show this is an estimate.
  
$$\hat{E_{ij}} = \frac{R_iC_j}{N}$$

- Where
  - $R_i$ = the row marginal for a cell $i$
  - $C_i$ = the column marginal for a cell $j$
  - $N$ = total sample size

- Here we will show the calculation for one cell (for the cell by cell calculations see the additional material).

---
# Calculation: Controls-No

```{r, echo=FALSE}
tabs
```

$$\hat{E_{11}} = \frac{R_1C_1}{N} = \frac{40*65}{120} = \frac{2600}{120} = 21.67 \\$$

$$\frac{(\hat{E_{11}} - O_{11})^2}{\hat{E_{11}}} = \frac{(21.67 - 19)^2}{21.67} = \frac{7.1289}{21.67} = 0.33$$

---
# Null Distribution
- Again, we evaluate the $\chi^2$ test of independence statistic against the $\chi^2$-distribution.

- Here:

$$
df = (r-1)(c-1)
$$

- Note, $r$ and $c$ are just the number of levels for each categorical variable.

- In our example $(r-1)(c-1)=(3-1)(2-1)=2*1=2$
  - Thus using the same $\alpha$=0.05, we would have the same critical value = 5.99

---
# In R
```{r}
con <- table(exp$condition, exp$lang)
ind_res <- chisq.test(con)
ind_res
```


---
# Write up
A $\chi^2$ test of independence was performed to examine whether the distribution of English first language speakers was consistent across experimental conditions (n=120). The relation between these variables was significant ($\chi^2$(`r ind_res$parameter`) = `r round(ind_res$statistic,2)`, p <.05). Therefore, we reject the null hypothesis.


---
class: center, middle
# Time for a break

**For your mid-lecture exercise, please look over the full calculations of the test statistic for this example in the additional slides.**

---
class: center, middle
# Welcome Back!

**Our last recording for this week will look at cell residuals, assumptions, corrections and effect size.**

---
# Output
- Here I want to make brief comment about analysis objects.

- The object `ind_res` contains the output of our analysis.
  - This has lots of elements to it.

- We can view and work with these by using the $ sign

```{r}
names(ind_res)
```

---
# Residuals
- For example, lets look at the residuals.

- The Pearson residuals tell us which cells in the contingency table had the greatest differences.

```{r}
ind_res$residuals
```


---
# Assumptions
- Sufficiently large N to approximate a normal sampling distribution
  - We saw last semester this actually begins to happen pretty fast.

- Expected and observed cell frequencies are sufficiently large.
  - If either drop below 5, then there is not really enough data.

- Each observation appears in only 1 cell.
  - Data are independent.
  - If data are dependent, we can use a McNemar test.

---
# Yate's correction
- Our $\chi^2$ test only approximates a $\chi^2$ sampling distribution.

- When we have a 2x2 table with df=1, it turns out this approximation is not very good.
  - So for 2x2 tables we apply Yateâ€™s continuity correction.
  - This subtracts 0.5 from each cell deviation.
  - It is the default in R when we have a 2x2 table.

---
# Effect size
- Three possibilities:
  - Phi coefficient (for 2x2 tables)
  - Odds ratios
  - Cramer's V
  
- We will discuss odds ratios more in year 2, so let's look at Phi and Cramer's V.

---
# Effect size
- The equations for both measures are shown below:

$$Phi = \sqrt{\frac{\chi^2}{N}}$$


$$CramerV = \sqrt{\frac{\chi^2}{N*min(r-1,c-1)}}$$

- Cramer's V generalizes Phi to larger contingency tables.

---
# Cramer's V
- There is no base R calculation for Cramer's V.

- It is included in the `lsr` package for the Navarro book.

- Else we can construct it ourselves.

---
# Cramer's V

```{r}
CV = sqrt(ind_res$statistic /
    (length(exp$ID) * 
       (min(length(unique(exp$condition)),
            length(unique(exp$lang))
            ) - 1)))
CV
```



---
# Summary of today
- We have looked at tests for categorical data:
  1. Against a known distribution
  2. As a test of independence.
  
- We have considered the calculations, inferential tests, and interpretations.


---
class: center, middle
# Additional Materials

---
# Full calculations
```{r}
ind_res
```

- Let's do all the steps to calculate $\chi^2$ and the exact $p$-value.

---
# Full calculations
- Let's start with the expected values

$$
\hat{E_{ij}} = \frac{R_iC_j}{N}
$$

---
# Full calculations
```{r, echo=FALSE}
tabs
```

$$
\hat{E_{11}} = \frac{R_1C_1}{N} = \frac{40*65}{120} = \frac{2600}{120} = 21.67 \\
$$

- As we have the same number of participants in each condition, this is also the expected value for $\hat{E_{21}}$ and $\hat{E_{31}}$

---
# Full calculations
```{r, echo=FALSE}
tabs
```

$$
\hat{E_{12}} = \frac{R_1C_2}{N} = \frac{40*55}{120} = \frac{2200}{120} = 18.33 \\
$$

- As we have the same number of participants in each condition, this is also the expected value for $\hat{E_{22}}$ and $\hat{E_{23}}$

---
# Full calculations
- We can check these against the information in the output to the R analysis

```{r}
ind_res$expected
```

---
# Full calculations
- Now, the $\chi^2$

```{r, echo=FALSE}
tabs
```

$$\frac{(\hat{E_{11}} - O_{11})^2}{\hat{E_{11}}} = \frac{(21.67 - 19)^2}{21.67} = \frac{7.1289}{21.67} = 0.33$$

---
# Full calculations
- Now, the $\chi^2$

```{r, echo=FALSE}
tabs
```

$$\frac{(\hat{E_{21}} - O_{21})^2}{\hat{E_{21}}} = \frac{(21.67 - 31)^2}{21.67} = \frac{87.05}{21.67} = 4.02$$


---
# Full calculations
- Now, the $\chi^2$

```{r, echo=FALSE}
tabs
```

$$\frac{(\hat{E_{31}} - O_{31})^2}{\hat{E_{31}}} = \frac{(21.67 - 15)^2}{21.67} = \frac{44.49}{21.67} = 2.05$$


---
# Full calculations
- Now, the $\chi^2$

```{r, echo=FALSE}
tabs
```

$$\frac{(\hat{E_{12}} - O_{12})^2}{\hat{E_{12}}} = \frac{(18.33 - 21)^2}{18.33} = \frac{7.1289}{18.33} = 0.39$$


---
# Full calculations
- Now, the $\chi^2$

```{r, echo=FALSE}
tabs
```


$$\frac{(\hat{E_{22}} - O_{22})^2}{\hat{E_{22}}} = \frac{(18.33 - 9)^2}{18.33} = \frac{87.05}{18.33} = 4.75$$

---
# Full calculations
- Now, the $\chi^2$

```{r, echo=FALSE}
tabs
```


$$\frac{(\hat{E_{32}} - O_{32})^2}{\hat{E_{32}}} = \frac{(18.33 - 25)^2}{18.33} = \frac{44.49}{18.33} = 2.43$$

---
# Full calculations
- Last step is to add them up:

$$\chi^2 = \sum_{i=1}^{r}\sum_{i=1}^{c} \frac{(\hat{E_{ij}} - O_{ij})^2}{\hat{E_{ij}}}$$

```{r}
x2i <- 0.33 + 4.02 + 2.05 + 0.39 + 4.75 + 2.43
x2i
```


---
# Full calculations
- And check against the R results (tiny bit of rounding error)

```{r}
ind_res
```

---
# Full calculations
- And the p-value

```{r}
1 - pchisq(13.964, 2)
```


---
# Full calculations
- The Pearson's residuals are calculated as:

$$Residual_{ij} = \frac{(E_{ij} - O_{ij})}{\sqrt{E_{ij}}}$$


---
# Full calculations
- So let's do one residual and then look at the output of our analysis:

$$Residual_{11} = \frac{(E_{11} - O_{11})}{\sqrt{E_{11}}} = \frac{(21.67 - 19)}{\sqrt{21.67}} = \frac{2.67}{4.655105} = 0.57$$

```{r}
ind_res$residuals
```


---
# Full calculations
- Hold on....why is our calculation positive, and the R results negative?

- This is just an interpretation point.
  - In our calculation, we have used $E_{ij} - O_{ij}$
  - If instead we calculate $O_{ij} - E_{ij}$, then we would get the same absolute value but negative.
  - Why not try it.
